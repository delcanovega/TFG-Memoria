\chapter{Aportación de los participantes}


\section{Juan Ramón del Caño Vega}


\subsection{Antecedentes}

Antes de empezar el proyecto ya contaba con un conocimiento básico sobre el Aprendizaje por Refuerzo. En la asignatura que cursé de Inteligencia Artificial se le daba bastante importancia a este apartado, especialmente de forma práctica. En los laboratorios trabajamos con Q-Learning en un entorno Java. Se trataba de una simulación en la que teníamos que estabilizar una nave espacial con tres motores, no obstante tan sólo tuvimos que implementar las funciones de recompensa y discretización.

Respecto a Redes Neuronales, recuerdo que no entramos en profundidad. Se nos explicaron, pero ni llegamos a utilizarlas de forma práctica ni se consideraba materia de examen, por lo que quedaron bastante de lado. Lo mismo ocurrió con el Aprendizaje por Refuerzo Profundo, el cual se nos mencionó al final del curso junto con sus posibles usos en campos como el reconocimiento de imágenes.

También tenía experiencia en otras áreas del Aprendizaje Automático, tanto aprendizaje supervisado como no supervisado, con los que he trabajado en las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. Esto, a pesar de no estar directamente relacionado con nuestro trabajo, me facilitó acostumbrarme a trabajar con \texttt{Keras}.


\subsection{Aportación}

Inicialmente me dediqué a la parte de Aprendizaje por Refuerzo. Puesto que ya tenía una buena base teórica pudimos empezar a hacer pruebas con bastante rapidez.

Empecé por programar el simulador para poder ejecutar CartPole. El objetivo era hacerlo de forma modular, principalmente por un motivo: encapsular toda la lógica del agente en una clase propia nos permitiría mantener el ``bucle de ejecución'' lo más limpio y simple posible, de esta forma se asemejaba mucho al pseudocódigo que veíamos en los libros, como puede verse en los fragmentos de código de la sección \ref{sec:cartpoleDQN}).

Una vez conseguido eso, sólo quedaba por implementar la lógica del agente. Quizá lo más complicado fue la función de discretización, explicada en \ref{sec:disc}. Una vez implementada la función parametrizada sólo fue cuestión de probar algunas configuraciones e hiperparámetros hasta dar con la solución que más nos gustase. Eso y corregir algún que otro bug, como que la ecuación de Bellman no sumase recompensas futuras si el agente se encontraba en un estado final, lo cual hacía que nuestro algoritmo divergiese.

A la hora de documentar este primer bloque recurrí al libro \textit{Artificial Intelligence: A Moddern Approach} \citep{Russell:2009:AIM:1671238}, el cuál también fue mi libro de referencia durante la asignatura de Inteligencia Artificial y proporciona explicaciones bastante concisas de distintos campos. Aproveché este libro para escribir la introducción del proyecto y la primera parte de Aprendizaje por Refuerzo. Para los apartados más técnicos cambié a \textit{Reinforcement Learning: An Introduction} \citep{Sutton:2018:RLI:3312046}, manual por excelencia del Aprendizaje por Refuerzo. No obstante fue Juan Luis Romero (quien también ayudó e hizo pruebas con CartPole) el encargado de rematar el capítulo con las secciones de Q-Learning y Markov Decision Process.

En este punto el resto del equipo ya había acabado de investigar Redes Neuronales (especialmente Lidia Concepción y Francisco Ponce), y nos preparábamos para empezar con el Aprendizaje por Refuerzo Profundo. Para ponerme al día con Redes Neuronales repetí uno de los ejemplos de clasificación que ellos ya habían hecho, MNIST \citep{MNISTKeras}, pero esta vez usando Jupyter Notebooks y el conjunto de datos propio de Keras.

Una vez hecho esto todos nos pusimos a volver a resolver CartPole utilizando las DQN que vimos en libros como \textit{Fundamentals of Deep Learning} \citep{Buduma:grad-dec} y los artículos de DeepMind. Mientras el resto del equipo salto directamente a las implementaciones vistas en \ref{sec:cartpoledqn3} y \ref{sec:DA}, yo empecé desde \ref{code:dqn}. Estas implementaciones, a pesar de que sabíamos que no funcionarían demasiado bien, nos permitieron comprender el proceso de mejora del agente mucho mejor, y por supuesto a documentarlo mejor, en lo que también participé.

Finalmente, también dediqué tiempo al problema de MountainCar. Especialmente a darnos cuenta de por qué es un problema tan especial y cuáles eran los motivos por los que presentaba nuevos retos. Finalmente fue Ricardo Arranz quien se enfrentó con el problema hasta el final.


\section{Juan Luis Romero Sánchez}

\subsection{Antecedentes}

Hace un año, antes de embarcarme en la realización de este trabajo de investigación, no tenía las ideas claras sobre qué tema o idea sentar las bases del proyecto, por lo que decidí investigar en la lista de trabajos propuestos por los profesores y encontré en este la única temática que me llamaba la atención. Tras una primera toma de contacto con Antonio, donde me explicó la idea a desarrollar y que el grupo inicial ya estaba formado por Juan, Francisco y Lidia, me preguntó acerca de mis conocimientos en relación al tópico tratado. Me di cuenta que no había cursado ninguna asignatura en relación a Aprendizaje Automático, Redes Neuronales, Inteligencia Artificial ni nada por el estilo, ni si quiera había programado una sola línea de código en python antes, por lo que el mensaje para verano fue claro ``Ponte las pilas''.

Y así fue, para no quedarme rezagado respecto a los conocimientos previos que otros compañeros ya tenían de haber cursado dichas asignaturas, durante el verano me puse a estudiar python ya que sería el lenguaje de programación que utilizaríamos en toda la parte referente al código, así como a leer artículos \citep{mnih2013playing} \citep{Turing1950-TURCMA} \citep{Rodriguez2018} y libros como el de \citet{Buduma:general} recomendados por el profesor que hablaran sobre el tema. 

Toda la información que iba recogiendo me sonaba muy abstracta, pero sin darme cuenta, estaba creando una base que más tarde cuando comenzase a trabajar con el resto de compañeros cogería forma.

\subsection{Aportación}

Al principio hicimos una división en dos del grupo, con lo que unos empezarían a trabajar con la parte relacionada con las Redes Neuronales, y otra parte (en la que yo estuve) que trabajase en el Aprendizaje por Refuerzo. Decidí empezar a trabajar sobre este apartado ya que había leido algo más acerca de él, de modo que creí que podría aportar más, al mismo tiempo que no me sintiera tan perdido en la materia. 

Si bien es cierto que fue Juan el que hizo todo el código sobre el que empezásemos a trabajar, en un primer momento fuí un poco más a remolque ya que él tenía una idea más concreta del tema al haber cursado con anterioridad asignaturas que le ayudasen a ello, de modo que mi aportación se limitó a entender qué y cómo lo hacíamos. Con el paso de las semanas y alguna reunión con el profesor entre medias, logré reengancharme y entender mejor el tema que estábamos tratando, gracias a lo cual pude servir de más ayuda haciendo más pruebas con el código, probando y entendiendo más configuraciones, así como arreglando algún que otro bug que encontrábamos como fue afinar la ecuación de Bellman para que actualizase las recompensas de una forma correcta.

En paralelo, el resto de compañeros estuvieron trabajando con problemas relacionados con las Redes Neuronales, con lo que consiguieran entenderlas y saber como trabajar con ellas, ya que más tarde tendríamos que unificar ambos conceptos para trabajar en conjunto, con lo que una vez la parte de Aprendizaje por Refuerzo quedo bastante estable a falta de pocos retoques para darla por terminada, empecé a revisar a documentación y los ejercicios en los que mis compañeros habían estado trabajando este tiempo.

Referente a la memoria, hasta este punto revisé lo que ya había escrito Juan, corrigiendo pequeñas cosas del capítulo \ref{cap:reinforcementLearning} y completándolo con las secciones de Problemas de Decisión de Markov y Q-Learning, para las cuales me basé en el libro de \citet[cap. 9]{Buduma:general} y \citet{Watkins1992}.

Llegada la hora de empezar a trabajar con el Aprendizaje Profundo por Refuerzo, me salté la parte del \texttt{SimpleAgent} y el \texttt{BatchAgent} para comenzar directamente sobre el \texttt{RandomBatchAgent}, en el cual volqué mucho esfuerzo para hacer que funcionara como debería, solventando la mayor parte de los problemas en la parte del \texttt{Experience Replay} puesto que no lográbamos aplicar bien la ecuación de Bellman y de ese modo hacer que el agente aprendiese, hasta llegar al punto de tener un aprendizaje bastante estable, pero aun así con significativas diferencias entre ejecuciones, no lográbamos que convergiese del todo, hasta que Ricardo añadió la parte del \texttt{DoubleAgent}, sobre la cual trabajé haciendo pequeños retoques para conseguir que funcionase. Así mismo, hice una redacción inicial en lo referente a esta sección en la que había trabajado, ayudando posteriormente en su refinamiento.

Por último le llegó el turno al cambio de entorno y empezar a trabajar con ell MountainCar, sobre el cual hice pruebas al principio, pero al ver que otros compañeros conseguían más y mejores avances, en especial Ricardo, decidí dejar en esas manos el problema y volcar mi esfuerzo en corregir y redactar partes de la memoria en base a las correcciones que Antonio nos fue indicando.