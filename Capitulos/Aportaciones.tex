\chapter{Aportación de los participantes}


\section{Juan Ramón del Caño Vega}


\subsection{Antecedentes}

Antes de empezar el proyecto ya contaba con un conocimiento básico sobre el Aprendizaje por Refuerzo. En la asignatura que cursé de Inteligencia Artificial se le daba bastante importancia a este apartado, especialmente de forma práctica. En los laboratorios trabajamos con Q-Learning en un entorno Java. Se trataba de una simulación en la que teníamos que estabilizar una nave espacial con tres motores, no obstante tan sólo tuvimos que implementar las funciones de recompensa y discretización.

Respecto a Redes Neuronales, recuerdo que no entramos en profundidad. Se nos explicaron, pero ni llegamos a utilizarlas de forma práctica ni se consideraba materia de examen, por lo que quedaron bastante de lado. Lo mismo ocurrió con el Aprendizaje por Refuerzo Profundo, el cual se nos mencionó al final del curso junto con sus posibles usos en campos como el reconocimiento de imágenes.

También tenía experiencia en otras áreas del Aprendizaje Automático, tanto aprendizaje supervisado como no supervisado, con los que he trabajado en las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. Esto, a pesar de no estar directamente relacionado con nuestro trabajo, me facilitó acostumbrarme a trabajar con \texttt{Keras}.


\subsection{Aportación}

Inicialmente me dediqué a la parte de Aprendizaje por Refuerzo. Puesto que ya tenía una buena base teórica pudimos empezar a hacer pruebas con bastante rapidez.

Empecé por programar el simulador para poder ejecutar CartPole. El objetivo era hacerlo de forma modular, principalmente por un motivo: encapsular toda la lógica del agente en una clase propia nos permitiría mantener el ``bucle de ejecución'' lo más limpio y simple posible, de esta forma se asemejaba mucho al pseudocódigo que veíamos en los libros, como puede verse en los fragmentos de código de la sección \ref{sec:cartpoleDQN}).

Una vez conseguido eso, sólo quedaba por implementar la lógica del agente. Quizá lo más complicado fue la función de discretización, explicada en \ref{sec:disc}. Una vez implementada la función parametrizada sólo fue cuestión de probar algunas configuraciones e hiperparámetros hasta dar con la solución que más nos gustase. Eso y corregir algún que otro bug, como que la ecuación de Bellman no sumase recompensas futuras si el agente se encontraba en un estado final, lo cual hacía que nuestro algoritmo divergiese.

A la hora de documentar este primer bloque recurrí al libro \textit{Artificial Intelligence: A Moddern Approach} \citep{Russell:2009:AIM:1671238}, el cuál también fue mi libro de referencia durante la asignatura de Inteligencia Artificial y proporciona explicaciones bastante concisas de distintos campos. Aproveché este libro para escribir la introducción del proyecto y la primera parte de Aprendizaje por Refuerzo. Para los apartados más técnicos cambié a \textit{Reinforcement Learning: An Introduction} \citep{Sutton:2018:RLI:3312046}, manual por excelencia del Aprendizaje por Refuerzo. No obstante fue Juan Luis Romero (quien también ayudó e hizo pruebas con CartPole) el encargado de rematar el capítulo con las secciones de Q-Learning y Markov Decision Process.

En este punto el resto del equipo ya había acabado de investigar Redes Neuronales (especialmente Lidia Concepción y Francisco Ponce), y nos preparábamos para empezar con el Aprendizaje por Refuerzo Profundo. Para ponerme al día con Redes Neuronales repetí uno de los ejemplos de clasificación que ellos ya habían hecho, MNIST \citep{MNISTKeras}, pero esta vez usando Jupyter Notebooks y el conjunto de datos propio de Keras.

Una vez hecho esto todos nos pusimos a volver a resolver CartPole utilizando las DQN que vimos en libros como \textit{Fundamentals of Deep Learning} \citep{Buduma:dnn} y los artículos de DeepMind. Mientras el resto del equipo salto directamente a las implementaciones vistas en \ref{sec:cartpoledqn3} y \ref{sec:DA}, yo empecé desde \ref{code:dqn}. Estas implementaciones, a pesar de que sabíamos que no funcionarían demasiado bien, nos permitieron comprender el proceso de mejora del agente mucho mejor, y por supuesto a documentarlo mejor, en lo que también participé.

Finalmente, también dediqué tiempo al problema de MountainCar. Especialmente a darnos cuenta de por qué es un problema tan especial y cuáles eran los motivos por los que presentaba nuevos retos. Finalmente fue Ricardo Arranz quien se enfrentó con el problema hasta el final.

\section{Francisco Ponce Belmonte}


\subsection{Antecedentes}

Al principio tenía conocimientos bastante ligeros sobre temas como Aprendizaje por Refuerzo y Redes Neuronales, todos ellos lo aprendí en la asignatura de Inteligencia Artificial. Sin embargo, considerando esta base insuficiente, decidí cursar la optativa de Aprendizaje Automático, con el fin de ganar más conocimientos y soltura en el uso y funcionamiento de las Redes Neuronales.

Por otro lado, también cursé Minería de Datos. Aunque la asignatura no estaba directamente relacionada con el objetivo de este proyecto, sí que utilizaba algunas herramientas y principios que me resultaron útiles para empezar con mis aportaciones. Entre ellos, cabría destacar las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. 


\subsection{Aportación}

Mientras algunos de mis compañeros se encargaban de la base para Aprendizaje por Refuerzo, yo me centré en el desarrollo de la red neuronal junto a Lidia Concepción. Aunque ya teníamos unos conocimientos base, además de lo que íbamos aprendiendo paralelamente en Aprendizaje Automático, decidimos empezar desde abajo para ir analizando paso a paso las posibilidades de esta rama.

Debido a ello, comenzamos con el desarrollo de una red neuronal de clasificación \ref{sec:classif_NN} usando las herramientas con las que ya estábamos familiarizados, consiguiendo resultados rápidamente y sin ningún problema. Una vez más acostumbrados al funcionamiento de las Redes Neuronales, empezamos a preparar lo que realmente necesitaría para nuestro proyecto, una red neuronal de regresión \ref{sec:regres_NN}. Para ello, dejamos atrás las herramientas conocidas y comenzamos a utilizar \texttt{Keras}. Éste ya poseía unos cuantos ejemplos que podía aprovechar, aparte de automatizar muchos de los procesos necesarios para la resolución del problema.

Para entonces, el resto del grupo ya había concluido con su parte y pudimos empezar a unir nuestros aportes para empezar con el Aprendizaje por Refuerzo Profundo. Por mi parte, empecé directamente con las implementaciones vistas en \ref{sec:cartpoledqn3}, en busca de obtener resultados que analizar. Sin embargo, ante la tesitura de que éramos muchos trabajando individualmente sobre el mismo problema, y que algunos de mis compañeros estaban consiguiendo mejores resultados, decidí centrarme más en la memoria del proyecto.

Durante esta parte \ref{cap:deepLearning} comencé referenciando lo aprendido en las pruebas de los primeros meses\ref{sec:classif_NN} \ref{sec:regres_NN}, para luego basarme en el libro de \textit{Fundamentals of Deep Learning}, especialmente en sus segundo\citep{Buduma:backprop} y cuarto \citep{Buduma:dnn} capítulos. En ellos se tratan de manera profunda los fundamentos y mecánicas de Redes Neuronales y su aplicación y uso en el Aprendizaje por Refuerzo Profundo.

En última instancia, al igual que el resto de mis compañeros, revisé el trabajo completo tanto para búsqueda de errores como para una mayor comprensión de lo conseguido en el proyecto.
