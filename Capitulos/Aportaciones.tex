\chapter{Aportación de los participantes}

\section{Lidia Concepción Echeverría}
\subsection{Antecedentes}

La asignatura que me sirvió de base para entrar al proyecto fue la de Inteligencia Artificial. Si bien es cierto que ésta era más una introducción que un análisis en profundidad de toda la rama a la que se refiere, me llamaron la atención los temas dedicados a Aprendizaje Automático. Las nociones que aprendí entonces eran las de los tipos de Aprendizaje, diferenciando entre Supervisado y No Supervisado, además de conocer como concepto los algoritmos genéticos y las Redes Neuronales. 

Minería de Datos y Aprendizaje Automático fueron el siguiente paso, ambas asignaturas destinadas a aprender más sobre la rama con el mismo nombre que ésta última. Mientras que en Minería de Datos aprendí el uso de las librerías de Python para resolver directamente problemas y buscar mejoras en los resultados, en Aprendizaje Automático estudié el funcionamiento de esas librerías y los cálculos matemáticos que había tras los procesos que se llevaban a cabo para llegar finalmente a una resolución, ambas desde un enfoque práctico. Aprendizaje Automático fue la que más me sirvió para entender el funcionamiento de Redes Neuronales y los problemas aplicables a las mismas. 

Respecto a las tecnologías utilizadas, apenas había manejado Python antes de cursar las dos asignaturas mencionadas anteriormente. Gracias a ellas conseguí una introducción para el uso de librerías como \texttt{numpy}, \texttt{pandas} y \texttt{scikit-learn}, en forma de \textit{notebook}. Las prácticas en empresa que realicé este mismo curso tocaban temas relacionados con Aprendizaje No Supervisado, como técnicas de clustering, utilizando también Python. De esta forma, conseguí mantener una base sólida de este lenguaje de forma que no entorpeciera el trabajo durante el proyecto.

\subsection{Aportación}

Durante la primera parte de la investigación, me encargué junto a Francisco Ponce de lo relacionado con Redes Neuronales, ya que ambos estábamos cursando Aprendizaje Automático y teníamos más reciente ese tema. El libro que usamos de referencia para saber por dónde avanzar y ampliar mi investigación fue el \textit{Fundamentals of Deep Learning} \citep{Buduma:general}.

Lo primero era realizar algunas pruebas con los conocimientos que tenía sobre Redes Neuronales, con el fin de familiarizarnos con \texttt{Keras}, la librería que íbamos a utilizar a lo largo del proyecto. Para ello, desarrollamos una prueba sobre un ejemplo de clasificación conocido \ref{sec:classif_NN}. Viendo que funcionaba sin problemas, realizamos una segunda prueba, esta vez con un problema de regresión \ref{sec:regres_NN}. Este tipo sería el que utilizaríamos posteriormente, por lo que debíamos asegurarnos de que podíamos manejarnos con redes destinadas a estos problemas. 

Tras la segunda prueba, fue el momento de poner en común lo aprendido por nuestra parte respecto al grupo que se había dedicado a la parte de Q-Learning, y comenzar a discutir cómo avanzar al siguiente paso en nuestro proyecto: Aprendizaje por Refuerzo Profundo, aplicado concretamente al problema de CartPole. Cada uno intentó ver el problema desde un enfoque distinto, convergiendo finalmente en lo que sería el resultado final \ref{sec:DA}. Llegar a este punto conllevó una importante investigación por parte de todos, ya que las versiones anteriores no funcionaban correctamente o daban resultados poco deseables. La mayor carga de trabajo en este punto fue llevada por Juan Ramón del Caño y Juan Luis Romero; por mi parte, me dediqué a apoyar en la resolución de problemas y a investigar en busca de otras opciones.

Mientras el desarrollo del agente utilizando DQNs era trasladado al entorno de MountainCar, por parte de Ricardo Arranz, decidimos desarrollar y corregir las ideas que habíamos ido planteando para la memoria final. Me encargué del capítulo dedicado a explicar las Redes Neuronales y DQNs, además de añadir múltiples aportaciones y correcciones en los demás capítulos.


\section{Juan Ramón del Caño Vega}


\subsection{Antecedentes}

Antes de empezar el proyecto ya contaba con un conocimiento básico sobre el Aprendizaje por Refuerzo. En la asignatura que cursé de Inteligencia Artificial se le daba bastante importancia a este apartado, especialmente de forma práctica. En los laboratorios trabajamos con Q-Learning en un entorno Java. Se trataba de una simulación en la que teníamos que estabilizar una nave espacial con tres motores, no obstante tan sólo tuvimos que implementar las funciones de recompensa y discretización.

Respecto a Redes Neuronales, recuerdo que no entramos en profundidad. Se nos explicaron, pero ni llegamos a utilizarlas de forma práctica ni se consideraba materia de examen, por lo que quedaron bastante de lado. Lo mismo ocurrió con el Aprendizaje por Refuerzo Profundo, el cual se nos mencionó al final del curso junto con sus posibles usos en campos como el reconocimiento de imágenes.

También tenía experiencia en otras áreas del Aprendizaje Automático, tanto aprendizaje supervisado como no supervisado, con los que he trabajado en las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. Esto, a pesar de no estar directamente relacionado con nuestro trabajo, me facilitó acostumbrarme a trabajar con \texttt{Keras}.


\subsection{Aportación}

Inicialmente me dediqué a la parte de Aprendizaje por Refuerzo. Puesto que ya tenía una buena base teórica pudimos empezar a hacer pruebas con bastante rapidez.

Empecé por programar el simulador para poder ejecutar CartPole. El objetivo era hacerlo de forma modular, principalmente por un motivo: encapsular toda la lógica del agente en una clase propia nos permitiría mantener el ``bucle de ejecución'' lo más limpio y simple posible, de esta forma se asemejaba mucho al pseudocódigo que veíamos en los libros, como puede verse en los fragmentos de código de la sección \ref{sec:cartpoleDQN}).

Una vez conseguido eso, sólo quedaba por implementar la lógica del agente. Quizá lo más complicado fue la función de discretización, explicada en \ref{sec:disc}. Una vez implementada la función parametrizada sólo fue cuestión de probar algunas configuraciones e hiperparámetros hasta dar con la solución que más nos gustase. Eso y corregir algún que otro bug, como que la ecuación de Bellman no sumase recompensas futuras si el agente se encontraba en un estado final, lo cual hacía que nuestro algoritmo divergiese.

A la hora de documentar este primer bloque recurrí al libro \textit{Artificial Intelligence: A Moddern Approach} \citep{Russell:2009:AIM:1671238}, el cuál también fue mi libro de referencia durante la asignatura de Inteligencia Artificial y proporciona explicaciones bastante concisas de distintos campos. Aproveché este libro para escribir la introducción del proyecto y la primera parte de Aprendizaje por Refuerzo. Para los apartados más técnicos cambié a \textit{Reinforcement Learning: An Introduction} \citep{Sutton:2018:RLI:3312046}, manual por excelencia del Aprendizaje por Refuerzo. No obstante fue Juan Luis Romero (quien también ayudó e hizo pruebas con CartPole) el encargado de rematar el capítulo con las secciones de Q-Learning y Markov Decision Process.

En este punto el resto del equipo ya había acabado de investigar Redes Neuronales (especialmente Lidia Concepción y Francisco Ponce), y nos preparábamos para empezar con el Aprendizaje por Refuerzo Profundo. Para ponerme al día con Redes Neuronales repetí uno de los ejemplos de clasificación que ellos ya habían hecho, MNIST \citep{MNISTKeras}, pero esta vez usando Jupyter Notebooks y el conjunto de datos propio de Keras.

Una vez hecho esto todos nos pusimos a volver a resolver CartPole utilizando las DQN que vimos en libros como \textit{Fundamentals of Deep Learning} \citep{Buduma:general} y los artículos de DeepMind. Mientras el resto del equipo salto directamente a las implementaciones vistas en \ref{sec:cartpoledqn3} y \ref{sec:DA}, yo empecé desde \ref{code:dqn}. Estas implementaciones, a pesar de que sabíamos que no funcionarían demasiado bien, nos permitieron comprender el proceso de mejora del agente mucho mejor, y por supuesto a documentarlo mejor, en lo que también participé.

Finalmente, también dediqué tiempo al problema de MountainCar. Especialmente a darnos cuenta de por qué es un problema tan especial y cuáles eran los motivos por los que presentaba nuevos retos. Finalmente fue Ricardo Arranz quien se enfrentó con el problema hasta el final.

\section{Francisco Ponce Belmonte}


\subsection{Antecedentes}

Al principio tenía conocimientos bastante ligeros sobre temas como Aprendizaje por Refuerzo y Redes Neuronales, todos ellos lo aprendí en la asignatura de Inteligencia Artificial. Sin embargo, considerando esta base insuficiente, decidí cursar la optativa de Aprendizaje Automático, con el fin de ganar más conocimientos y soltura en el uso y funcionamiento de las Redes Neuronales.

Por otro lado, también cursé Minería de Datos. Aunque la asignatura no estaba directamente relacionada con el objetivo de este proyecto, sí que utilizaba algunas herramientas y principios que me resultaron útiles para empezar con mis aportaciones. Entre ellos, cabría destacar las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. 


\subsection{Aportación}

Mientras algunos de mis compañeros se encargaban de la base para Aprendizaje por Refuerzo, yo me centré en el desarrollo de la red neuronal junto a Lidia Concepción. Aunque ya teníamos unos conocimientos base, además de lo que íbamos aprendiendo paralelamente en Aprendizaje Automático, decidimos empezar desde abajo para ir analizando paso a paso las posibilidades de esta rama.

Debido a ello, comenzamos con el desarrollo de una red neuronal de clasificación \ref{sec:classif_NN} usando las herramientas con las que ya estábamos familiarizados, consiguiendo resultados rápidamente y sin ningún problema. Una vez más acostumbrados al funcionamiento de las Redes Neuronales, empezamos a preparar lo que realmente necesitaría para nuestro proyecto, una red neuronal de regresión \ref{sec:regres_NN}. Para ello, dejamos atrás las herramientas conocidas y comenzamos a utilizar \texttt{Keras}. Éste ya poseía unos cuantos ejemplos que podía aprovechar, aparte de automatizar muchos de los procesos necesarios para la resolución del problema.

Para entonces, el resto del grupo ya había concluido con su parte y pudimos empezar a unir nuestros aportes para empezar con el Aprendizaje por Refuerzo Profundo. Por mi parte, empecé directamente con las implementaciones vistas en \ref{sec:cartpoledqn3}, en busca de obtener resultados que analizar. Sin embargo, ante la tesitura de que éramos muchos trabajando individualmente sobre el mismo problema, y que algunos de mis compañeros estaban consiguiendo mejores resultados, decidí centrarme más en la memoria del proyecto.

Durante esta parte \ref{cap:deepLearning} comencé referenciando lo aprendido en las pruebas de los primeros meses tanto el problema de casificación\ref{sec:classif_NN} como el problema de regresión \ref{sec:regres_NN}, para luego basarme en el libro de \textit{Fundamentals of Deep Learning}, especialmente los capitulos sobre el \textbf{descenso de gradiente}\citep[cap. 2]{Buduma:general} y sobre  optimizadores\citep[cap. 4]{Buduma:general} . En ellos se tratan de manera profunda los fundamentos y mecánicas de Redes Neuronales y su aplicación y uso en el Aprendizaje por Refuerzo Profundo.

En última instancia, al igual que el resto de mis compañeros, revisé el trabajo completo tanto para búsqueda de errores como para una mayor comprensión de lo conseguido en el proyecto.

\section{Juan Luis Romero Sánchez}

\subsection{Antecedentes}

Hace un año, antes de embarcarme en la realización de este trabajo de investigación, no tenía las ideas claras sobre qué tema o idea sentar las bases del proyecto, por lo que decidí investigar en la lista de trabajos propuestos por los profesores y encontré en este la única temática que me llamaba la atención. Tras una primera toma de contacto con Antonio, donde me explicó la idea a desarrollar y que el grupo inicial ya estaba formado por Juan, Francisco y Lidia, me preguntó acerca de mis conocimientos en relación al tópico tratado. Me di cuenta que no había cursado ninguna asignatura en relación a Aprendizaje Automático, Redes Neuronales, Inteligencia Artificial ni nada por el estilo, ni si quiera había programado una sola línea de código en Python antes, por lo que tenía trabajo pendiente para verano antes de ponernos a trabajar todos en conjunto.

De este modo y para no quedarme rezagado respecto a los conocimientos previos que otros compañeros ya tenían al haber cursado dichas asignaturas, durante el verano me puse a estudiar Python ya que sería el lenguaje de programación que utilizaríamos en toda la parte referente al código, así como a leer artículos \citep{mnih2013playing} \citep{Turing1950-TURCMA} \citep{Rodriguez2018} y libros como el de \citet{Buduma:general}, recomendados por el profesor. 

Toda la información que iba recogiendo me sonaba muy abstracta, pero sin darme cuenta, estaba creando una base que más tarde cuando comenzase a trabajar con el resto de compañeros cogería forma.

\subsection{Aportación}

Al principio hicimos una división en dos del grupo, con lo que unos empezarían a trabajar con la parte relacionada con las Redes Neuronales, y otra parte (en la que yo estuve) que trabajase en el Aprendizaje por Refuerzo. Decidí empezar a trabajar sobre este apartado ya que había leido algo más acerca de él, de modo que creí que podría aportar más, al mismo tiempo que no me sintiera tan perdido en la materia. 

Si bien es cierto que fue Juan el que hizo todo el código sobre el que empezásemos a trabajar, en un primer momento fuí un poco más a remolque ya que él tenía una idea más concreta del tema al haber cursado con anterioridad asignaturas que le ayudasen a ello, de modo que mi aportación se limitó a entender qué y cómo lo hacíamos. Con el paso de las semanas y alguna reunión con el profesor entre medias, logré reengancharme y entender mejor el tema que estábamos tratando, gracias a lo cual pude servir de más ayuda haciendo más pruebas con el código, probando y entendiendo más configuraciones, así como arreglando algún que otro bug que encontrábamos como fue afinar la ecuación de Bellman para que actualizase las recompensas de una forma correcta.

En paralelo, el resto de compañeros estuvieron trabajando con problemas relacionados con las Redes Neuronales, con lo que consiguieran entenderlas y saber como trabajar con ellas, ya que más tarde tendríamos que unificar ambos conceptos para trabajar en conjunto, con lo que una vez la parte de Aprendizaje por Refuerzo quedo bastante estable a falta de pocos retoques para darla por terminada, empecé a revisar a documentación y los ejercicios en los que mis compañeros habían estado trabajando este tiempo.

Referente a la memoria, hasta este punto revisé lo que ya había escrito Juan, corrigiendo pequeñas cosas del capítulo \ref{cap:reinforcementLearning} y completándolo con las secciones de Problemas de Decisión de Markov y Q-Learning, para las cuales me basé en el libro de \citet[cap. 9]{Buduma:general} y \citet{Watkins1992}.

Llegada la hora de empezar a trabajar con el Aprendizaje Profundo por Refuerzo, me salté la parte del \texttt{SimpleAgent} y el \texttt{BatchAgent} para comenzar directamente sobre el \texttt{RandomBatchAgent}, en el cual volqué mucho esfuerzo para hacer que funcionara como debería, solventando la mayor parte de los problemas en la parte del \texttt{Experience Replay} puesto que no lográbamos aplicar bien la ecuación de Bellman y de ese modo hacer que el agente aprendiese, hasta llegar al punto de tener un aprendizaje bastante estable, pero aun así con significativas diferencias entre ejecuciones, no lográbamos que convergiese del todo, hasta que Ricardo añadió la parte del \texttt{DoubleAgent}, sobre la cual trabajé haciendo pequeños retoques para conseguir que funcionase. Así mismo, hice una redacción inicial en lo referente a esta sección en la que había trabajado, ayudando posteriormente en su refinamiento.

Por último le llegó el turno al cambio de entorno y empezar a trabajar con ell MountainCar, sobre el cual hice pruebas al principio, pero al ver que otros compañeros conseguían más y mejores avances, en especial Ricardo, decidí dejar en esas manos el problema y volcar mi esfuerzo en corregir y redactar partes de la memoria en base a las correcciones que Antonio nos fue indicando.
