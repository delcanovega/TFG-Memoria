\chapter{Aportación de los participantes}

\section{Lidia Concepción Echeverría}
\subsection{Antecedentes}

La asignatura que me sirvió de base para entrar al proyecto fue la de Inteligencia Artificial. Si bien es cierto que ésta era más una introducción que un análisis en profundidad de toda la rama a la que se refiere, me llamaron la atención los temas dedicados a Aprendizaje Automático. Las nociones que aprendí entonces eran las de los tipos de Aprendizaje, diferenciando entre Supervisado y No Supervisado, además de conocer como concepto los algoritmos genéticos y las Redes Neuronales. 

Minería de Datos y Aprendizaje Automático fueron el siguiente paso, ambas asignaturas destinadas a aprender más sobre la rama con el mismo nombre que ésta última. Mientras que en Minería de Datos aprendí el uso de las librerías de Python para resolver directamente problemas y buscar mejoras en los resultados, en Aprendizaje Automático estudié el funcionamiento de esas librerías y los cálculos matemáticos que había tras los procesos que se llevaban a cabo para llegar finalmente a una resolución, ambas desde un enfoque práctico. Aprendizaje Automático fue la que más me sirvió para entender el funcionamiento de Redes Neuronales y los problemas aplicables a las mismas. 

Respecto a las tecnologías utilizadas, apenas había manejado Python antes de cursar las dos asignaturas mencionadas anteriormente. Gracias a ellas conseguí una introducción para el uso de librerías como \texttt{numpy}, \texttt{pandas} y \texttt{scikit-learn}, en forma de \textit{notebook}. Las prácticas en empresa que realicé este mismo curso tocaban temas relacionados con Aprendizaje No Supervisado, como técnicas de clustering, utilizando también Python. De esta forma, conseguí mantener una base sólida de este lenguaje de forma que no entorpeciera el trabajo durante el proyecto.

\subsection{Aportación}

Durante la primera parte de la investigación, me encargué junto a Francisco Ponce de lo relacionado con Redes Neuronales, ya que ambos estábamos cursando Aprendizaje Automático y teníamos más reciente ese tema. El libro que usamos de referencia para saber por dónde avanzar y ampliar mi investigación fue el \textit{Fundamentals of Deep Learning} \citep{Buduma:backprop}.

Lo primero era realizar algunas pruebas con los conocimientos que tenía sobre Redes Neuronales, con el fin de familiarizarnos con \texttt{Keras}, la librería que íbamos a utilizar a lo largo del proyecto. Para ello, desarrollamos una prueba sobre un ejemplo de clasificación conocido \ref{sec:classif_NN}. Viendo que funcionaba sin problemas, realizamos una segunda prueba, esta vez con un problema de regresión \ref{sec:regres_NN}. Este tipo sería el que utilizaríamos posteriormente, por lo que debíamos asegurarnos de que podíamos manejarnos con redes destinadas a estos problemas. 

Tras la segunda prueba, fue el momento de poner en común lo aprendido por nuestra parte respecto al grupo que se había dedicado a la parte de Q-Learning, y comenzar a discutir cómo avanzar al siguiente paso en nuestro proyecto: Aprendizaje por Refuerzo Profundo, aplicado concretamente al problema de CartPole. Cada uno intentó ver el problema desde un enfoque distinto, convergiendo finalmente en lo que sería el resultado final \ref{sec:DA}. Llegar a este punto conllevó una importante investigación por parte de todos, ya que las versiones anteriores no funcionaban correctamente o daban resultados poco deseables. La mayor carga de trabajo en este punto fue llevada por Juan Ramón del Caño y Juan Luis Romero; por mi parte, me dediqué a apoyar en la resolución de problemas y a investigar en busca de otras opciones.

Mientras el desarrollo del agente utilizando DQNs era trasladado al entorno de MountainCar, por parte de Ricardo Arranz, decidimos desarrollar y corregir las ideas que habíamos ido planteando para la memoria final. Me encargué del capítulo dedicado a explicar las Redes Neuronales y DQNs, además de añadir múltiples aportaciones y correcciones en los demás capítulos.


\section{Juan Ramón del Caño Vega}


\subsection{Antecedentes}

Antes de empezar el proyecto ya contaba con un conocimiento básico sobre el Aprendizaje por Refuerzo. En la asignatura que cursé de Inteligencia Artificial se le daba bastante importancia a este apartado, especialmente de forma práctica. En los laboratorios trabajamos con Q-Learning en un entorno Java. Se trataba de una simulación en la que teníamos que estabilizar una nave espacial con tres motores, no obstante tan sólo tuvimos que implementar las funciones de recompensa y discretización.

Respecto a Redes Neuronales, recuerdo que no entramos en profundidad. Se nos explicaron, pero ni llegamos a utilizarlas de forma práctica ni se consideraba materia de examen, por lo que quedaron bastante de lado. Lo mismo ocurrió con el Aprendizaje por Refuerzo Profundo, el cual se nos mencionó al final del curso junto con sus posibles usos en campos como el reconocimiento de imágenes.

También tenía experiencia en otras áreas del Aprendizaje Automático, tanto aprendizaje supervisado como no supervisado, con los que he trabajado en las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. Esto, a pesar de no estar directamente relacionado con nuestro trabajo, me facilitó acostumbrarme a trabajar con \texttt{Keras}.


\subsection{Aportación}

Inicialmente me dediqué a la parte de Aprendizaje por Refuerzo. Puesto que ya tenía una buena base teórica pudimos empezar a hacer pruebas con bastante rapidez.

Empecé por programar el simulador para poder ejecutar CartPole. El objetivo era hacerlo de forma modular, principalmente por un motivo: encapsular toda la lógica del agente en una clase propia nos permitiría mantener el ``bucle de ejecución'' lo más limpio y simple posible, de esta forma se asemejaba mucho al pseudocódigo que veíamos en los libros, como puede verse en los fragmentos de código de la sección \ref{sec:cartpoleDQN}).

Una vez conseguido eso, sólo quedaba por implementar la lógica del agente. Quizá lo más complicado fue la función de discretización, explicada en \ref{sec:disc}. Una vez implementada la función parametrizada sólo fue cuestión de probar algunas configuraciones e hiperparámetros hasta dar con la solución que más nos gustase. Eso y corregir algún que otro bug, como que la ecuación de Bellman no sumase recompensas futuras si el agente se encontraba en un estado final, lo cual hacía que nuestro algoritmo divergiese.

A la hora de documentar este primer bloque recurrí al libro \textit{Artificial Intelligence: A Moddern Approach} \citep{Russell:2009:AIM:1671238}, el cuál también fue mi libro de referencia durante la asignatura de Inteligencia Artificial y proporciona explicaciones bastante concisas de distintos campos. Aproveché este libro para escribir la introducción del proyecto y la primera parte de Aprendizaje por Refuerzo. Para los apartados más técnicos cambié a \textit{Reinforcement Learning: An Introduction} \citep{Sutton:2018:RLI:3312046}, manual por excelencia del Aprendizaje por Refuerzo. No obstante fue Juan Luis Romero (quien también ayudó e hizo pruebas con CartPole) el encargado de rematar el capítulo con las secciones de Q-Learning y Markov Decision Process.

En este punto el resto del equipo ya había acabado de investigar Redes Neuronales (especialmente Lidia Concepción y Francisco Ponce), y nos preparábamos para empezar con el Aprendizaje por Refuerzo Profundo. Para ponerme al día con Redes Neuronales repetí uno de los ejemplos de clasificación que ellos ya habían hecho, MNIST \citep{MNISTKeras}, pero esta vez usando Jupyter Notebooks y el conjunto de datos propio de Keras.

Una vez hecho esto todos nos pusimos a volver a resolver CartPole utilizando las DQN que vimos en libros como \textit{Fundamentals of Deep Learning} \citep{Buduma:dnn} y los artículos de DeepMind. Mientras el resto del equipo salto directamente a las implementaciones vistas en \ref{sec:cartpoledqn3} y \ref{sec:DA}, yo empecé desde \ref{code:dqn}. Estas implementaciones, a pesar de que sabíamos que no funcionarían demasiado bien, nos permitieron comprender el proceso de mejora del agente mucho mejor, y por supuesto a documentarlo mejor, en lo que también participé.

Finalmente, también dediqué tiempo al problema de MountainCar. Especialmente a darnos cuenta de por qué es un problema tan especial y cuáles eran los motivos por los que presentaba nuevos retos. Finalmente fue Ricardo Arranz quien se enfrentó con el problema hasta el final.

\section{Francisco Ponce Belmonte}


\subsection{Antecedentes}

Al principio tenía conocimientos bastante ligeros sobre temas como Aprendizaje por Refuerzo y Redes Neuronales, todos ellos lo aprendí en la asignatura de Inteligencia Artificial. Sin embargo, considerando esta base insuficiente, decidí cursar la optativa de Aprendizaje Automático, con el fin de ganar más conocimientos y soltura en el uso y funcionamiento de las Redes Neuronales.

Por otro lado, también cursé Minería de Datos. Aunque la asignatura no estaba directamente relacionada con el objetivo de este proyecto, sí que utilizaba algunas herramientas y principios que me resultaron útiles para empezar con mis aportaciones. Entre ellos, cabría destacar las librerías \texttt{scikit-learn}, \texttt{pandas} o \texttt{numpy} de Python. 


\subsection{Aportación}

Mientras algunos de mis compañeros se encargaban de la base para Aprendizaje por Refuerzo, yo me centré en el desarrollo de la red neuronal junto a Lidia Concepción. Aunque ya teníamos unos conocimientos base, además de lo que íbamos aprendiendo paralelamente en Aprendizaje Automático, decidimos empezar desde abajo para ir analizando paso a paso las posibilidades de esta rama.

Debido a ello, comenzamos con el desarrollo de una red neuronal de clasificación \ref{sec:classif_NN} usando las herramientas con las que ya estábamos familiarizados, consiguiendo resultados rápidamente y sin ningún problema. Una vez más acostumbrados al funcionamiento de las Redes Neuronales, empezamos a preparar lo que realmente necesitaría para nuestro proyecto, una red neuronal de regresión \ref{sec:regres_NN}. Para ello, dejamos atrás las herramientas conocidas y comenzamos a utilizar \texttt{Keras}. Éste ya poseía unos cuantos ejemplos que podía aprovechar, aparte de automatizar muchos de los procesos necesarios para la resolución del problema.

Para entonces, el resto del grupo ya había concluido con su parte y pudimos empezar a unir nuestros aportes para empezar con el Aprendizaje por Refuerzo Profundo. Por mi parte, empecé directamente con las implementaciones vistas en \ref{sec:cartpoledqn3}, en busca de obtener resultados que analizar. Sin embargo, ante la tesitura de que éramos muchos trabajando individualmente sobre el mismo problema, y que algunos de mis compañeros estaban consiguiendo mejores resultados, decidí centrarme más en la memoria del proyecto.

Durante esta parte \ref{cap:deepLearning} comencé referenciando lo aprendido en las pruebas de los primeros meses\ref{sec:classif_NN} \ref{sec:regres_NN}, para luego basarme en el libro de \textit{Fundamentals of Deep Learning}, especialmente en sus segundo\citep{Buduma:backprop} y cuarto \citep{Buduma:dnn} capítulos. En ellos se tratan de manera profunda los fundamentos y mecánicas de Redes Neuronales y su aplicación y uso en el Aprendizaje por Refuerzo Profundo.

En última instancia, al igual que el resto de mis compañeros, revisé el trabajo completo tanto para búsqueda de errores como para una mayor comprensión de lo conseguido en el proyecto.
