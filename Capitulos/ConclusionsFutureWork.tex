\chapter{Conclusions and future work}
\label{cap:conclusions}

\chapterquote{We are what we do repeatedly. Excellence, then, is not an act, it is a habit}{Aristotle}

\section{Conclusions}

This project has served us as an extensive study about the origins and evolution of one of the most explored technologies within the Artificial Inteligence: Machine Learning, concretely the Deep Learning field.

When we started this project, we just had a slight idea about this term's complexity involvements or the utilities's scope that can be reached with this technology. But fortunately, our goal was clear and our research was aimed for a single goal: To make it possible for an AI to play a videogame on its own.

We began by studying the basics of Machine Learning, taking as our starting point the technique of Reinforcement Learning. This one fitted perfectly our project, since when it comes to videogames, it is easy to find rewards in that environment. Investigating about it, we discovered the Q-Learning method, which seemed to be the best suited to our case. It allowed us to make decisions at every moment, maintaining a reliable representation of any possible environment, in a way that the AI could play normally regardless of the game.

After the previous study, it was time to do our first tests and learn how to use the environment. We started with a simple game: CartPole, available in the OpenAI Gym library. The environment was simple and the actions to be taken by the agent were reduced. The results were good, but we needed to go one step further. The model used in Q-Learning could give us a lot of problems in larger environments, even though the idea was good at first.

Almost simultaneously, we started looking for other ways to develop our project, finally opting for Neural Networks. We verified with several examples that these were compatible with our problem, concluding that it was a good point to advance. Exploring beyond what we already knew about them, we found the perfect way to combine the idea and results of Q-Learning with the speed and comfort that Neural Networks provide: DQNs.

Testing DQNs was somehow more tedious to perform, as it was something totally new for us and required extra research time. The results were not good at the beginning, but after stabilizing them with a second network and adding a memory for them to learn from their own mistakes, we achieve the correct learning of our agent, giving us better results.

The last step we reached in our project was to transfer everything we learned and accomplished in CartPole to another environment to check that the agent was still learning correctly. This time it was the MountainCar game, also available in the OpenAI Gym library. After several adaptations and some new minor components, we got successful results, so we concluded that we had managed to develop an AI capable of playing.

What has been achieved in this project has shown us the amount of possibilities that exist when it comes to solving a problem of this type. The research and time invested, has led us to understand the work involved in building an AI, even having a small initial base on which to stand, and the amount of information we still have to learn about this topic.

\section{Achievement of objectives}

At the beginning of this document, we proposed some objectives to explore during the course of this project, which have helped us to organize the work and its presentation in this document:

\begin{enumerate}
        \item The first objective was to understand what Reinforcement Learning is and how it differs from other branches of Machine Learning. Throughout the chapter \ref{cap:reinforcementLearning} we saw an explanation of this concept, the ideas on which it was based and their differences from other branches of Machine Learning, aswell as why we chose it as a starting point. Continuing with this idea, we explain all the elements that compose it and the way in which they relate to each other.
        \item Having seen how all the elements of Reinforcement Learning are related, we decided to test these concepts during the chapter \ref{cap:q-learning}, in which we explained the OpenAI Gym library. We chose the CartPole environment to carry out our experiments, managing to overtake the objective set for this environment and experimenting the problems involved in the implementation of this algorithm, in reference to the exponential increase of the Q-table and the complexity involved in keeping it completely updated, and achieving the correct learning of our agent.
        \item Another objective was to get into the field of Deep Learning and the fundamentals of Neural Networks. We discussed it throughout the chapter \ref{cap:deepLearning}, during which we explained the principles of Neural Networks, their composition, structure and behaviour. This way, we played with different problems of classification and regression that served us to have a first contact that later would help us to manage problems within the scope of Deep Reinforcement Learning.
        \item Our last goal was to combine Neural Networks and Reinforcement Learning, in a way that we could get the benefits of both, as well as finding solutions to their different issues, as we saw during the chapter \ref{cap:dqnEnAccion}. We studied step by step how to combine the two scopes, first replacing the Q-table of Reinforcement Learning with a Neuronal Network, which we saw that it didn't give our agent enough learning skills. After that, we introduced the concept of experience replay and the use of two networks, thanks to which we managed to stabilize learning and convergence.
        \item Having fulfilled all the objectives, and in view of the obtained results, we have been able to see the limitations of this field of study as of today, which is quickly evolving with new techniques every day. There definetly are lots of ways to follow if you are eager and interested in it.
\end{enumerate}

\section{Future work}

The project was aimed for teaching an AI to play and, although it has learned successfully, it has hardly been tested in a couple of environments with few variables. The path of our research could be the right one, but without further progress, or testing in other environments, we cannot be sure of that.

That said, if we could continue our project, we would focus on continuing with more tests on different games. After verifying that our AI is adaptable, we would start by adding more options to the possible actions that the player can perform, updating it to make it scalable, in case it already wasn't.

From there, work could follow several paths. On one hand we could experiment with other types of algorithms, looking for a greater efficiency. Proof of this is ``Baselines'', the OpenAI repository that we have already mentioned \citep{baselines}. Another option would be to go a step further and start interpreting images directly as input, so as not to depend on observations in the form of a variable as we have done so far.

DeepMind already demonstrated this a few years ago. In \citet{mnih2013playing} they proved that their AI was able to learn to play some Atari games at a professional level, all of this only through the pixels of each image. To achieve this we would have to specialize in Neural Convolutional Networks, which would be quite a challenge. Not only because they are more complex, but also because, since they need more resources when it comes to training, the system must be much more precise to be viable.

But every effort pays off: Once you have an AI capable of recognizing images, the possibilities soar. By not being tied to observations provided through variables, it is no longer necessary to rely on frameworks or tedious manual implementations to perform tests. One could work at a much more ``real'' level, in the sense that perhaps it would no longer be necessary to describe a problem perfectly, with all its physical laws, in order to work on it. Perhaps it would simply be necessary to provide the agent with a video so that it would be able to learn about it, understanding behaviors and patterns and learning in a much more humane way. However, this idea is still a long way off for us.
