\chapter{Conclusions and future work}
\label{cap:conclusions}

\chapterquote{We are what we do repeatedly. Excellence, then, is not an act, it is a habit}{Aristotle}

\section{Conclusions}

This project has served us as an extensive study about the origins and evolution of one of the most explored technologies within the Artificial Inteligence: Machine Learning, concretely the Deep Learning field.

When we started this project, we just had a slight idea about this term's complexity involvements or the utilities's scope that can be reached with this technology. But fortunately, our objective was clear and our research was aimed for a single goal: To make it possible for an AI to play a videogame on its own.

We began by studying the basics of Machine Learning, taking as our starting point the technique of Reinforcement Learning. This one fitted perfectly our project, since when it comes to videogames, it is easy to find rewards in that environment. Investigating about it, we discovered the Q-Learning method, which seemed to be the best suited to our case. It allowed us to make decisions at every moment, maintaining a reliable representation of any possible environment, in a way that the AI could play normally regardless of the game.

After the previous study, it was time to do our first tests and learn how to use the environment. We started with a simple game: CartPole, available in the OpenAI Gym library. The environment was simple and the actions to be taken by the agent were reduced. In spite of that, the model used in Q-Learning could give us a lot of problems in larger environments, even though the idea was good at first: The required size of the representation for all possible states grew almost exponentially every time we introduced new environment variables or we modified the existing ones. That is why we needed to go one step further. 

It was not easy to get used to Q-Learning. The concept was interesting and also simple, due to its similarity with a state machine. However, it was during the implementation when we started to notice the complexity that its use implies, even in an easy problem as it was the CartPole one. The fact of setting the problem representation was a challenge itself, since we were treating with continuous values. Multiple configurations were needed for discretizing these values, to see which one was the best for the algorithm.

As a first contact with the elements that will form our future researchs, in this case, everything related to the CartPole example on Q-Learning, we can conclude that the results were satisfying and not only because our algorithm was finally able to predict correctly the moves in the game. It served us to be prepared for the kind of errors we would find later, besides learning this method's functioning and creating a theoretical base, in order to know how to apply it on future environments.

Almost simultaneously, we started looking for other ways to develop our project, finally opting for Neural Networks. We verified with several examples that these were compatible with our problem, concluding that it was a good point to advance. Exploring beyond what we already knew about them, we found the perfect way to combine the idea and results of Q-Learning with the speed and comfort that Neural Networks provide: DQNs, or Deep Q-Networks.

Even though the concepts involving the DQNs' origination were already familiar for us, the very first results caught us by surprise. Our agent was not learning correctly and although we were expecting worse results than the Q-Learning implementation, for being our first attempt, we were not prepared for that bad. It was at this point when the research task took a crucial role for us, compeling us to get involved in it, until we realized our mistakes and the possible solutions to them.

Finally, thanks to the use of an additional memory, for our agent to learn from its own mistakes, and a second network, for stabilizing its results, we achieved a correct learning. From this elements, we understood that it did not matter the quantity of the available data or the number of episodes that the agent was learning. The essential was how significant and representative should out data be, so that we could learn something useful from them. In the same way, we understood that the fact of learning something new was not necessarily good for our agent. The second neural network was very helpful to ``guide'' the agent along the best learning way, accomplishing better results in few episodes, compared to previous tests.

The last step we reached in our project was to transfer everything we learned and accomplished in CartPole to another environment to check that the agent was still learning correctly. This time it was the MountainCar game, also available in the OpenAI Gym library. 

The main problem we had to face was the learning rate. After observing our agent's progress in this new environment, we realized that the reward was not giving us any helpful information for the model's learning; or, at least, not until the game goal was accomplished, which implied a very long lasting training.

Because of this fact, we decide to focus on changing the reward. After many different approaches and tests to modify it, we finally achieved a faster learning for our agent. Surprisingly, the simplest solutions were also the most effective ones. These leads us to conclude that we had managed to develop an AI capable of playing.

Maybe at first sight, choosing a research project, with an increasingly distant aim, does not sound very attractive. Nevertheless, this has been result way more profitable for us than focusing ourselves in complete a final product. This project opened us a door towards Deep Reinforcement Learning, including its component techniques and the progresses that continue to be made in this field.  

The research and time invested, has led us to understand the work involved in building an AI, even having a small initial base on which to stand. We have been able to experience personally the facilities of its use, applied to many projects, including ours. In addition to new knowledge, we have acquired new ideas and base on which build them.

Deep Reinforcement Learning is field still growing, on which much remains to be learned. But if we can say something for sure is that its evolution is not over yet and that, in a not too distant future, it will gain much more importance than it already has. And luckily, we will be prepared to follow its progress.

\section{Achievement of objectives}

At the beginning of this document, we proposed some objectives to explore during the course of this project, which have helped us to organize the work and its presentation in this document:

\begin{enumerate}
        \item The first objective was to understand what Reinforcement Learning is and how it differs from other branches of Machine Learning. Throughout the chapter \ref{cap:reinforcementLearning} we saw an explanation of this concept, the ideas on which it was based and their differences from other branches of Machine Learning, aswell as why we chose it as a starting point. Continuing with this idea, we explain all the elements that compose it and the way in which they relate to each other.
        \item Having seen how all the elements of Reinforcement Learning are related, we decided to test these concepts during the chapter \ref{cap:q-learning}, in which we explained the OpenAI Gym library. We chose the CartPole environment to carry out our experiments, managing to overtake the objective set for this environment and experimenting the problems involved in the implementation of this algorithm, in reference to the exponential increase of the Q-table and the complexity involved in keeping it completely updated, and achieving the correct learning of our agent.
        \item Another objective was to get into the field of Deep Learning and the fundamentals of Neural Networks. We discussed it throughout the chapter \ref{cap:deepLearning}, during which we explained the principles of Neural Networks, their composition, structure and behaviour. This way, we played with different problems of classification and regression that served us to have a first contact that later would help us to manage problems within the scope of Deep Reinforcement Learning.
        \item Our last goal was to combine Neural Networks and Reinforcement Learning, in a way that we could get the benefits of both, as well as finding solutions to their different issues, as we saw during the chapter \ref{cap:dqnEnAccion}. We studied step by step how to combine the two scopes, first replacing the Q-table of Reinforcement Learning with a Neuronal Network, which we saw that it didn't give our agent enough learning skills. After that, we introduced the concept of experience replay and the use of two networks, thanks to which we managed to stabilize learning and convergence.
        \item Having fulfilled all the objectives, and in view of the obtained results, we have been able to see the limitations of this field of study as of today, which is quickly evolving with new techniques every day. There definetly are lots of ways to follow if you are eager and interested in it.
\end{enumerate}

\section{Future work}

The project was aimed for teaching an AI to play and, although it has learned successfully, it has hardly been tested in a couple of environments with few variables. The path of our research could be the right one, but without further progress, or testing in other environments, we cannot be sure of that.

That said, if we could continue our project, we would focus on continuing with more tests on different games. After verifying that our AI is adaptable, we would start by adding more options to the possible actions that the player can perform, updating it to make it scalable, in case it already wasn't.

From there, work could follow several paths. On one hand we could experiment with other types of algorithms, looking for a greater efficiency. Proof of this is ``Baselines'', the OpenAI repository that we have already mentioned \citep{baselines}. Another option would be to go a step further and start interpreting images directly as input, so as not to depend on observations in the form of a variable as we have done so far.

DeepMind already demonstrated this a few years ago. In \citet{mnih2013playing} they proved that their AI was able to learn to play some Atari games at a professional level, all of this only through the pixels of each image. To achieve this we would have to specialize in Neural Convolutional Networks, which would be quite a challenge. Not only because they are more complex, but also because, since they need more resources when it comes to training, the system must be much more precise to be viable.

But every effort pays off: Once you have an AI capable of recognizing images, the possibilities soar. By not being tied to observations provided through variables, it is no longer necessary to rely on frameworks or tedious manual implementations to perform tests. One could work at a much more ``real'' level, in the sense that perhaps it would no longer be necessary to describe a problem perfectly, with all its physical laws, in order to work on it. Perhaps it would simply be necessary to provide the agent with a series of images so that it would be able to learn about them, understanding behaviors and patterns and learning in a much more humane way. However, this idea is still a long way off for us.
