\chapter{Conclusions and future work}
\label{cap:conclusions}

\chapterquote{We are what we do repeatedly. Excellence, then, is not an act, it is a habit}{Aristotle}

This project has served us as an extensive study about the origins and evolution of one of the most explored technologies within the Artificial Inteligence: Machine Learning, concretely the Deep Learning field.

When we started this project, we just had a slight idea about this term complexity involvements or the utilities's scope that can be reached with this technology. But fortunately, our goal was clear and our research was aimed for a single goal: to make it possible for an AI to play a videogame on its own.

We began by studying the basics of Machine Learning, taking as our starting point the technique of Reinforced Learning. This one fitted perfectly our project, since when it comes to videogames, it is easy to find rewards in that environment. Investigating about it, we discovered the Q-Learning method, which seemed to be the best suited to our case. It allowed us to make decisions at every moment, maintaining a reliable representation of any possible environment, in a way that the AI could play normally independently of the game.

After the previous study, it was time to do our first tests and learn how to use the environment. We started with a simple game: CartPole, available in the OpenAI Gym library. The environment was simple and the actions to be taken by the agent were reduced. The results were good, but we needed to go one step further. The model used in Q-Learning could give us a lot of problems in larger environments, even though the perspective was good at first.

Almost simultaneously, we started looking for other ways to develop our project, finally opting to use Neural Networks. We verified with several examples that these were compatible with our problem, arriving at the conclusion that it was a good point to advance. Exploring beyond what we already knew about them, we found the perfect way to combine the idea and results of Q-Learning with the speed and comfort that neural networks supposed: DQNs.

Testing DQNs was somehow more tedious to perform, as it was something totally new to us and required extra research time. The results were not good at the beginning, but after stabilizing them with a second network and adding a memory for them to learn from their own mistakes, we got our agent to learn correctly, giving us better results.

The last step we reached in our project was to transfer everything we learned and achieved in CartPole to another environment to check that the agent was still learning correctly. This time it was the MountainCar game, also available in the OpenAI Gym library. After several adaptations and some new minor components, we got successful results, so we concluded that we had managed to develop an AI capable of playing.

What has been achieved in this project has shown us the amount of possibilities that exist when it comes to solving a problem of this type. The research and time invested, has led us to understand the work involved in building an AI, even having a small initial base on which to stand, and the amount of information we still have to learn about this topic.

\section{Achievement of objectives}

At the beginning of this document, we proposed some objectives to explore during the course of this project, which have helped us to organize the work and its presentation in this document:

\begin{enumerate}
        \item The first objective was to understand what Reinforcement Learning is and how it differs from other branches of Machine Learning. Throughout the chapter \ref{cap:reinforcementLearning} we saw an explanation of this concept, as well as the ideas on which it was based and their differences from other branches of Machine Learning, aswell as why we chose it as a starting point. Continuing with this idea, we explain all the elements that compose it and the way in which they relate to each other.
        \item Having seen how all the elements of Reinforcement Learning are related, we decided to test these concepts during the chapter \ref{cap:q-learning}, in which we explained the OpenAI Gym library and chose the CartPole environment to carry out our experiments, managing to overtake the objective set for this environment and seeing the problems involved in the implementation of this algorithm, in reference to the exponential increase of the Q-table and the complexity involved in keeping it completely updated and achieving the correct learning of our agent.
        \item Another objective was to get into the field of Deep Learning and the fundamentals of Neural Networks, which we discussed throughout the chapter \ref{cap:deepLearning}, during which we explained the principles of Neural Networks, their composition, structure and behaviour. This way, we played with different problems of classification and regression that served us to have a first contact that later would help us to manage problems within the scope of Deep Reinforcement Learning.
        \item Our last goal was to combine Neural Networks and Reinforcement Learning, in a way that we could get the benefits of both, as well as finding solutions to their different issues, as we saw during the chapter \ref{cap:dqnEnAccion}. We studied step by step how to combine the two scopes, first replacing the Q-table of Reinforcement Learning with a Neuronal Network, which we saw that it didn't give our agent enough learning skills. After that we introduced the concept of experience replay and later the use of two networks, thanks to which we managed to stabilize learning and convergence.
        \item Having fulfilled all the objectives and in view of the results obtained, we have been able to see the limitations of this field of study as of today, which is quickly evolving with new techniques every day. There definetly are lots of ways to follow if you are eager and interested in it.
\end{enumerate}

\section{Future work}

The project was aimed for teaching an AI to play and, although it has learned successfully, it has hardly been tested in a couple of environments with few variables. The path of our research could be the right one, but without further progress, or testing in other environments, we cannot be sure of that.

That said, if we could continue our project, we would focus on continuing with more tests on different games. After verifying that our AI is adaptable, we would start by adding more options to the possible actions that the player can perform, updating it to make it scalable, in case it already wasn't.

=== REVISAR / AMPLIAR ===
As a last point to emphasize as far as future work is concerned, we believe that another thread to pull would be to get into the terrain of Convolutional Neural Networks and the recognition by images of what happens in each episode, with what we get to provide the ``vision'' agent with which to make decisions, and not only based on a handful of numbers but to actually see what happens and act accordingly. It remains as future work due to the greater complexity that these networks suppose.

