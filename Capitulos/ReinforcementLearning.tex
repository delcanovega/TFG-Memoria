\chapter{Reinforcement Learning}
\label{cap:reinforcementLearning}

En el que estudiaremos cómo un agente puede aprender a través del éxito y el fracaso, mediante recompensas y penalizaciones.

\chapterquote{El comportamiento es modelado y mantenido por sus consecuencias}{B. F. Skinner}

\section{Contexto}
La \textbf{inteligencia artificial} es una de las ramas de la computación que más interés genera en las personas, sean o no expertas en la informática. Dotar a una maquina de la capacidad de realizar funciones asociadas sólo al intelecto humano ha sido siempre considerado ciencia ficción. Pero, ¿cómo podemos definir la inteligencia artificial dejando de lado la ciencia ficción?

Es común hacer referencia al \textbf{Test de Turing}, propuesto por Alan Turing (1950) \todo{referencia}. En él una persona realiza un interrogatorio, el cual se considera superado si el interrogador no es capaz de discernir si las respuestas provienen de una máquina o una persona.

Intentar definir la inteligencia artificial nos lleva rápidamente a conceptos como el \textit{proceso del pensamiento} o el \textit{razonamiento}, los cuales terminan por conducir a otros más complejos como el \textit{comportamiento}. A partir de estas ideas podemos encontrar otras definiciones, clasificadas en la matriz~\ref{fig:tabla_IA}.

\figura{Bitmap/Capitulo2/tabla_IA}{width=1\textwidth}{fig:tabla_IA}%
       {Definiciones de inteligencia artificial}

En base a estas clasificaciones podríamos diferenciar dos corrientes de interpretación:
\begin{itemize}
    \item Una visión empírica (columna izquierda) con el ser humano como centro de la investigación.
    \item Una visión racionalista (columna derecha) que involucra una combinación de matemáticas e ingeniería.
\end{itemize}

Multitud de expertos han abordado ambos acercamientos de distintas formas. Nosotros nos centraremos en la visión empírica, buscando que nuestro \textit{agente} tome las ``decisiones correctas'' en función del conocimiento que posea.

\subsection{Aprendiendo a aprender}
Una de las funciones humanas de las que necesitaremos dotar a nuestra máquina en busca de este \textit{rendimiento ideal} es el \textbf{aprendizaje}.

El campo del \textbf{aprendizaje automático} (o \textit{machine learning}) se encarga de esta tarea, a través de la generalización y la búsquedas de patrones en experiencias pasadas. En función del tipo de \textit{feedback} obtenido existen diferentes técnicas de aprendizaje, diseñadas para distintos casos y objetivos:
\begin{itemize}
    \item \textbf{Aprendizaje supervisado} (\textit{supervised learning}): el agente observa una serie de ejemplos de entradas y salidas, aprendiendo una función que es capaz de asignar a una entrada su salida correspondiente. Se llama \textbf{supervisado} porque esta serie de ejemplos, llamado conjunto de entrenamiento, deberá estar correctamente clasificado en un primer momento (es decir, supervisado por un humano). Podría decirse que el agente aprende en base a estas experiencias, hasta que llegado un punto es capaz de clasificar una entrada completamente nueva. La exactitud de esta clasificación dependerá del entrenamiento recibido; más adelante en el proyecto haremos uso de esta técnica de aprendizaje.
    \item \textbf{Aprendizaje no supervisado} (\textit{unsupervised learning}): a diferencia del supervisado, en el aprendizaje no supervisado los ejemplos no cuentan con una etiqueta que los clasifica inequívocamente. En su lugar el agente busca patrones en el conjunto de entrada, intentando extraer características comunes de sus elementos. Uno de los usos más comunes del aprendizaje no supervisado es el \textbf{clustering}: la agrupación de ejemplos en grupos que comparten características comunes. Por ejemplo, la agrupación de películas con características similares, de modo que si a un usuario le resultan interesantes varias de un mismo \textit{cluster} es muy probable que también disfrute de otros elementos de esa misma agrupación – es decir, recomendadores.
    \item \textbf{Aprendizaje por refuerzo} (\textit{reinforcement learning}): el agente aprende a partir de una serie de refuerzos – recompensas si son positivos, o penalizaciones en caso de ser negativos. Por ejemplo, cuando una mascota cumple una orden y recibe una galleta como recompensa; o un músico desafina en directo y recibe un abucheo por parte del público. Esta técnica de aprendizaje será el punto de partida de nuestro proyecto, y explicaremos su funcionamiento a lo largo de este capítulo.
\end{itemize}

\section{¿Por qué aprendizaje por refuerzo?}
Como muchos otros campos pertenecientes a la rama de Inteligencia Artificial, el aprendizaje por refuerzo se inspira en estudios sobre el comportamiento en humanos y animales. De esta forma toma su base en la psicología conductivista, en la que un algoritmo decide si una acción tomada ha sido positiva o negativa, y la refuerza consecuentemente.

Pero antes de todo esto, ¿por qué es el Aprendizaje por refuerzo una opción interesante?

Echemos un vistazo a las técnicas de aprendizaje vistas en la sección anterior. ¿Qué es lo que necesita cada una de ellas para triunfar? Empecemos por el \textit{unsupervised learning}: este es quizá el caso más sencillo, ya que lo único que necesitaremos es algo de lo que vivimos rodeados, \textbf{información}. En internet disponemos de grandes cantidades de ella. Podemos servirnos de las APIs de servicios de música, series, entretenimiento, organismos públicos... Todo esto, en la mayoría de los casos, sin necesidad de tratar la información antes de alimentarla a nuestro algoritmo. Esto, por otra parte, es uno de los problemas del \textit{supervised learning}. Para poder usar información necesitamos que esté correctamente clasificada y, aunque hay conjuntos de pruebas maravillosos disponibles en internet, ante la necesidad de crear uno nuevo es esfuerzo requerido es muy grande.

El caso del \textit{reinforcement learning} es distinto. El agente no necesita grandes cantidades de casos de prueba, si no un entorno con el que interactuar por sí mismo, de esta forma los casos de prueba se generan de forma automática. La forma más rápida de hacernos con un entorno en el que soltar nuestro agente y verlo actuar es, como no, simulándolo. Una simulación es la forma más sencilla de controlar las condiciones y el progreso de nuestro experimento, además de quitarnos otros problemas del mundo real: variables que no se tuvieron en cuenta, cosas rompiendose... el mundo real.

Un dominio muy interesante sobre el que realizar simulaciones de Aprendizaje por refuerzo son los videojuegos. Durante los últimos años han tenido bastante éxito debido a varios motivos, entre ellos porque son un ejemplo muy fiel del proceso de aprendizaje en las personas: cuando alguien juega por primera vez a uno lo más probable es que no tenga buenos resultados, pero intento tras intento su habilidad mejora. Descubre información nueva, como que caer en un agujero del suelo puede finalizar la partida, o que ciertas acciones en el momento adecuado pueden hacerle salvar obstáculos. Esto es exactamente lo que buscamos simular. Queremos que cuando nuestro agente se encuentre ante un desafío, use sus experiencias pasadas para decidir cuál es la mejor decisión a tomar.

Pero hay mucho más. En la próximas secciones describiremos los distintos componentes de nuestros problemas, además de cuestiones más complejas: cómo definir la interacción entre agente y entorno, cómo representar el entorno de una forma eficiente o cómo discernir si una acción tomada ha sido buena o mala.

\section{Elementos del Reinforcement Learning}
A lo largo de este capítulo el término ``agente'' ha aparecido en repetidas ocasiones. Un \textbf{agente} no es más que el sujeto de nuestro experimento. Si nos encontrasemos en el mundo real un agente podría ser un ratón intentando salir de un laberinto. En nuestro contexto software, llamamos agente a un programa capaz de interactuar con el \textbf{entorno}, aprender de él y \textbf{tomar decisiones}.

El \textit{entorno} es el otro elemento fundamental del \textit{reinforcement learning}. El entorno proporciona información al agente, quien la usará para tomar una decisión. Esta decisión, en forma de acción, provocará cambios en el entorno, los cuales servirán para decidir la recompsa que el agente recibirá.

Más allá del agente y el entorno, existen cuatro subelementos en un sistema de aprendizaje por refuerzo: una \textit{política}, una \textit{recompensa}, una \textit{función de utilidad} y por último un \textit{modelo} del entorno.

La \textit{política} define la forma en la que el agente afronta un determinado problema. Por ejemplo, si nos encontrásemos a un enemigo en un videojuego una política válida sería enfrentarnos a él. Huir podría ser otra política perfectamente válida. Generalizando esto, la política es un mapeo entre el estado en que se encuentra el entorno y la acción que tomará el agente.

Es a través de la \textit{recompensa} como el agente obtiene \textit{feedback} del entorno. Después de cada acción tomada un valor numérico es enviado al agente. El objetivo del agente es maximizar la recompensa acumulada a lo largo de una simulación. A través de la recompensa se modela – y modifica de cara a acciones futuras – la política del agente.

Pero la recompensa no es suficiente para lograr un comportamiento óptimo por parte del agente. Es aquí donde entra en juego la \textit{función de utilidad}. Si nos paramos a pensarlo, existen acciones que no nos proporcionan una gran recompensa por si solas, pero facilitan el camino a otras que generan una recompensa aún mayor. En una partida de \textit{Tetris} podríamos apilar piezas a un lado mientras esperamos la ficha en forma de barra, completando así cuatro líneas en un solo movimiento y así obtener muchos más puntos que si hubiésemos completado una única línea cuatro veces seguidas. Desde un punto de vista más teórico podemos entender la utilidad como la recompensa máxima a la que se podría llegar desde un estado. Es decir, una recompensa a largo plazo.

Dependiendo del problema que afrontemos tendremos que ajustar la importancia que damos a la recompensa y a la utilidad, para que nuestro agente sea capaz de lograr buenos resultados.

Llegamos al último elemento por definir, el \textit{modelo}. Se encarga de simular cómo se comportaría el entorno el tomar una decisión. A través de él podemos \textbf{planificar}, es decir, decidir cuál de todas las acciones posibles nos beneficia más. No todos los problemas usan un modelo, también existen aquellos \textit{libres de modelo} en los cuales el agente depende únicamente en la prueba y error. 

\subsection{Problemas de decisión de Markov}

\section{Q-Learning}

\subsection{CartPole}

\section{Los límites del Reinforcement Learning}
 