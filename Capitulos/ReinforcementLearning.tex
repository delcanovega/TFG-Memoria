\chapter{Reinforcement Learning}
\label{cap:reinforcementLearning}

En el que estudiaremos cómo un agente puede aprender a través del éxito y el fracaso, mediante recompensas y penalizaciones.

\chapterquote{El comportamiento es modelado y mantenido por sus consecuencias}{B. F. Skinner}

\section{Contexto}
La \textbf{inteligencia artificial} es una de las ramas de la computación que más interés genera en las personas, sean o no expertas en la informática. Dotar a una maquina de la capacidad de realizar funciones asociadas sólo al intelecto humano ha sido siempre considerado ciencia ficción. Pero, ¿cómo podemos definir la inteligencia artificial dejando de lado la ciencia ficción?

Es común hacer referencia al \textbf{Test de Turing}, propuesto por Alan Turing (1950) \todo{referencia}. En él una persona realiza un interrogatorio, el cual se considera superado si el interrogador no es capaz de discernir si las respuestas provienen de una máquina o una persona.

Intentar definir la inteligencia artificial nos lleva rápidamente a conceptos como el \textit{proceso del pensamiento} o el \textit{razonamiento}, los cuales terminan por conducir a otros más complejos como el \textit{comportamiento}. A partir de estas ideas podemos encontrar otras definiciones, clasificadas en la matriz~\ref{fig:tabla_IA}.

\figura{Bitmap/Capitulo2/tabla_IA}{width=1\textwidth}{fig:tabla_IA}%
       {Definiciones de inteligencia artificial}

En base a estas clasificaciones podríamos diferenciar dos corrientes de interpretación:
\begin{itemize}
    \item Una visión empírica (columna izquierda) con el ser humano como centro de la investigación.
    \item Una visión racionalista (columna derecha) que involucra una combinación de matemáticas e ingeniería.
\end{itemize}

Multitud de expertos han abordado ambos acercamientos de distintas formas. Nosotros nos centraremos en la visión empírica, buscando que nuestro \textit{agente} tome las ``decisiones correctas'' en función del conocimiento que posea.

\subsection{Aprendiendo a aprender}
Una de las funciones humanas de las que necesitaremos dotar a nuestra máquina en busca de este \textit{rendimiento ideal} es el \textbf{aprendizaje}.

El campo del \textbf{aprendizaje automático} (o \textit{machine learning}) se encarga de esta tarea, a través de la generalización y la búsquedas de patrones en experiencias pasadas. En función del tipo de \textit{feedback} obtenido existen diferentes técnicas de aprendizaje, diseñadas para distintos casos y objetivos:
\begin{itemize}
    \item \textbf{Aprendizaje supervisado} (\textit{supervised learning}): el agente observa una serie de ejemplos de entradas y salidas, aprendiendo una función que es capaz de asignar a una entrada su salida correspondiente. Se llama \textbf{supervisado} porque esta serie de ejemplos, llamado conjunto de entrenamiento, deberá estar correctamente clasificado en un primer momento (es decir, supervisado por un humano). Podría decirse que el agente aprende en base a estas experiencias, hasta que llegado un punto es capaz de clasificar una entrada completamente nueva. La exactitud de esta clasificación dependerá del entrenamiento recibido; más adelante en el proyecto haremos uso de esta técnica de aprendizaje.
    \item \textbf{Aprendizaje no supervisado} (\textit{unsupervised learning}): a diferencia del supervisado, en el aprendizaje no supervisado los ejemplos no cuentan con una etiqueta que los clasifica inequívocamente. En su lugar el agente busca patrones en el conjunto de entrada, intentando extraer características comunes de sus elementos. Uno de los usos más comunes del aprendizaje no supervisado es el \textbf{clustering}: la agrupación de ejemplos en grupos que comparten características comunes. Por ejemplo, la agrupación de películas con características similares, de modo que si a un usuario le resultan interesantes varias de un mismo \textit{cluster} es muy probable que también disfrute de otros elementos de esa misma agrupación – es decir, recomendadores.
    \item \textbf{Aprendizaje por refuerzo} (\textit{reinforcement learning}): el agente aprende a partir de una serie de refuerzos – recompensas si son positivos, o penalizaciones en caso de ser negativos. Por ejemplo, cuando una mascota cumple una orden y recibe una galleta como recompensa; o un músico desafina en directo y recibe un abucheo por parte del público. Esta técnica de aprendizaje será el punto de partida de nuestro proyecto, y explicaremos su funcionamiento a lo largo de este capítulo.
\end{itemize}

\section{¿Por qué aprendizaje por refuerzo?}
Como muchos otros campos pertenecientes a la rama de Inteligencia Artificial, el aprendizaje por refuerzo se inspira en estudios sobre el comportamiento en humanos y animales. De esta forma toma su base en la psicología conductivista, en la que un algoritmo decide si una acción tomada ha sido positiva o negativa, y la refuerza consecuentemente.

Pero antes de todo esto, ¿por qué es el Aprendizaje por refuerzo una opción interesante?

Echemos un vistazo a las técnicas de aprendizaje vistas en la sección anterior. ¿Qué es lo que necesita cada una de ellas para triunfar? Empecemos por el \textit{unsupervised learning}: este es quizá el caso más sencillo, ya que lo único que necesitaremos es algo de lo que vivimos rodeados, \textbf{información}. En internet disponemos de grandes cantidades de ella. Podemos servirnos de las APIs de servicios de música, series, entretenimiento, organismos públicos... Todo esto, en la mayoría de los casos, sin necesidad de tratar la información antes de alimentarla a nuestro algoritmo. Esto, por otra parte, es uno de los problemas del \textit{supervised learning}. Para poder usar información necesitamos que esté correctamente clasificada y, aunque en internet hay conjuntos de entrenamiento de gran calidad y variedad, podría darse el caso de que ninguno se adapta a nuestra necesidades. En esta situación podríamos construir uno nosotros mismos, pero la tarea sería larga y tediosa, cuanto menos.

El caso del \textit{reinforcement learning} es distinto. El agente no necesita grandes cantidades de casos de prueba, si no un entorno con el que interactuar por sí mismo, de esta forma los casos de prueba se generan de forma automática. La forma más rápida de hacernos con un entorno en el que soltar nuestro agente y verlo actuar es, como no, simulándolo. Una simulación es la forma más sencilla de controlar las condiciones y el progreso de nuestro experimento, además de quitarnos otros problemas del mundo real: variables que no se tuvieron en cuenta, cosas rompiendose... el mundo real.

Un dominio muy interesante sobre el que realizar simulaciones de Aprendizaje por refuerzo son los videojuegos. Durante los últimos años han tenido bastante éxito debido a varios motivos, entre ellos porque son un ejemplo muy fiel del proceso de aprendizaje en las personas: cuando alguien juega por primera vez a uno lo más probable es que no tenga buenos resultados, pero intento tras intento su habilidad mejora. Descubre información nueva, como que caer en un agujero del suelo puede finalizar la partida, o que ciertas acciones en el momento adecuado pueden hacerle salvar obstáculos. Esto es exactamente lo que buscamos simular. Queremos que cuando nuestro agente se encuentre ante un desafío, use sus experiencias pasadas para decidir cuál es la mejor decisión a tomar.

Pero hay mucho más. En la próximas secciones describiremos los distintos componentes de nuestros problemas, además de cuestiones más complejas: cómo definir la interacción entre agente y entorno, cómo representar el entorno de una forma eficiente o cómo discernir si una acción tomada ha sido buena o mala.

\section{Elementos del Reinforcement Learning}
A lo largo de este capítulo el término ``agente'' ha aparecido en repetidas ocasiones. Un \textbf{agente} no es más que el sujeto de nuestro experimento. Si nos encontrasemos en el mundo real un agente podría ser un ratón intentando salir de un laberinto. En nuestro contexto software, llamamos agente a un programa capaz de interactuar con el \textbf{entorno}, aprender de él y \textbf{tomar decisiones}.

El \textit{entorno} es el otro elemento fundamental del \textit{reinforcement learning}. El entorno proporciona información al agente, quien la usará para tomar una decisión. Esta decisión, en forma de acción, provocará cambios en el entorno, los cuales servirán para decidir la recompsa que el agente recibirá.

Más allá del agente y el entorno, existen cuatro subelementos en un sistema de aprendizaje por refuerzo: una \textit{política}, una \textit{recompensa}, una \textit{función de utilidad} y por último un \textit{modelo} del entorno.

La \textit{política} define la forma en la que el agente afronta un determinado problema. Por ejemplo, si nos encontrásemos a un enemigo en un videojuego una política válida sería enfrentarnos a él. Huir podría ser otra política perfectamente válida. Generalizando esto, la política es un mapeo entre el estado en que se encuentra el entorno y la acción que tomará el agente.

Es a través de la \textit{recompensa} como el agente obtiene \textit{feedback} del entorno. Después de cada acción tomada un valor numérico es enviado al agente. El objetivo del agente es maximizar la recompensa acumulada a lo largo de una simulación. A través de la recompensa se modela – y modifica de cara a acciones futuras – la política del agente.

Pero la recompensa no es suficiente para lograr un comportamiento óptimo por parte del agente. Es aquí donde entra en juego la \textit{función de utilidad}. Si nos paramos a pensarlo, existen acciones que no nos proporcionan una gran recompensa por si solas, pero facilitan el camino a otras que generan una recompensa aún mayor. En una partida de \textit{Tetris} podríamos apilar piezas a un lado mientras esperamos la ficha en forma de barra, completando así cuatro líneas en un solo movimiento y obteniendo muchos más puntos que si hubiésemos completado una única línea cuatro veces seguidas. Desde un punto de vista más teórico podemos entender la utilidad como la recompensa máxima a la que se podría llegar desde un estado. Es decir, una recompensa a largo plazo.

Dependiendo del problema que afrontemos tendremos que ajustar la importancia que damos a la recompensa y a la utilidad, para que nuestro agente sea capaz de lograr buenos resultados.

Llegamos al último elemento por definir, el \textit{modelo}. Se encarga de simular cómo se comportaría el entorno el tomar una decisión. A través de él podemos \textbf{planificar}, es decir, decidir cuál de todas las acciones posibles nos beneficia más. No todos los problemas usan un modelo, también existen aquellos \textit{libres de modelo} en los cuales el agente depende únicamente en la prueba y error. 

\subsection{Problemas de decisión de Markov}

Entendemos como Proceso de Decisión de Markov (MDP) el método mediante el cual buscamos el resultado de la evaluación de un problema en el tiempo, es decir, un problema que requiere de distintos pasos secuenciales para su resolución, de modo que las acciones tomadas previamente tienen influencia en las decisiones que tomaremos a futuro para dicha resolución.

Teniendo en cuenta esta idea, podemos definir los elementos que compondrán nuestro MDP y nos ayuden a comprender mejor cómo funciona este modelo de toma de decisiones:

\begin{itemize}
    \item \textbf{S}: Conjunto de estados finitos en los que nos podemos encontrar en un determinado momento.
    \item \textbf{A}: Conjunto de acciones que pueden ser tomadas.
    \item \textbf{Modelo de transición}: Función que define el estado futuro (\textbf{s'}) basándose en el estado actual (\textbf{s}) y la acción tomada en dicho estado (\textbf{a}). El modelo de transición se representa mediante la función \textbf{P(s, a, s’)}.
    \item \textbf{Recompensa}: “Puntuación” positiva o negativa que se obtiene al tomar una acción en un determinado estado. Se representa mediante la función \textbf{R(r | s, a)}.
\end{itemize}

Pongamos el ejemplo de un laberinto. Nuestro conjunto de \textbf{estados} serían todas las posibles posiciones dentro del laberinto donde podemos situarnos, las \textbf{acciones} serían las direcciones en las que nos podemos mover en cada posición del laberinto, nuestro \textbf{modelo de transición} estaría definido de modo que si nos encontramos en una posición [x, y] y decidimos tomar la acción de movernos hacia arriba, nuestra siguiente posición (estado) sería [x, y+1], de modo que podríamos definir la  \textbf{recompensa} como una puntuación de +1 si consigo moverme a una posición buena dentro del laberinto, 0 si me choco contra una pared y +100 si consigo encontrar la salida.

\figura{Bitmap/Capitulo2/MDP-Maze}{width=0.7\textwidth}{fig:mdp-01}%
       {Representación de un MDP}

Visto este ejemplo, podemos generalizar la idea de un MDP como un \textbf{agente} que toma acciones respecto a un entorno y, en función de los estados futuros y las recompensas obtenidas, buscará maximizar su recompensa final.

\figura{Bitmap/Capitulo2/MDP-Diagram}{width=0.6\textwidth}{fig:mdp-02}%
       {Diagrama de un MDP}


Este hecho de que un agente tome una acción y haga cambiar el estado del entorno lo llamamos \textbf{episodio}. Un episodio está compuesto por el estado en el que el agente se encuentre, la acción tomada y la recompensa obtenida:

\begin{equation}
    episode = (state, action, reward)
\end{equation}

Los episodios se suceden hasta que el entorno llega a un estado de “finalizado”. Un entorno puede llegar a su estado “finalizado” si perdemos el juego (Game Over) o, si por el contrario, lo ganamos. Podemos diferenciar 2 tipos de entornos y sus diferentes objetivos para conseguir ganar:

\begin{itemize}
    \item \textbf{Entornos finitos}: Finalizan cuanto el agente obtiene una determinada recompensa, es decir, conocemos el estado final. Un ejemplo de entorno finito podría ser el ejemplo del laberinto antes planteado.
    \item \textbf{Entornos infinitos}: Son aquellos entornos de los que no tienen un estado “finalizado”, si no que simplemente solo puedes perder, por lo que el objetivo es aguantar el máximo tiempo posible, acumulando recompensas, antes de perder. Un ejemplo de entornos infinitos podría ser el Cart Pole.
\end{itemize}

Pero, ¿cómo sabemos o cómo decidimos que acción es mejor tomar en cada estado? Éste es el objetivo de nuestro MDP, el de encontrar una \textbf{política} óptima que nos ayude a tomar buenas decisiones acerca de qué acciones tomar en cada momento basándonos en el estado actual y conseguir maximizar nuestro “future return”.

Entendemos como \textbf{future return} las consecuencias futuras a largo plazo de las acciones tomadas por el agente. De este modo, el objetivo de nuestro MDP es el de optimizar su política para buscar obtener una mayor recompensa a la larga, aunque eso conlleve obtener recompensas inmediatas más pequeñas. Este concepto se conoce como \textbf{recompensas retrasadas}, ya que no las obtendremos instantáneamente tras ejecutar una acción, si no después de la sucesión de una serie de acciones.

Gracias a esto, conseguimos que nuestra política aprenda a pensar no solo en qué acción tomar ahora, si no en que acción tomar después de la misma. Un escollo que podemos encontrar de esta manera es el de que nuestra política piense demasiado a la larga y le tome infinitos episodios para encontrar una recompensa que le merezca la pena, de modo que nuestra recompensa futura diverge. Para evitar que esto suceda, introduciremos las llamadas penalizaciones.

Una \textbf{penalización} es un pequeño valor que se descuenta de la recompensa, con el fin de evitar que, como acabamos de ver, nuestra política tome demasiados episodios antes de encontrar una recompensa positiva. Teniendo esto en cuenta, podemos definir nuestra función de recompensa como:

\begin{equation}
    R_{t} = \sum^{T}_{k = 0} \gamma^{t}r_{t + k + 1}
\end{equation}

El factor de penalización, \( \gamma \), representa cuánto se descuenta de la recompensa en cada episodio, y su valor siempre se encuentra entre 0 y 1. Valores más altos corresponden a una penalización menor. Los factores de penalización más típicamente usados se encuentran entre 0,97 y 0,99.


\section{Q-Learning}
Hasta ahora hemos hablado de muchos conceptos y elementos: agente, entorno, recompensa, política... Llega el momento de aplicarlos de manera conjunta para conseguir una técnica de aprendizaje tangible. \textbf{Q-Learning} es una técnica de aprendizaje por refuerzo que tiene el objetivo de enseñar al agente qué acción es mejor tomar en cada circunstancia. Se trata de un algoritmo \textit{off-policy} con \textit{diferenciación temporal} que busca encontrar una función acción-utilidad que nos dirá cómo de útil es un estado. 

La primera etiqueta que hemos usado ha sido \textbf{off-policy}, ¿qué quiere decir esto?. Los algoritmos \textit{off-policy} son aquellos que predicen (o actualizan) sus valores-Q usando el valor-Q del estado futuro $s'$ y la acción $a'$ que más utilidad produzca. Es decir, simula un resultado voraz a pesar de que no se están tomando decisiones voraces. A diferencia de este modelo, el Q-Learning no necesita un modelo para aprender ya que se basa en la prueba y error mediante la ejecución de acciones en determinado estado, de modo que no aprendemos sobre una política, si no en base al valor de los pares acción-estado. Siendo $Q(s, a)$ el valor de aplicar una acción $a$ en el estado $s$, los valores devueltos por nuestra función-Q serán la utilidad máxima alcanzable de forma:

\begin{equation}
    U(s) = \max_{a}Q(s, a)
\end{equation}

Con \textbf{diferenciación temporal} hacemos referencia al hecho de que, en busca de un aprendizaje correcto, nuestro agente tendrá que tener en cuenta tanto las recompensas directas como aquellas que puede llegar con retardo, como anticipamos en la sección anterior. Reformulando esto, el agente deberá predecir el resultado de la recompensa final mediante la ejecucióno secuencial de distintas acciones. Para poder hacerlo tenemos que encontrar un equilibrio entre las recompensas inmediatas y las futuras. Como podemos imaginar, potenciar la importancia de las recompensas directas perjudica el valor de las futuras.

Este balance está muy relacionado con el problema de \textbf{Exploración vs. Explotación}. Nuestro agente contará con un hiperparámetro \textit{exploración}, el cual determina con qué frecuencia el agente decide tomar una acción aleatoria (exploración) en lugar de aquella que le proporcione el valor más prometedor (explotación), con la esperanza de encontrar un nuevo estado en el que nunca haya estado y el cual pueda resultar más beneficioso. En otras palabras, el agente decide si sacar provecho de lo ya aprendido, o intentar aprender más.

Existen varias fórmulas para calcular la utilidad de los estados. Uno de los métodos más utilizados es a través de la \textbf{Ecuación Bellman}, la cual nos indica que la máxima recompensa futura de tomar una acción es la suma de la recompensa actual y la máxima recompensa futura del siguiente episodio, permitiéndonos así obtener una relación entre valores-Q.

\begin{equation}
    Q^*(s_{t}, a_{t}) = E [ r_{t} + \gamma\ max_{a'}\ Q^*(s_{t+1}, a')]
\end{equation}

De este modo, podemos calcular el valor-Q para un par estado-acción, del mismo modo que con cada nuevo episodio podremos actualizar valores-Q previamente calculador con nueva información obtenida.

Q-Learning cuenta con tres hiperparámetros, dos de los cuales ya hemos visto: la \textit{exploración} y el \textit{factor de penalización} introducido al final de la sección anterior. El tercer hiperparámetro en cuestión corresponde a la \textbf{tasa de aprendizaje}. Este hiperparámetro, que toma valores entre 0 y 1, nos indica cuánta nueva información actualizamos sobre los valores-Q previamente calculados. De este modo, si nuestra tasa de aprendizaje es igual a 0, con cada nuevo episodio no se actualilzarán valores-Q con nueva información, y si es igual a 1, los valores previos se sobreescribirán por completo con la nueva información obtenida.

Llegados a este punto podemos observar como se crea una tabla-Q que relaciona los pares estado-acción con su correspondiente valor-Q, encontrándonos así que esta tabla crece alarmantemente rápido para problemas con un pequeño grado de complejidad y haciendo excesivamente costoso el hecho de visitar todas las experiencias recogidas, así como de actualizarlas, volviéndose el problema de esta forma inmanejable desde un punto de vista computacional.

\section{CartPole}
\begin{quote}
    Un mástil está unido por una bisagra a un carro, el cual se mueve a lo largo de una pista sin rozamiento. El sistema es controlado aplicando una fuerza de +1 o -1 al carro. EL péndulo (mástil) comienza en posición vertical, el objetivo es prevenir que caiga. Una recompensa de +1 es otorgada por cada \textit{timestep} que el mástil permanece erguido. El episodio finaliza cuando el mástil se encuentra a más de 15 grados de su posición vertical, o el carro se mueve más de 2.4 unidades del centro.
\end{quote}

Este es el enunciado del primer problema que resolveremos mediante técnicas de Reinforcement Learning. La implementación del problema nos viene dada como parte de la librería \textit{OpenAI Gym}. Una vez importado el problema, dispondremos de un entorno con el que interactuar y nuestro objetivo será crear un agente que interaccione con el mismo.

Nuestro conjunto de acciones, como se indica en el enunciado, tiene dos elementos: aplicar fuerza hacia la derecha o aplicar fuerza hacia la izquierda. El agente deberá decidir cuál de estas dos acciones tomar y se lo comunicará al entorno. Éste responderá con cuatro elementos:

\begin{itemize}
    \item \textbf{Observación} (objeto): es un array que contiene la información necesaria para describir el estado actual (tras aplicar la última acción). Los valores posibles pueden verse en la tabla \ref{obs-cartpole}.
    \item \textbf{Recompensa} (float): refuerzo para el agente, +1.0 mientras la simulación continúe.
    \item \textbf{Fin} (boolean): señal que indica que la simulación ha concluido, bien porque el mástil ha caído o porque lo hemos mantenido en pie durante el tiempo suficiente.
    \item \textbf{Info} (diccionario): información de diagnóstico útil para la depuración del agente. No obstante, esta información no debe ser usada por el agente para aprender.
\end{itemize}
\begin{table}[]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Num} & \textbf{Observación} & \textbf{Min} & \textbf{Max} \\ \hline
    0            & Posición del carro   & -4.8         & 4.8          \\ \hline
    1            & Velocidad del carro  & -Inf         & Inf          \\ \hline
    2            & Ángulo del poste     & -24º         & 24º          \\ \hline
    3            & Velocidad del poste  & -Inf         & Inf          \\ \hline
    \end{tabular}
    \caption{Observación del entorno para CartPole}
    \label{obs-cartpole}
\end{table}

Como podemos ver a través de las observaciones, este es uno de esos problemas en los que el número de estados distintos es potencialmente infinito, debido a que los valores recibidos son vectores. Por ello tendremos que encontrar una buena \textbf{función de discretización} que simplifique la representación del estado y nos permita trabajar con un número óptimo de estados.

Por otra parte, disponer de un número tan limitado de acciones posibles relaja la complejidad del problema. Al tratarse tan solo de dos opciones, si nuestro agente tomase acciones completamente aleatorias podría acertar en el 50\% de los casos.

\subsection{Discretizando el estado}
A la hora de discretizar una observación, una de las mejores formas de hacerlo es a través de una simplificación de la misma. Este acercamiento es una adaptación del \textit{Constraint Relaxation} que se aplica en muchos problemas de satisfacción de restricciones. De esta forma podemos comprender el problema desde un punto de partida mucho más sencillo, para después pulir la fórmula y tener en cuenta más variables. Además, podemos ir evaluando el rendimiento de nuestro agente a lo largo del camino, y así ver si las modificaciones que vamos añadiendo de verdad mejoran su comportamiento o simplemente añaden ruido.

Centrémonos por un momento en el objetivo más básico del problema: mantener el mástil en pie. Podemos dividir el problema en dos estados muy simples, uno en el que el mástil está cayendo hacia la derecha y otro en el cae hacia la izquierda, como podemos ver en la Figura \ref{fig:cartpole_01}. Esto coincide con nuestras dos acciones disponibles y, de forma algo ingenua, podemos considerar esto un punto de partida válido.

\figura{Bitmap/Capitulo2/cartpole_01}{width=0.7\textwidth}{fig:cartpole_01}%
       {Simplificación de estados}

Por supuesto esta discretización es demasiado ingenua para resolver el problema de una forma eficiente, es necesario probar otras divisiones de estados y tener en cuenta las demás observaciones. Para ello creamos una función parametrizable, la cual decide en cuantos estados dividir cada observación. Así podemos decidir a qué observaciones damos más importancia, creando más o menos estados en función a ellas, como podemos apreciar en la Figura \ref{fig:cartpole_02}.

\figura{Bitmap/Capitulo2/cartpole_02}{width=0.7\textwidth}{fig:cartpole_02}%
       {División de las observaciones}

Este aumento en el número de estados permite que nuestro agente sea capaz de especializarse más en acciones concretas. Pero esta mejora viene con un precio a pagar: cuantos más estados distintos tengamos, más tiempo tardará en visitarlos todos (en repetidas ocasiones) durante su etapa de entrenamiento. Esto quiere decir que nuestro agente tardará más en tener un buen rendimiento. Es importante entonces que evaluemos el problema al que nos enfrentamos para encontrar el equilibrio ideal y así poder ahorrar recursos (memoria y tiempo).

\subsection{Resultados}
\begin{quote}
    CartPole define solucionado a obtener una recompensa media de 195.0 durante 100 episodios consecutivos.
\end{quote}

Para realizar las mediciones, creamos una cola con una longitud máxima de 100 elementos. En ella añadimos el resultado de cada simulación, de forma que el resultado número 101 elimina el número 1. De esta forma podemos calcular la media sobre la estructura de datos y comprobar si el problema se consideraría resuelto por OpenAI Gym. Además dibujaremos esta media de la estructura tras cada episodio en una gráfica, con el fin de que los resultados sean más fáciles de apreciar y analizar. Las gráficas resultantes tienen variaciones tras cada ejecución, pero el caso más común es el mostrado en la Figura \ref{fig:cartpole_03}.

\figura{Bitmap/Capitulo2/cartpole_03}{width=0.7\textwidth}{fig:cartpole_03}%
       {Visualización de los resultados del agente RL}

En dicha gráfica se pueden apreciar tres etapas distintas en el rendimiento del agente:
\begin{itemize}
    \item Episodios 1-50: durante las primeras simulaciones el agente aún ha de rellenar la tabla-Q. Se encuentra ante situaciones nuevas ante las que aún no sabe como reaccionar. Además hay que tener en cuenta que su rendimiento se calcula en base a la media con los primeros resultados, por lo que en la etapa final de esta fase aún es lastrado por esos primeros episodios.
    \item Episodios 50-300: suponen el despegue en rendimiento del agente. En ellos se empieza a librar del lastre de las primeras simulaciones, además de que su tabla interna ya se encuentra en un estado bastante estable y es capaz de generar mejores resultados.
    \item A partir del episodio 300: el agente alcanza su meta y el algoritmo comienza a estabilizarse.
\end{itemize}

Cabe destacar que estos resultados han sido obtenidos con una configuración en la discretización de \texttt{(1, 1, 3, 6)}. ¿Por qué optamos por esta configuración? Mientras probábamos distintas configuraciones descubrimos que el principal motivo de finalización de una simulación era la caída del mástil. En la mayoría de las ocasiones el mástil cae mucho antes de que el carro alcance los límites laterales. Esto nos permite ignorar las dos primeras observaciones (posición del carro y velocidad del carro) reduciendo notablemente la dimensionalidad del problema.

De esta forma, contamos con un espacio dimensional bastante reducido ($1*1*3*6=18$ estados!). Si por el contrario hubiésemos optado por un número mayor de estados, la primera fase descrita arriba se alargaría considerablemente, pero a cambio podríamos esperar una mayor estabilidad del agente en la tercera fase.

\section{Los límites del Reinforcement Learning}
A lo largo de este capítulo hemos introducido el \textit{Reinforcement Learning} como método de aprendizaje, las ventajas que tiene sobre otros métodos de aprendizaje, sus mecánicas y elementos, algunos de sus algoritmos... Pero también hemos dejado entrever ciertos problemas.

El más importante es el que vimos durante la discretización del estado. Este problema a menudo es llamado \textit{The Curse of Dimensionality} y hace referencia al aumento exponencial en el tamaño de las tablas de estados en la memoria de los agentes. El problema puede ser contenido a través de buenas aproximaciones en la discretización, no obstante, un entorno continuo lo bastante complejo acaba resultando inviable de afrontar.

Otra solución ha surgido durante los últimos años y consiste en combinar el \textit{Reinforcement Learning} con técnicas de \textit{Deep Learning}, las cuales estudiaremos en los próximos capítulos.