\chapter{DQNs en acción}
\label{cap:dqnEnAccion}

\chapterquote{La vida sólo puede entenderse hacia atrás, pero debe vivirse hacia adelante}{Søren Kierkegaard}

\section{CartPole}
\label{sec:cartpoleDQN}

El primer problema que trataremos al trabajar con aprendizaje por refuerzo profundo se corresponde con el de CartPole, ya descrito en el apartado \ref{cartpole-sec}, en el que jugamos con un carro cuyo objetivo es moverse a derecha o izquierda para evitar que el poste vertical que hay sobre él se caiga. Todas las técnicas aplicadas para resolverlo han sido implementadas desde cero y su código se encuentra en el repositorio de nuestro proyecto.


\subsection{Reemplazar la tabla-Q: \texttt{SimpleAgent}}

Como primer paso para la resolución del problema, sustituimos la tabla-Q y su consecuente discretización de estados por una red neuronal como la de la figura \ref{fig:nn_cartpole}, abandonando por fin los problemas y limitaciones que habíamos observado anteriormente. La red recibe como entrada el \textbf{estado} actual (posición del carro, velocidad del carro, posición del péndulo y velocidad del péndulo) y cuenta con dos capas ocultas tipo \textit{ReLu}, de 24 neuronas. La última capa, de tipo \textit{Linear} produce dos salidas que son los valores Q aproximados por la red neuronal, asociados a cada una de las acciones disponibles: ejercer fuerza hacia la izquierda o hacia la derecha. Después de ejecutar cada movimiento, se aplica la ecuación de Bellman para entrenar la red e intentar que el agente aprenda a jugar. El optimizador que utilizamos es el Adam, debido a su popular uso en este tipo de problemas.

\figura{Bitmap/DQNEnAccion/nn_cartpole.png}{width=1\textwidth}{fig:nn_cartpole}%
       {Estructura de la red neuronal en \texttt{SimpleAgent}}

En el fragmento de código \ref{code:dqn} podemos ver que la implementación del algoritmo es muy similar a la usada en \ref{code:q-learning}, con unas modificaciones que describiremos a continuación.

\begin{minipage}{0.9\linewidth}%
\begin{lstlisting}[frame=tb, caption=Pseudocódigo SimpleAgent, inputencoding=latin1, label=code:dqn]      
  state = env.reset()

  while not done:
    action = agent.predict_action(state)
    next, reward, done = env.step(action)
    agent.learn(state, action, reward, next, done)
    state = next
        
\end{lstlisting}%
\end{minipage}

Para \textbf{predecir acciones} utilizamos la red neuronal, a la cual introducimos el estado actual para que nos devuelva los resultados de ambas acciones, de los que nos quedaremos con el mejor. Una vez ejecutada la acción disponemos del estado siguiente y la recompensa, lo cual podemos usar para \textbf{entrenar} la red neuronal de nuevo.

Como cabía esperar con una modificación tan ingenua, los resultados no son buenos, tal y como podemos apreciar en la imagen \ref{fig:CartPole-SimpleAgent}. El agente no logra aprender de una forma estable. Si miramos atrás, en nuestra solución con una tabla-Q, el único estado que es modificado tras un paso de simulación es aquel en el que nos encontramos. Por otra parte, cuando trabajamos con una red neuronal esto no es posible. Entrenar una red neuronal implica que toda la red se modificará, buscando satisfacer el nuevo ejemplo de entrenamiento proporcionado \citep[cap. 2]{Buduma:general}. En otras palabras, el agente mejora su rendimiento en el estado actual, pero al mismo tiempo empeora en todos los demás.

Los hiperparámetros usados también son una ligera modificación de los usados en la solución de aprendizaje por refuerzo. Como puede verse en \ref{code:cartpole_drl}, el mayor cambio es en la tasa de aprendizaje, la cual debe ser mucho menor para intentar evitar grandes modificaciones en la red.

\begin{minipage}{0.9\linewidth}%
    \begin{lstlisting}[frame=tb, language=Python, caption=Hiperparámetros, label=code:cartpole_drl]
    LEARNING_RATE = 0.0001  # Alpha
    DISCOUNT_FACTOR = 0.95  # Gamma
    EXPLORATION = 0.5       # Epsilon (initial)
    MIN_EXPLORATION = 0.01  # Epsilon (final)
    \end{lstlisting}%
\end{minipage}

\figura{Bitmap/DQNEnAccion/Results-SimpleAgent.png}{width=0.7\textwidth}{fig:CartPole-SimpleAgent}%
       {Resultados del \texttt{SimpleAgent}}

\subsection{Añadir memoria al agente: \texttt{BatchAgent}}
\label{sec:cartpoledqn2}

Una forma de conseguir estabilidad es añadir una memoria adicional al agente, en la que se almacenen las decisiones tomadas con los resultados obtenidos, de forma que a la hora de entrenar la red se puedan usar varias experiencias en lugar de sólo una. 
% Para hacerlo recurriremos al reproductor de experiencias (o \textit{Experience Replay} \citep{Lin1992}), también conocidas como valores-Q, al que nos referiremos como \textbf{replay}. Este método consiste en almacenar múltiples experiencias pasadas del agente en una memoria y para más tarde tomar una muestra del conjunto de experiencias, acumuladas a lo largo de varios episodios, para entrenar \ref{code:cartpole_drl2}.
En nuestra memoria guardamos todos los datos necesarios para la perfecta representación de lo sucedido en pasos de ejecución anteriores, lo que se traduce en: \textbf{estado-actual}, \textbf{acción-tomada}, \textbf{recompensa}, \textbf{estado-siguiente} y \textbf{finalizado}.

\begin{minipage}{0.9\linewidth}%
\begin{lstlisting}[frame=tb, caption=Pseudocódigo función Replay de BatchAgent, label=code:cartpole_drl2]
# Create a iterator to go through the memory 
# starting in the last position 
iter = reversed(memory)

for _ in range(BATCH_SIZE):
    state,action,reward,next_state,done = next(iter)

    if done:
       q_update = -reward
    else:
       q_update = bellman(reward, next_state)

    target = model.predict(state)
    target[action] = q_update

    model.fit(state, target)
              
\end{lstlisting}%
\end{minipage}

Tras la ejecución de cada acción, y con la información proporcionada por el entorno en ese momento, guardamos todos estos valores en la memoria para ser utilizados posteriormente. De esta forma contamos con un conjunto ($batch$) de los últimos $N$ pasos guardados, para que podamos entrenar la red neuronal con un mayor número de casos de prueba, intentando evitar así la inestabilidad del \texttt{SimpleAgent}.

El problema es que los pasos tomados dentro de estos $batch$ están fuertemente cohesionados entre sí, ya que al ser consecutivos la información aportada por cada uno de ellos es muy similar. La memoria de estados consecutivos provee de más información a la red pero las similitudes entre ellos provocan que el agente aprenda muy lentamente, de forma insuficiente para la resolución del problema, como se puede ver en la imagen \ref{fig:CartPole-BatchAgent}.

\figura{Bitmap/DQNEnAccion/Results-BatchAgent.png}{width=0.7\textwidth}{fig:CartPole-BatchAgent}%
       {Resultados del \texttt{BatchAgent}}


\subsection{Romper la cohesión de la memoria: \texttt{RandomBatchAgent}}
\label{sec:cartpoledqn3}

Para solucionar el problema de la cohesión en el \texttt{BatchAgent} tendremos que, en lugar de utilizar las últimas $N$ experiencias, elegir experiencias pasadas suficientemente diferentes para que el entrenamiento de la red no esté sesgado hacia un tipo de situación concreta. Tan sólo necesitaremos que la memoria que habíamos utilizado antes sea lo bastante grande como para albergar experiencias de varias partidas. Con este repertorio ya preparado, la mejor forma de romper la cohesión de los datos consiste en seleccionar elementos aleatorios dentro del mismo, a diferencia de tomar una muestra de elementos seguidos como habíamos hecho anteriormente. Con esta solución, el agente debería tener una muestra más amplia, variada y significativa respecto a los posibles estados en los que se puede encontrar y así rendirá mejor en la mayoría de las situaciones.
  
\begin{minipage}{0.9\linewidth}%
\begin{lstlisting}[frame=tb, caption=Pseudocódigo RandomBatchAgent, inputencoding=latin1, label=code:cartpole_drl3]
# We get a random sample of past experiences
batch = random.sample(self.memory, BATCH_SIZE)

for experience in batch:
  state,action,reward,next_state,done = experience

  # We calculate the new Q-value
  if done:
    q_update = -reward
  else:
    q_update = bellman(reward, next_state)

  # We update the Q-value of our action...
  target = model.predict(state)
  target[action] = q_update

  # ...and try to approximate it
  model.fit(state, target)
\end{lstlisting}%
\end{minipage}

El resultado es notoriamente mejor, como muestra la imagen \ref{fig:CartPole-RandomBatchAgent}. El agente consigue resolver el problema de forma rápida, además de conseguir puntuaciones mucho más altas en general. Sin embargo, entre unas simulaciones y otras los resultados son muy dispares, desde encontrar rápidamente y reforzar un camino ``bueno'' hasta lograr la resolución del problema, hasta no encontrar nunca un camino válido. En éste último caso puede deberse a que las muestras tomadas de la memoria no son lo suficiente variadas y el estado de la red neuronal se degrada progresivamente.

El problema del volumen de datos estaba solucionado, ya que en varias ocasiones el agente había conseguido aprender una solución, y en las que no, al menos daba la impresión de que iba por buen camino. No obstante, la volatilidad de los resultados nos da a entender que el modelo propuesto aún no estaba completo.

\figura{Bitmap/DQNEnAccion/Results-RandomBatchAgent.png}{width=0.7\textwidth}{fig:CartPole-RandomBatchAgent}%
       {Resultados del \texttt{RandomBatchAgent}}


\subsection{Estabilizar la red: \texttt{DoubleAgent}}
\label{sec:DA}

Con el fin de en estabilizar el aprendizaje, intentando evitar esos escollos que el agente puede experimentar en algunas ejecuciones, surge por parte de \textit{DeepMind} el concepto de la \textbf{Target Q-Network} \citep{NIPS2010_3964}. Éste implica la utilización de una segunda red objetivo (\textit{target}), como copia de la red principal de predicción. A través de la fórmula siguiente, podemos ver que la red objetivo ($Y$) sólo se actualiza cada cierto número de pasos ($t$ como paso actual), tomando el mejor resultado de la red de predicción junto con la siguiente recompensa ($R$), ahorrándose los cambios innecesarios en los pesos que ésta sufre constantemente y consiguiendo así más estabilidad en los resultados. 

$$Y^Q_{t} \equiv R_{t+1} + \gamma max_{a} Q(S_{t+1}, a; \theta_{t})$$

Como derivación de este método existe otro conocido como \textbf{Double Q-Network}, en esta, se cuenta con dos redes que llamaremos objetivo (\textit{target}) y agente (\textit{agent}). La red objetivo se utiliza para guardar los estados con los que tomar las decisiones en cuanto a los movimientos a ejecutar, mientras que la red de aprendizaje, agente, se encarga de guardar los valores de la ejecución actualizados. De la misma forma que antes, cada cierto número de pasos ($t$ como paso actual) ambas redes son comparadas y se guarda la que haya dado mejores resultados como objetivo ($Y$), junto con su siguiente recompensa ($R$), para la toma de decisiones, y se utiliza la otra para el aprendizaje.

$$Y^Q_{t} = R_{t+1} + \gamma Q(S_{t+1}, argmax_{a} Q(S_{t+1}, a; \theta_{t}); \theta_{t})$$

El principal inconveniente que mostraba nuestro agente hasta el momento se debía a la gran diferencia de resultados entre las ejecuciones, dando unos resultados que era necesario estabilizar. Tomando como referencia estas soluciones, optamos por el uso de dos redes diferentes.

\begin{minipage}{0.9\linewidth}%
\begin{lstlisting}[frame=tb, caption=Pseudocódigo DoubleAgent, inputencoding=latin1, label=code:cartpole_drl4]
  state = env.restart()

  while not done:
    action = target.predict_action(state)
    next, reward, done = env.step(action)

    agent.remember(state, action, reward, next, done)
    # Takes the N random memories and trains with them
    agent.learn_by_replay()

    state = next

  # If the agent's performance surpasses the target's
  if agent.performance() > target.performance():
    # the agent becomes the target to improve his solution
    target = agent
        
\end{lstlisting}%
\end{minipage}

Tras la implementación de este método, comprobamos que existe una mayor consistencia en los datos entre unas simulaciones y otras, obteniendo resultados menos dispares que con el entrenamiento por conjunto aleatorio, como podemos observar en la imagen \ref{fig:CartPole-DoubleAgent}.

\figura{Bitmap/DQNEnAccion/Results-DoubleAgent.png}{width=0.7\textwidth}{fig:CartPole-DoubleAgent}%
       {Resultados del \texttt{DoubleNetworkAgent}}

Gracias a este método se han logrado grandes hitos en el campo del aprendizaje por refuerzo profundo. Por ejemplo, hacer que una red neuronal sea capaz de jugar a juegos de Atari a nivel profesional utilizando tan solo los píxeles como entrada \citep{mnih2013playing}

Por otro lado, este método no es perfecto, en la imagen \ref{fig:CartPole-DoubleAgent} se puede ver una pequeña caída al final de la ejecución. Inicialmente achacamos este comportamiento a una desafortunada conjunción probabilística. Pero tras investigarlo, descubrimos que se trataba de un fenómeno mucho más cotidiano: el causante era el propio método de Deep Q-Learning. Se ha observado cómo algoritmos basados en Q-Learning que buscan aproximar la función Q con modelos no lineales, como una red neuronal, sufren de esta inestabilidad. A este problema nos enfrentaremos más a fondo en el siguiente capítulo, pero en el apartado de CartPole nos damos por satisfechos. 

\section{MountainCar}

Con el fin de comprobar la versatilidad de nuestro agente, lo pondremos a prueba en un entorno distinto. En este caso será el problema de MountainCar, descrito por primera vez por A. \citet{Moore90efficientmemory-based}, y con el siguiente enunciado en OpenAI:

\begin{quote}
    Un coche se encuentra en una pista unidimensional, posicionado entre dos "montañas". El objetivo es alcanzar la cima de la montaña derecha; sin embargo, el motor del coche no es lo bastante potente como para alcanzar la cima de la montaña con un único impulso. Por ello, la única forma de lograrlo es conducir hacia delante y hacia atrás aumentando el impulso.
\end{quote}

\figura{Bitmap/DQNEnAccion/MountainCar_01}{width=0.7\textwidth}{fig:mountaincar_01}%
       {Entorno de simulación MountainCar, \citet{brockman2016openai}}


\subsection{Especificación del problema}

\begin{itemize}
    \item \textbf{Estado:} el estado de este problema nos viene dado a través de la posición respecto al eje de abscisas y la velocidad del coche.
    \item \textbf{Recompensa:} la recompensa recibida será de \texttt{-1} por cada paso en el que el coche no alcance la meta.
    \item \textbf{Terminación:} el episodio terminará con éxito cuando el coche alcance la meta, o fracasará si no lo logra tras 200 pasos.
    \item \textbf{Acciones:} dispondremos de tres acciones disponibles, impulsarse hacia la derecha, impulsarse hacia la izquierda y no hacer nada. En cada situación el agente deberá elegir la opción que más impulso le ayude a acumular.
\end{itemize}

A primera vista puede parecer que es un problema muy parecido a CartPole; no obstante, tiene detalles que lo convierten en un problema especialmente interesante. En particular, su función de recompensa: recibir \texttt{-1} en cada paso, combinado con el hecho de que la simulación termine tras sólo 200 pasos si no se alcanza la meta, hace que prácticamente todas las simulaciones terminen con un resultado de -200. Esto elimina el factor de progresión del que disponíamos en CartPole, donde el agente generalmente iba obteniendo recompensas pequeñas, pero cada vez mayores.

Para resolver este problema partiremos de la versión más estable de nuestro agente desarrollado en \ref{sec:cartpoleDQN}.


\subsection{Enfoques de resolución}

Ejecutando el problema con el agente descrito en \ref{sec:DA} y los parámetros usados en \ref{code:cartpole_drl}, obtenemos como resultado que el agente no aprende nada en absoluto. La topología interna de la red se mantiene igual, adaptando el número de entradas y salidas a las del problema; CartPole describía su estado a través de cuatro variables y tenía dos acciones disponibles, mientras que MountainCar dispone de dos variables y tres acciones que elegir.

Los resultados tan pobres obtenidos puede que sorprendan, viendo el rendimiento demostrado en el problema de CartPole, pero recordemos que el tamaño de las capas internas de la red se eligió para ese problema en concreto. Es de esperar que distintos escenarios necesiten redes de distinto tamaño para que sean capaces de interpretar las observaciones de manera efectiva. En definitiva, nuestra red actual no es lo bastante descriptiva para la complejidad del problema.


\subsubsection{Nueva topología e hiperparámetros}

Tras modificar al agente para usar la topología de red visible en la imagen \ref{fig:nn_mountainCar}, utilizar los siguientes hiperparámetros  (Listing \ref{code:mountaincar_drl}) y dejar al agente aprendiendo durante un largo entrenamiento, conseguimos que llegue a ser capaz de resolver el problema. 

\figura{Bitmap/DQNEnAccion/nn_mountainCar}{width=0.7\textwidth}{fig:nn_mountainCar}%
       {Topología MountainCar}

\begin{minipage}{0.9\linewidth}%
    \begin{lstlisting}[frame=tb, language=Python, caption=Hiperparámetros MountainCar, label=code:mountaincar_drl]
    LEARNING_RATE = 0.0001  # Alpha
    DISCOUNT_FACTOR = 0.95  # Gamma
    EXPLORATION = 0.5       # Epsilon (initial)
    MIN_EXPLORATION = 0.01  # Epsilon (final)
    \end{lstlisting}%
\end{minipage}

\figura{Bitmap/DQNEnAccion/mountainCar_02}{width=0.7\textwidth}{fig:mountaincar_02}%
       {Resultados de la nueva topología}

La gráfica \ref{fig:mountaincar_02} representa las puntuaciones obtenidas durante nuestra simulación. En este punto de desarrollo, la puntuación representa la inversa de los pasos que ha tardado el avatar en llegar a la meta, dado que la recompensa que da el entorno para el estado objetivo es 0 y -1 a cualquier otro, la puntuación -200 implica que la meta no se ha alcanzado y la puntuación -199 implica que ha llegado en el paso 200.

No obstante, creemos que este resultado tiene un gran margen de mejora, por lo que seguiremos explorando otras soluciones que permitan al agente aprender de una forma más eficiente.


\subsubsection{Función de recompensa}

El primer obstáculo a superar para acelerar la curva de aprendizaje, es la función de la recompensa. La recompensa proporcionada por el entorno no es lo bastante útil para el agente. Podríamos entenderlo como el siguiente ejemplo: al hacer un examen, sin importar las veces que se intente o las respuestas que se den, si no está todo contestado de forma correcta la nota obtenida siempre es cero. Viéndolo así, resulta casi imposible identificar qué puntos en particular es necesario reforzar.

Nuestra solución, basada en la propuesta en \citep{mediumTim}, pasa por sustituir la recompensa dada por el entorno, reemplazando ese resultado binario de \textit{meta} o \textit{no meta} por algo más descriptivo, que empuje al agente a mejorar su funcionamiento. Utilizando la \textbf{velocidad} o la \textbf{posición} del agente como recompensa, motiva al mismo a tomar las decisiones que lo llevan a moverse más lejos, hasta que finalmente termine alcanzando la meta.

\figura{Bitmap/DQNEnAccion/mountainCar_03}{width=0.8\textwidth}{fig:mountaincar_03}%
       {Visualización de la recompensa modificada}

Fijándonos en la imagen \ref{fig:mountaincar_03}, podemos ver que usando una recompensa modificada favorecemos que el coche se mueva e intente llegar cada vez más lejos, a diferencia de la recompensa original con que el coche solía quedarse estancado en la parte más baja de la curva.

A continuación, evaluaremos en mayor detalle cada una de las opciones disponibles a la hora de reemplazar la recompensa, para ver cuál se adaptará mejor al problema y nos proporcionará mejores resultados.


\subsubsection*{Posición}

La posición es el primer candidato para utilizar como recompensa. Al fin y al cabo, si conseguimos que el coche avance lo suficiente, terminará llegando a la meta. Hicimos una serie de pruebas en las que directamente reemplazábamos el valor de la recompensa por el valor de la posición. Para ser capaces de apreciar en qué momento se alcanzaba la meta, le otorgamos una recompensa significativamente más grande cuando lo conseguía, inicialmente optamos por dar 4000 menos el numero de pasos que tarda en alcanzar la meta, con la esperanza de que esto reforzase mucho más ese comportamiento.

\figura{Bitmap/DQNEnAccion/mountaincar_04}{width=0.7\textwidth}{fig:mountaincar_04}%
       {Visualización de la recompensa en función de la posición}

Como podemos ver en la gráfica \ref{fig:mountaincar_04}, el rendimiento sigue siendo muy parecido al visto anteriormente en la imagen \ref{fig:mountaincar_02}. La repentina subida de puntuación en torno al episodio 1400, se traduce en que el coche ha conseguido alcanzar la meta, la cima de la montaña derecha. Por otro lado el punto más alto que llega a alcanzar la gráfica es cerca de 470, teniendo en cuenta que la recompensa por llegar a la meta es de 3800 a 4000, podemos deducir que solo ha logrado alcanzar la meta poco más de 10 veces en los últimos 100 episodios de la simulación. A partir de los nuevos resultados podemos hacer las siguientes observaciones:

\begin{itemize}
    % \item En primer lugar, dimos una recompensa exageradamente alta al alcanzar la meta, no solo para que ese camino se refuerce más que con la recompensa original, para acelerar el aprendizaje, sino para además permitirnos distinguir más fácilmente (en la gráfica) las partidas que llegan a la meta de las que no.  
    \item Nuestra nueva recompensa sólo empuja al agente a intentar escalar la rampa de la derecha, ya que es la dirección que hace crecer el valor de la \texttt{posición}. No obstante, para conseguir ascender por la derecha también necesita tomar impulso desde la izquierda, lo que se corresponde con valores negativos de la posición. Como solución, deberemos considerar usar una posición definida con valores absolutos para conseguir un mejor resultado.
    \item Otro detalle observado es que la posición central no se corresponde con \texttt{0}, sino con \texttt{-0.5}. También tendremos que corregir esta desviación.
    % \item Por último, el agente no recibe ningún estímulo que lo lleve a resolver el problema cuanto antes, por lo que volveremos a introducir una penalización por cada paso tomado, de forma similar a la recompensa original del entorno.
\end{itemize}

\figura{Bitmap/DQNEnAccion/mountaincar_05}{width=0.7\textwidth}{fig:mountaincar_05}%
       {Resultados finales de la posición como recompensa}
 
Teniendo en cuenta estos nuevos factores, los resultados obtenidos pueden verse en la gráfica \ref{fig:mountaincar_05}. Claramente, conseguimos que el problema se resuelva mucho más rápido: 400-500 episodios frente a los 1400 anteriores. Sin embargo la estabilidad aún sigue lejos de parecerse a la vista en el problema de CartPole. Siguen existiendo situaciones en las que el agente consigue resolver el problema en algunas partidas, pero luego vuelve a fracasar en las siguientes.
Por otro lado, cabe destacar que la gráfica \ref{fig:mountaincar_05} obtiene valores mucho más altos, en sus primeros episodios, que los de la gráfica \ref{fig:mountaincar_04}, esto se debe a que ahora la recompensa se basa en la \texttt{posición absoluta acumulada}, mientras que en la anterior se basaba en la posición acumulada, esto sumado a la desviación del centro, hace que gran parte del entorno tuviera posición negativa.


\subsubsection*{Velocidad}

Otra opción explorada es la de usar la velocidad como recompensa. Al fin y al cabo, cuanta más velocidad consiga acumular el coche, más alto llegará subiendo la rampa.

\figura{Bitmap/DQNEnAccion/mountaincar_06}{width=0.7\textwidth}{fig:mountaincar_06}%
       {Resultados finales de la velocidad como recompensa}

A diferencia de la posición, la velocidad devuelta por el entorno tiene unos valores muy pequeños, por lo que basar la recompensa en la velocidad recibida sería semejante a darle una recompensa constante. Cuando alcanzase la meta, sería necesario proporcionarle una notablemente mayor, lo que nos dejaría en una situación muy similar a la vista en \ref{fig:mountaincar_02}. Por ello decidimos multiplicarla por 100, haciéndola más visible y relevante para el agente. 

Como vemos en la imagen \ref{fig:mountaincar_06}, nos encontramos con un comportamiento muy similar al visto usando la posición en valores absolutos. El agente consigue resolver el problema en varias ocasiones pero no se estabiliza.

\subsubsection*{Otras pruebas}

Puesto que los resultados obtenidos por la posición y velocidad parecían haberse estancado, decidimos investigar otras piezas clave de nuestro modelo, como el optimizador. Empezamos a cuestionarnos si Adam \citep{kingma2014adam} era nuestra mejor opción, e hicimos algunas pruebas con Adadelta \citep{NIPS2017_7003}.

Para poder comparar lo mejor posible los resultados, decidimos crear una serie de gráficas para cada ejemplo, de forma que no sólo se apreciara mejor la evolución general de los resultados, sino que se añadieran otros datos de interés, como la recompensa original del entorno, la velocidad máxima, o la posición máxima alcanzada. Así seríamos capaces de controlar en una sola simulación todas las variables.

\figura{Bitmap/DQNEnAccion/mountainCar_08}{width=1\textwidth}{fig:mountaincar_08}%
       {Resultados velocidad Adam}

\figura{Bitmap/DQNEnAccion/mountainCar_09}{width=1\textwidth}{fig:mountaincar_09}%
       {Resultados velocidad Adadelta}

Se puede ver claramente en los resultados de Adadelta (figura \ref{fig:mountaincar_09}) consigue alcanzar el objetivo varias iteraciones antes que Adam (figura \ref{fig:mountaincar_09}), y ambas se mantienen casi totalmente estables durante los siguientes episodios.

%TODO: elegir imagen  y explicarla bien
La imagen cuenta con tres gráficas:
\begin{itemize}
    \item La primera muestra la evolución general del sistema en función de los episodios. Se pueden ver los resultados de las dos redes neuronales, la azul para agente y la cian para el aprendiz. La línea roja representa la recompensa real que devuelve el entorno. La negra muestra la recompensa media de las cien ultimas partidas. Así tenemos una observación general y fácil de interpretar.
    \item La gráfica inferior izquierda muestra la posición máxima alcanzada durante la partida. Esto nos permite ver lo cerca que se ha quedado el agente de alcanzar la meta.
    \item La gráfica inferior derecha representa las puntuaciones obtenidas por ambas redes en una simulación al final de la ejecución, con el fin de comprobar si realmente han aprendido. Esta simulación se realiza utilizando la recompensa original, simplificando su interpretación.  
\end{itemize}

Dado que seguíamos encontrando fluctuaciones en el aprendizaje de algunas ejecuciones y continuábamos en busca de una mayor estabilidad, decidimos añadir una tercera red neuronal que permitiera almacenar el mejor modelo encontrado hasta el momento, como una copia de seguridad, introduciendo así una especie de elitismo entre los episodios. El objetivo de dicho modelo es intervenir eventualmente para medir la progresión del aprendizaje del modelo y, en caso de producirse un empeoramiento en los resultados, imponer la configuración de la copia de seguridad; en caso contrario se guarda la nueva mejor configuración. 

\figura{Bitmap/DQNEnAccion/mountaincar_10}{width=1\textwidth}{fig:mountaincar_10}%
       {Resultado Final}

En la imagen \ref{fig:mountaincar_10} se puede ver cómo, una vez alcanza el máximo, el modelo se mantiene sin divergir y sin apenas fluctuar, no dista demasiado con respecto a los últimos prototipos, que ya mantenían un alto nivel de convergencia, por lo que podemos concluir que este último añadido, no aporta demasiado al modelo. 

