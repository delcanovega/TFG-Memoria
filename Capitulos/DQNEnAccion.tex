\chapter{DQNs en acción}
\label{cap:dqnEnAccion}

\chapterquote{La vida sólo puede entenderse hacia atrás, pero debe vivirse hacia adelante}{Søren Kierkegaard}

\section{CartPole}
\label{sec:cartpoleDQN}

\subsection{Reemplazar la tabla-Q: SimpleAgent}

El primer problema que trataremos al trabajar con DQNs se corresponde con el de CartPole, ya descrito en el punto \ref{cartpole-sec}, en el que jugamos con un carro, cuyo objetivo es moverse a derecha o izquierda para evitar que el poste vertical que hay sobre él se caiga.

Como primer paso para la resolución del problema, en este caso utilizando DQNs, se sustituyó la tabla-Q y su consecuente discretización de estados por una red neuronal simple, abandonando por fin los problemas y limitaciones que habíamos observado anteriormente. La red neuronal únicamente interaccionaba con el entorno y, después de ejecutar cada movimiento, se aplicaba la ecuación de Bellman para entrenar la red e intentar que el agente aprendiese a jugar.

El resultado fue nefasto, como podemos apreciar en la imagen \ref{fig:CartPole-SimpleAgent}. El agente no lograba aprender a jugar de ninguna manera, debido a la falta de información usada para el entrenamiento. Fue por ello que necesitábamos avanzar un paso más en pos de mejorar la eficiencia de nuestro agente.

\figura{Bitmap/DQNEnAccion/Results-SimpleAgent.png}{width=0.6\textwidth}{fig:CartPole-SimpleAgent}{Resultados del SimpleAgent}

\subsection{Añadir memoria al agente: BatchAgent}

Con la intención de proporcionar a la red neuronal más datos con los que entrenar, decidimos añadir una memoria, siguiendo la idea del método de \textit{replay} visto anteriormente. En ella guardamos todos los datos necesarios para la perfecta representación de lo sucedido en pasos de ejecución anteriores, lo que se traduce en: \textbf{estado-actual}, \textbf{acción-tomada}, \textbf{recompensa}, \textbf{estado-siguiente} y \textbf{finalizado}.

Tras la ejecución de cada acción, y con la información proporcionada por el entorno en ese momento, guardamos todos estos valores en la memoria para ser utilizados posteriormente. Al completarse un episodio, ya contábamos con un conjunto ($batch$) de los últimos n pasos guardados en la memoria y se entrenaba la red con dicho conjunto, con la intención de hacerle llegar un mayor volumen de información.

El problema observado en esta solución era la secuencia que formaban los pasos tomados para el entrenamiento, ya que al ser consecutivos la información aportada por cada uno de ellos era muy similar. Investigando una posible solución, nos dimos cuenta de que el problema se trataba de uno ya conocido que mencionamos en el capítulo anterior. La memoria de estados consecutivos proveía de más información a la red pero las similitudes entre ellos provocaban que el agente aprendiese muy lentamente, de forma insuficiente para la resolución del problema, tal y como se puede ver en la imagen \ref{fig:CartPole-BatchAgent}.

\figura{Bitmap/DQNEnAccion/Results-BatchAgent.png}{width=0.6\textwidth}{fig:CartPole-BatchAgent}{Resultados del BatchAgent}

\subsection{Romper la cohesión de la memoria: RandomBatchAgent}

Otra manera de conseguir estabilidad es reduciendo las similitudes entre los estados de los que aprende nuestro agente, de forma que no sean estados relacionados y puedan ser representativos de distintos momentos dentro de nuestra ejecución. Para hacerlo recurriremos al reproductor de experiencias (o \textit{Experience Replay}), también conocidas como valores-Q, al que nos referiremos como \textbf{replay}. Éste método consiste en almacenar múltiples experiencias pasadas del agente al finalizar cada episodio y tomar una muestra aleatoria del conjunto de experiencias acumuladas a lo largo de varios episodios. De esta forma conseguiremos casos no relacionados para optimizar y acelerar el aprendizaje de nuestro agente.

Debido a las limitaciones que que observamos tomando los últimos n pasos de la memoria, como si de una ventana se tratase, decidimos utilizar el método de \textit{replay} ya explicado. La memoria seguía conteniendo los n pasos consecutivos de las ejecuciones anteriores, pero el conjunto de datos seleccionado para el entrenamiento tomaba los pasos de manera aleatoria, provocando que no fueran consecutivos ni repetidos. Con esta solución, el agente debería tener una muestra más amplia respecto a los estados posibles y sería mucho más fácil que encontrase una solución con prontitud.

El resultado fue notoriamente mejor, como muestra la imagen \ref{fig:CartPole-RandomBatchAgent}. El agente consiguió incluso la resolución del problema y puntuaciones mucho más altas en algunas simulaciones. Sin embargo, entre unas simulaciones y otras los resultados eran muy dispares, desde encontrar rápidamente y reforzar un camino ``bueno" hasta lograr la resolución del problema, hasta no encontrar nunca un camino válido. En éste último caso se debía principalmente a que el agente no lograba aprender lo suficiente en el número de episodios que le permitíamos.

El problema del volumen de datos estaba solucionado ya que en varias ocasiones el agente había conseguido aprender una solución, y en las que no, al menos daba la impresión de que iba por buen camino. No obstante, la volatilidad de los resultados nos daba a entender que el modelo propuesto aún no estaba completo.

\figura{Bitmap/DQNEnAccion/Results-RandomBatchAgent.png}{width=0.6\textwidth}{fig:CartPole-RandomBatchAgent}{Resultados del RandomBatchAgent}

\subsection{Estabilizar la red: DoubleAgent}

Con el fin de en estabilizar la evolución del aprendizaje de la red y evitar que repita los mismos caminos constantemente, o que ignore soluciones por ser demasiado similares, surge el concepto de la \textbf{Target Q-Network}. Éste implica la utilización de una segunda red objetivo (\textit{target}), como copia de la red principal de predicción. La red objetivo sólo se actualiza cada cierto número de pasos, ahorrándose los cambios en los pesos que sufre constantemente la red de predicción y consiguiendo así más estabilidad en los resultados.

$$Y^Q_{t} \equiv R_{t+1} + \gamma max_{a} Q(S_{t+1}, a; \theta_{t})$$

Como derivación de este método existe otro conocido como \textbf{Double Q-Network}, donde las redes se dividen entre "agente" y "aprendizaje". La red agente se utiliza para guardar los estados con los que tomar las decisiones en cuanto a los movimientos a ejecutar, mientras que la red de aprendizaje se encarga de guardar los valores de la ejecución actualizados: cada cierto número de pasos ambas redes son comparadas, guardando la que haya dado mejores resultados como agente, para la toma de decisiones, y utilizando la otra para el aprendizaje.

$$Y^Q_{t} = R_{t+1} + \gamma Q(S_{t+1}, argmax_{a} Q(S_{t+1}, a; \theta_{t}); \theta_{t})$$

El principal inconveniente que mostraba nuestro agente hasta el momento se debía a la gran diferencia entre las ejecuciones, dando unos resultados que era necesario estabilizar. Tomando como referencia las soluciones consideradas en el capítulo anterior, optamos por el uso de dos redes diferentes. 

Tras la implementación de este método, comprobamos que lográbamos una mayor consistencia en los datos entre unas simulaciones y otras, obteniendo resultados menos dispares que con el entrenamiento por conjunto aleatorio como podemos observar en la imagen \ref{fig:CartPole-DoubleAgent}.

\figura{Bitmap/DQNEnAccion/Results-DoubleAgent.png}{width=0.6\textwidth}{fig:CartPole-DoubleAgent}{Resultados del DoubleNetworkAgent}

\section{MountainCar}

\begin{quote}
    Un coche se encuentra en una pista unidimensional, posicionado entre dos "montañas". El objetivo es alcanzar la cima de la montaña derecha; sin embargo, el motor del coche no es lo bastante potente como para alcanzar la cima de la montaña con un único impulso. Por ello, la única forma de lograrlo es conducir hacia delante y hacia atrás aumentando el impulso.
\end{quote}

\figura{Bitmap/DQNEnAccion/MountainCar_01}{width=0.7\textwidth}{fig:mountaincar_01}%
       {Entorno de simulación MountainCar, \citet{brockman2016openai}}

Este problema fue descrito por primera vez por A. \citet{Moore90efficientmemory-based}.

\subsection{Especificación del problema}

\begin{itemize}
    \item \textbf{Estado:} el estado de este problema nos viene dado a través de dos observaciones: la posición respecto al eje de abscisas y la velocidad del coche.
    \item \textbf{Recompensa:} la recompensa recibida será de \texttt{-1} por cada paso en el que el coche no alcance la meta.
    \item \textbf{Terminación:} el episodio terminará con éxito cuando el coche alcance la meta, o fracasará si no lo logra tras 200 pasos.
    \item \textbf{Acciones:} dispondremos de tres acciones disponibles, impulsarse hacia la derecha, impulsarse hacia la izquierda y no hacer nada. El cada situación el agente deberá elegir la opción que más impulso le ayude a acumular.
\end{itemize}

Se puede apreciar que las características del problema son muy similares a las vistas en CartPole. Por ello para resolver este problema partiremos de la versión más estable de nuestro agente desarrollado en \ref{sec:cartpoleDQN}.


\subsection{Enfoques de resolución}

Ejecutando el problema con el agente descrito en [INSERTAR REF] y los parámetros usados en [INSERTAR REF], obtendremos unos resultados como los mostrados en la figura [INSERTAR REF]. La topología de la red también se mantiene igual, por supuesto adaptando el número de entradas y salidas a las del problema; ya que CartPole describía su estado a través de cuatro observaciones y tenía dos acciones disponibles, frente a las dos observaciones y tres acciones de las que MountainCar dispone.

Los resultados tan pobres obtenidos puede que sorprendan viniendo del rendimiento visto en CartPole, pero recordemos que el tamaño de las capas internas de la red se eligió para ese problema en concreto. Es de esperar que distintos escenarios necesiten redes de distinto tamaño que sean capaces de interpretar las observaciones de manera efectiva. En definitiva, nuestra red actual \textbf{no es lo bastante descriptiva} para la complejidad del problema.

\subsubsection*{Nueva topología e hiperparámetros}
Modificando el agente para usar la siguiente red e hiperparámetros:

((Imagen de la topología))

$$ alpha = 0.0001$$
$$ gamma = 0.95$$
$$ epsilon = 0.5$$
$$ Hiden layer size = 24$$
$$ Minibatch size = 32 $$

Obteniendo resultados... 

((Grafica de resultados(laFuerzaBrutaSiempreGana.png)))

\subsubsection*{Función de recompensa}
Observando la gráfica es apreciable que las modificaciones no son suficiente para recibir los resultados deseados.
Durante nuestra investigación de las distintas posibilidades de acelerar la curva de aprendizaje, dimos con algunas propuestas interesantes que porponía \href{ https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2}{sustituir la recompensa devuelta por el entorno} por una que fuera más significativa para el aprendizaje del agente. En esta nos inspiramos para desarrollar nuestros prototipos.
Como ya explicamos antes, el entorno devuelve -1 si no alcanza la meta y en el caso contrario 0 y termina la ejecución. 
Por un lado, esta recompensa guía al jugador a completar el objetivo con la mayor brevedad posible, cuanto antes complete el objetivo, menos negativa será su recompensa final.
Pero por otro lado devuelve recompensas negativas hasta que alcanza el objetivo final, que se traduce en probar descartando caminos hasta alcanzar uno que logre el objetivo. Esto en teoría no suena tan mal, de no ser que puede tardar varias horas en encontrar uno de los caminos correctos. 
//Fuerza bruta 
Por ello decidimos orientarnos a una recompensa no binaria, alejándonos del meta o no-meta y basándonos en parámetros como la posición o la velocidad, para así tener una medida para evaluar no solo los episodios acertados sino también de los fallidos. Esto permitiría distinguir entre una ejecución en la que coche no se mueve y una en la que el coche casi llega a alcanzar la meta, mientras que con el método de Recompensa Binario serían indistinguibles. De esta forma el sistema tendrá acceso a recompensas de las que pueda aprender para llegar antes de lograr el camino a la meta. 

//Recompensa envi vs modificado (centro, lejos)
//Acumulados envi vs modificado

Con este pensamiento en mente comenzamos a desarrollar las distintas posibilidades. A continuación, hablaremos de cada uno de los prototipos desarrollados acompañados de las consiguientes reflexiones que desembocaron.



\paragraph{Posición}
 Decidimos empezar por investigar las posibilidades de basar la recompensa en la posición del coche.
 Dado que el estado final se basa en la posición del coche, nos pareció la opción más razonable por la que iniciar nuestra investigación. Nuestro primer prototipo basaba enteramente la función de recompensa en la posición tal y como la devuelve el entorno. 

 RESULTADO:mainPosicionNoAbs

Visto la poca diferencia entre los resultados de esta y la original, nos dimos cuenta de que hacía falta hacer una serie de ajustes:

\begin{itemize}
    \item En primer lugar, dimos una recompensa exageradamente alta al alcanzar la meta, no solo para que ese camino se refuerce más que con la recompensa original, para acelerar el aprendizaje, sino para además permitirnos distinguir más fácilmente (en la gráfica) las partidas que llegan a la meta de las que no.  
    \item Después, nos percatamos de que en el planteamiento de Recompensa Binaria no se hace distinción entre derecha e izquierda, por lo que consideramos hacer los cálculos con la posición en valor absoluto, para así no dar preferencia a la derecha frente a la izquierda con la intención de favorecer el balanceo.
     Para poder llevar a cabo esto, necesitábamos fijar el centro, el cual, según el entorno, es -0.5, por lo tanto, debíamos sumarle 0.5 para corregir la desviación.
    \item Por otro lado pensamos en darle una penalización en función del tiempo para dirigir el aprendizaje a una mayor presteza al igual que hacia la Recompensa Binaria. Pero esto último lo descartamos al ver los resultados.
\end{itemize}
 RESULTADO: CapturaCambiandoRewardAPosicion.png

La grafica representa claramente como la puntuación converge a un valor, esto se debe a que la penalización crece demasiado.
Penalización inicial empieza en 1, sin penalización, y a cada Step esta se multiplica por 0.995, de forma que la penalización del último Step es de 0.36695. Esto hace que las puntuaciones de los últimos Steps de la partida sean despreciables para el sistema, de tal forma que se optimicen solo las recompensas iniciales e ignorando las de los últimos. Así llegando a la convergencia. 
Además, nos dimos cuenta de que este tipo de penalización incumplía lo establecido de basar la recompensa únicamente en el estado, sin tener en cuenta factores externos como, en el caso de la penalización, el número de Steps que lleva la partida.
Ante los inconvenientes de la penalización optamos por descartar ese método y en su lugar añadir una pequeña disminución de la recompensa, en caso de alcanzar la meta, según el número de pasos realizados al final del Episodio para reforzar las soluciones rápidas frente a las lentas.

RESULTADOS: mainPosicion

\paragraph{Velocidad}
Por otro lado, dado que para alcanzar la meta en el juego hace falta coger impulso para tener suficiente fuerza como para subir la montaña, decidimos abrir otra línea de investigación basada en la velocidad absoluta.

RESULTADO: mainVelocidadTalcualNoAbs

A diferencia de la posición, la velocidad devuelta por el entorno tiene unos valores muy pequeños, por lo que basar la recompensa en la velocidad bruta sería semejante a darle una recompensa constante, para cuando se alcance la meta darle una notablemente mayor, lo cual se asemeja a la Recompensa Binaria. Por ello decidimos multiplicarla por 100, haciéndola más visibles y relevantes para el agente. 

Se puede ver como alcanza el meta más rápido, pero sufre caídas debido al overfitting, aun siendo caídas menores que las de posición nos parecía preocupante, por lo que probamos a cambiar la función de optimización de Adam a Adadelta.

RESULTADO: main3.2 y  

Comprendimos que la razón es que Adadelta funcionara mejor era que 
https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
Estos resultados nos parecieron suficientemente satisfactorios para este apartado.

Para poder comparar lo mejor posible los resultamos decidimos crear una serie de gráficas para cada ejemplo en el que no solo se viera mejor la evolución general de los resultados, sino que añadiera otros datos de interés como la recompensa original del entorno, la velocidad máxima la posición máxima alcanzada, así seríamos capaces de ver más claramente la evolución de los distintos modelos y permitirnos compararlos en estos campos más fácilmente.
Además, facilitando el avistamiento de partidas “buenas” podíamos permitirnos bajar la recompensa "exagerada" que le dábamos al alcanzar la meta.

RESULTADO :main3.2.GraficaMejorada
RESULTADO :mainPosicion.GraficaMejorada

Por último, decidimos añadir al agente una modificación para favorecer la convergencia de los resultados del sistema y reducir las caídas vistas en las gráficas.
Esto lo conseguimos añadiendo un tercer modelo al agente, siendo su composición final de:
\begin{itemize}
 \item Modelo Mentor: se limitará a responder a los estímulos del entorno sin hacer replay de sus jugadas, es decir, sin aprender y simplemente almacenando en la memoria los estados por los que ha pasado y sus correspondientes recompensas.
 \item Modelo Aprendiz: se dedicará a aprender, haciendo los replays de la batería de estados-recompensa que va generando el Modelo Mentor.
 \item Modelo Best: será el mejor de los modelos encontrados hasta el momento. 
\end{itemize}
El desarrollo de la ejecución se divide en 3 fases:

Primera fase, fase Inicial, durante los primeros episodios, equivalente a dos veces la periodicidad de las competiciones (COMP), el modelo mentor aprende del entorno tal y como hacen los agentes predecesores, jugando la partida y aprendiendo mediante backtracking. Mientras que los otros dos, Aprendiz y Best no hacen nada, se limitan a inicializarse.

La segunda fase, fase de Competiciones, Mentor se limita a interactuar con el entorno, es decir, toma las decisiones que controlan al avatar durante la partida, y guarda los resultados obtenidos en la memoria. Por otro lado, Aprendiz comienza a entrenar con dicha memoria, aprendiendo de las acciones de Mentor.
Cada COMP episodios, se produce una competición, que somete a Mentor y Aprendiz a una prueba, que no tiene repercusión alguna en la memoria y permite comparar los resultados para ver quien reacciona mejor en el entorno. La que salga victoriosa se copia sobre la vencida, y prosigue el proceso.
Así, si Aprendiz aprende de las experiencias de Mentor, y es capaz de superarle en la prueba, pasará a ser Mentor, dejando una copia en Aprendiz para seguir desarrollándose. Por otro lado, si Aprendiz aprende “mal”, es decir, si los resultados de la prueba indican que Mentor es mejor que Aprendiz, Mentor trasmitirá su configuración de los pesos a Aprendiz para que este proceda a mejorarlo y aprenda de Mentor hasta la siguiente Competición.  

Fase final, fase de Mantenimiento, es semejante a Competición, salvo que ahora Best guarda el mejor modelo que se haya generado durante la simulación. Cada 100 episodios, los tres modelos se pondrán a prueba con 100 partidas de testeo, que al igual que durante las competiciones normales, no tendrán ninguna repercusión en la memoria y por tanto serán invisibles para el entrenamiento. El mejor modelo se guardará en Best y se pondrá como Mentor, para así generar los mejores resultados posibles de los que Aprendiz pueda aprender.  

RESULTADOS: 	main7SustituyendoModelPorBestEXT, main7SustituyendoModelPorBestEXT2, main7SustituyendoModelPorBestEXT4

\paragraph{Variantes}
Nos planteamos otros aderezos para las soluciones ya planteadas para optimizar el aprendizaje. 
Consideramos añadir pequeñas recompensas como dar una bonificación extra al alcanzar cierta posición o al llevar una velocidad positiva.
Algunas de estas fueron descartadas por la premisa de solo basarnos en el estado para definir la recompensa, pero otras pudimos llevarlas a cabo.

 Dar bonus +10 cada vez que alcanza un multiplo de 0.1

 Dar bonus al llevar velocidad positiva al pasar por 0

 Dar bonus al superar su velocidad máxima al pasar por cero(se dedicaba a pasar muchas veces por cero)

 Dar una bonificación cada vez que supera mu mayor altura por 0.1, a primera vista puede parecer que de elementos externos al estado pero... teniendo en cuenta que se trata de solo posiciones de 0.1, al final solo se recompensan 0, 0.1, 0.2... sería equivalente la primera propuesta pero alternando la ladera. 

((algoritmo de modificacion))

((Gráfica de resultados))

\subsubsection*{Otros enfoques}
Hablar sobre randomwalks y transfer learning

\paragraph{Randomwalks}
A grandes rasgos Randomwalks, consiste en generar una gran memoria de partidas jugadas totalmente al azar para luego seleccionar los mejores resultados de entre estos para entrenar el agente. Por último, poner el agente en el entorno real.
Este planteamiento ha resultado ser con diferencia el más rapido de todos, dado que al ahorarse el entrenamiento intermedio de todas las partidas se ahora gran parte del tiempo de ejecución.


