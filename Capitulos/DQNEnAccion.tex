\chapter{DQNs en acción}
\label{cap:dqnEnAccion}

\chapterquote{La vida no es un problema por resolver, sino una realidad que experimentar}{Søren Kierkegaard}

\section{CartPole}
\label{sec:cartpoleDQN}

\subsection{Reemplazar la tabla-Q (SimpleAgent)}

El primer problema que tratamos al trabajar con DQNs se corresponde al CartPole ya descrito en el punto \ref{cartpole-sec}, en el que jugamos con un carro que tiene un poste sobre él y su objetivo es moverse para así evitar que el poste se caiga.

Como primer paso para la resolución del probelma utilizando DQNs, se sustituyó la taba-Q y su consecuente discretización de estados (con los probemas y limitaciones que ésto traía) por una red neuronal simple, la cual únicamente interaccionaba con el entorno y, después de ejecutar cada movimiento, aplicaba la ecuación de Bellman para entrenar la red e intentar que el agente aprendiese a jugar.

El resultado fue nefasto como podemos apreciar en la imagen \ref{fig:CartPole-SimpleAgent}, ya que la red no lograba aprender a jugar de ninguna manera debido a la falta de información usada para el entrenamiento.

\figura{Bitmap/Capitulo5/Results-SimpleAgent.png}{width=0.6\textwidth}{fig:CartPole-SimpleAgent}{Resultados del SimpleAgent}

\subsection{Añadir memoria al agente (BatchAgent)}

Con la intención de proporcionar a la red neuronal más información con la que entrenar, añadimos una memoria en la cual guardar todos los datos necesarios para la perfecta representación de lo sucedido en cada paso de ejecución.

De este modo, guardamos información referente al \textbf{estado-actual}, \textbf{acción-tomada}, \textbf{recompensa}, \textbf{estado-siguiente} y \textbf{finalizado}.

Así, después de la ejecución de cada acción y con la información proporcionada por el entorno en este acto, guardamos todos estos valores en la memoria para posteriormente ser utilizados en el entrenamiento de la red.

Siguiendo este proceso y con toda la información guardada, al completarse un episodio se cogia un conjunto ($batch$) de los últimos n pasos guardados en la memoria y se entrenaba la red con dicho conjunto, con la intención de hacerle llegar una mayor información.

El problema encontrado con esta solución era la consecución de los pasos tomados para el entrenamiento, ya que al ser consecutivos la información aportada por cada uno de ellos era muy similar, lo que conllevaba que la red aprendiese, pero no lo suficiente para la resolución del problema.

\subsection{Romper la cohesión de la memoria (RandomBatchAgent)}

Debido a las limitaciones que tenia el entrenamiento cogiendo los últimos n pasos de la memoria como si de una ventana se tratase, decidimos cambiar el conjunto de datos proporcionado en el entrenamiento de la red de forma que los distintos pasos selelccionados no fueran consecutivos, si no seleccionados de manera aleatoria y no repetidos entre todos los guardados en la memoria, evitando así la falta de información que la consecución de pasos conllevaba.

El resultado fue notoriamente mejor como muestra la imagen \ref{fig:CartPole-RandomBatchAgent}, incluso logrando en algunas simulaciones la resolución del problema y consiguiendo puntuaciones más altas, pero aun así entre unas simulaciones y otras los resultados eran muy dispares, de modo que en algunas de ellas encontraba un camino bueno y lo reforzaba hasta el punto de lograr la resolución del problema, como que en otras de no encontrarse ese buen camino, la red no lograba aprender o suficiente, obteniendo mucha volatibilidad en los resultados y un mejor aprendizaje.

\figura{Bitmap/Capitulo5/Results-RandomBatchAgent.png}{width=0.6\textwidth}{fig:CartPole-RandomBatchAgent}{Resultados del RandomBatchAgent}

\subsection{Estabilizar la red}

Lograda la resolución del problema, pero aún con la gran diferencia de los resultados entre distintas simulaciones, y siguiendo las soluciones consideradas en el capítulo anterior vamos a optar por el uso de dos redes diferentes. Con esto esperamos conseguir estabilizar los resultados obtenidos.

Tras la implementación de este método, comprobamos que lográbamos una mayor consistencia en los datos entre unas simulaciones y otras, obteniendo resultados menos dispares que con el entrenamiento por conjunto aleatorio como podemos observar en la imagen \ref{fig:CartPole-DoubleAgent}.

\figura{Bitmap/Capitulo5/Results-DoubleAgent.png}{width=0.6\textwidth}{fig:CartPole-DoubleAgent}{Resultados del DoubleNetworkAgent}

\section{MountainCar}

\begin{quote}
    Un coche se encuentra en una pista unidimensional, posicionado entre dos "montañas". El objetivo es alcanzar la cima de la montaña derecha; sin embargo, el motor del coche no es lo bastante potente como para alcanzar la cima de la montaña con un único impulso. Por ello, la única forma de lograrlo es conducir hacia delante y hacia atrás aumentando el impulso.
\end{quote}

\figura{Bitmap/Capitulo5/MountainCar_01}{width=0.7\textwidth}{fig:mountaincar_01}%
       {Entorno de simulación MountainCar, \citet{brockman2016openai}}

Este problema fue descrito por primera vez por A. \citet{Moore90efficientmemory-based}.

\subsection{Especificación del problema}

\begin{itemize}
    \item \textbf{Estado:} el estado de este problema nos viene dado a través de dos observaciones: la posición respecto al eje de abscisas y la velocidad del coche.
    \item \textbf{Recompensa:} la recompensa recibida será de \texttt{-1} por cada paso en el que el coche no alcance la meta.
    \item \textbf{Terminación:} el episodio terminará con éxito cuando el coche alcance la meta, o fracasará si no lo logra tras 200 pasos.
    \item \textbf{Acciones:} dispondremos de tres acciones disponibles, impulsarse hacia la derecha, impulsarse hacia la izquierda y no hacer nada. El cada situación el agente deberá elegir la opción que más impulso le ayude a acumular.
\end{itemize}

Se puede apreciar que las características del problema son muy similares a las vistas en CartPole. Por ello para resolver este problema partiremos de la versión más estable de nuestro agente desarrollado en \ref{sec:cartpoleDQN}.


\subsection{Enfoques de resolución}

Ejecutando el problema con el agente descrito en [INSERTAR REF] y los parámetros usados en [INSERTAR REF], obtendremos unos resultados como los mostrados en la figura [INSERTAR REF]. La topología de la red también se mantiene igual, por supuesto adaptando el número de entradas y salidas a las del problema; ya que CartPole describía su estado a través de cuatro observaciones y tenía dos acciones disponibles, frente a las dos observaciones y tres acciones de las que MountainCar dispone.

Los resultados tan pobres obtenidos puede que sorprendan viniendo del rendimiento visto en CartPole, pero recordemos que el tamaño de las capas internas de la red se eligió para ese problema en concreto. Es de esperar que distintos escenarios necesiten redes de distinto tamaño que sean capaces de interpretar las observaciones de manera efectiva. En definitiva, nuestra red actual \textbf{no es lo bastante descriptiva} para la complejidad del problema.

\subsubsection*{Nueva topología e hiperparámetros}
Modificando el agente para usar la siguiente red e hiperparámetros:

((Imagen de la topología))

$$ alpha = 0.0001$$
$$ gamma = 0.95$$
$$ epsilon = 0.5$$
$$ Hiden layer size = 24$$
$$ Minibatch size = 32 $$

Obteniendo resultados... 

((Grafica de resultados(laFuerzaBrutaSiempreGana.png)))

\subsubsection*{Función de recompensa}
Como pudimos ver estas modificaciones no eran suficiente para recibir los resultados deseados.
Tras investigar las distintas posibilidades para acelerar la curva de aprendizaje, pusimos en el punto de mira la función de recompensa. 

Como ya explicamos antes, el entorno devuelve -1 si no alcanza la meta y en el caso contrario 0 y termina la ejecución. 
Por un lado esta recompensa guiar al jugador a completar el objetivo con la mayor brevedad posible, cuanto antes complete el objetivo menos baja será su recompensa final.
Pero por otro lado devuelve recompensas negativas hasta que alcanza el objetivo final, que se traduce en probar descartando caminos hasta alcanzar uno que logre el objetivo. Esto en teoría no suena tan mal, de no ser que puede tardar varias horas en encontrar uno de los caminos correctos. Por ello decidimos orientarnos a una recompensa no binaria, alejándonos del meta o no-meta y basándonos en parámetros como la posición o la velocidad, para así tener una medida para evaluar no solo los episodios acertados sino también de los fallidos. Esto permitiría distinguir entre una ejecución en la que coche no se mueve y una en la que el coche casi llega a alcanzar la meta, mientras que con el método de Recompensa Binario serian indistinguibles. De esta forma el sistema tendrá acceso a recompensas de las que pueda aprender para llegar antes de lograr el camino a la meta. 

Con este pensamiento en mente comenzamos a desarrollar las distintas posibilidades. A continuación hablaremos de cada uno de los prototipos desarrollados acompañados de las consiguientes reflexiones que desembocaron.
 
\paragraph{Posición}
Inicialmente, dado que el estado final se basa en la posición del coche, empezamos a desarrollar un prototipo cuya función de recompensa se basará en dicha posición, por ello decidimos que fuera "tal cual" la que devuelve el entorno. 

 RESULTADO:mainPosicionNoAbs

Visto la poca diferencia entre los resultados entre esta y la original, nos dimos cuenta de que hacía falta hacer una serie de ajustes:

\begin{itemize}
    \item En primer lugar, dar una recompensa exageradamente alta al alcanzar la meta, no solo para que ese camino se refuerce más que con la recompensa original, para acelerar el aprendizaje, sino para además permitirnos distinguir más fácilmente (en la gráfica) las partidas que llegan a la meta de las que no.  
    \item Después, nos percatamos de que en el planteamiento de Recompensa Binaria no se hace distinción entre derecha y izquierda, por lo que consideramos hacer los cálculos con la posición en valor absoluto, para así no dar preferencia a la derecha frente a la izquierda con la intencion de favorecer el balanceo.
     Para poder llevar a cabo esto, necesitábamos fijar el centro, el cual, según el entorno, es -0.5, por lo tanto debíamos sumarle 0.5 para corregir la desviación.
    \item Por otro lado pensamos en darle una penalización en función del tiempo para dirigir el aprendizaje a una mayor presteza al igual que hacia la Recompensa Binaria. Pero esto último lo descartamos al ver los resultados.
\end{itemize}
 RESULTADO: CapturaCambiandoRewardAPosicion.png

La grafica representa claramente como la puntuación converge a un valor, esto se debe a que la penalización crece demasiado.
Penealizacion inicial empieza en 1, sin penalización, y a cada Step esta se multiplica por 0.995, de forma que la penalización del último Step es de 0.36695. Esto hace que las puntuaciones de los últimos Steps de la partida sean despreciables para el sistema, de tal forma que se optimicen solo las recompensas iniciales y ignorando las de los ultimos. Así llegando a la convergencia. 
Además nos dimos cuenta de que este tipo de penalización incumplía lo establecido de basar la recompensa únicamente en el estado, sin tener en cuenta factores externos como, en el caso de la penalización, el número de Steps que lleva la partida.
Ante los inconvenientes de la penalización optamos por descartar ese método y en su lugar añadir una pequeña disminución de la recompensa, en caso de alcanzar la meta, según el número de pasos realizados al final del Episodio para reforzar las soluciones rápidas frente a las lentas.

RESULTADOS: mainPosicion

\paragraph{Velocidad}
Por otro lado, dado que para alcanzar la meta en el juego hace falta coger impulso para tener suficiente velocidad como para subir la montaña, decidimos abrir otra línea de investigación basada en la velocidad absoluta.

RESULTADO: mainVelocidadTalcualNoAbs

A diferencia de la posición, la velocidad devuelta por el entorno tiene unos valores muy pequeños, por lo que basar la recompensa en la velocidad bruta sería semejante a darle una recompensa constante, para cuando se alcance la meta darle una notablemente mayor, lo cual se asemeja a la Recompensa Binaria. Por ello decidimos multiplicarla por 100, haciéndola más visibles y relevantes para la recompensa. 

Se puede ver como alcanza la meta más rápido, sufre caídas debido al overfitting, aun siendo caídas menores que las de posición nos parecía preocupante, por lo que probamos a cambiar la función de optimización de Adam a Adadelta.

RESULTADO: main3.2 y  

Estos resultados nos parecieson suficientemente satisfactorios para este apartado.

\paragraph{Variantes}
Nos planteamos otras aderezos para las soluciones ya planteadas para optimizar el aprendizaje. 
Consideramos añadir pequeñas recompensas como dar una bonificacion extra al alcanzar cierta posición o al llevar una velocidad positiva.
Algunas de estas fueron descartadas por la premisa de solo basarnos en el estado para definir la recompensa, pero otras pudimos llevarlas a cabo.

 Dar bonus +10 cada vez que alcanza un multiplo de 0.1

 Dar bonus al llevar velocidad positiva al pasar por 0

 Dar bonus al superar su velocidad maxima al pasar por cero(se dedicaba a pasar muchas veces por cero)

 Dar una bonificacion cada vez que supera mu mayor altura por 0.1, a primera vista puede parecer que de elementos externos al estado pero... teniendo en cuenta que se trata de solo posiciones de 0.1 , al final solo se recompensan 0, 0.1, 0.2... seria equivalente la primera propuesta pero alternando la ladera. 


((algoritmo de modificacion))

((Gráfica de resultados))

\subsubsection*{Otros enfoques}
Hablar sobre randomwalks y transfer learning

\paragraph{Randomwalks}
A grandes rasgos Randomwalks, consiste en generar una gran memoria de partidas jugadas totalmente al azar para luego seleccionar los mejores resultados de entre estos para entrenar el agente. Por último, poner el agente en el entorno real.
Este planteamiento ha resultado ser con diferencia el más rapido de todos, dado que al ahorarse el entrenamiento intermedio de todas las partidas se ahora gran parte del tiempo de ejecución.

