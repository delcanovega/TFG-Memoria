\chapter{DQNs en acción}
\label{cap:dqnEnAccion}

\chapterquote{La vida no es un problema por resolver, sino una realidad que experimentar}{Søren Kierkegaard}

\section{CartPole}
\label{sec:cartpoleDQN}

\subsection{Reemplazar la tabla-Q (SimpleAgent)}

El primer problema que tratamos al trabajar con DQNs se corresponde al CartPole ya descrito en el punto \ref{cartpole-sec}, en el que jugamos con un carro que tiene un poste sobre él y su objetivo es moverse para así evitar que el poste se caiga.

Como primer paso para la resolución del probelma utilizando DQNs, se sustituyó la taba-Q y su consecuente discretización de estados (con los probemas y limitaciones que ésto traía) por una red neuronal simple, la cual únicamente interaccionaba con el entorno y, después de ejecutar cada movimiento, aplicaba la ecuación de Bellman para entrenar la red e intentar que el agente aprendiese a jugar.

El resultado fue nefasto como podemos apreciar en la imagen \ref{fig:CartPole-SimpleAgent}, ya que la red no lograba aprender a jugar de ninguna manera debido a la falta de información usada para el entrenamiento.

\figura{Bitmap/Capitulo5/Results-SimpleAgent.png}{width=0.6\textwidth}{fig:CartPole-SimpleAgent}{Resultados del SimpleAgent}

\subsection{Añadir memoria al agente (BatchAgent)}

Con la intención de proporcionar a la red neuronal más información con la que entrenar, añadimos una memoria en la cual guardar todos los datos necesarios para la perfecta representación de lo sucedido en cada paso de ejecución.

De este modo, guardamos información referente al \textbf{estado-actual}, \textbf{acción-tomada}, \textbf{recompensa}, \textbf{estado-siguiente} y \textbf{finalizado}.

Así, después de la ejecución de cada acción y con la información proporcionada por el entorno en este acto, guardamos todos estos valores en la memoria para posteriormente ser utilizados en el entrenamiento de la red.

Siguiendo este proceso y con toda la información guardada, al completarse un episodio se cogia un conjunto ($batch$) de los últimos n pasos guardados en la memoria y se entrenaba la red con dicho conjunto, con la intención de hacerle llegar una mayor información.

El problema encontrado con esta solución era la consecución de los pasos tomados para el entrenamiento, ya que al ser consecutivos la información aportada por cada uno de ellos era muy similar, lo que conllevaba que la red aprendiese, pero no lo suficiente para la resolución del problema.

\subsection{Romper la cohesión de la memoria (RandomBatchAgent)}

Debido a las limitaciones que tenia el entrenamiento cogiendo los últimos n pasos de la memoria como si de una ventana se tratase, decidimos cambiar el conjunto de datos proporcionado en el entrenamiento de la red de forma que los distintos pasos selelccionados no fueran consecutivos, si no seleccionados de manera aleatoria y no repetidos entre todos los guardados en la memoria, evitando así la falta de información que la consecución de pasos conllevaba.

El resultado fue notoriamente mejor como muestra la imagen \ref{fig:CartPole-RandomBatchAgent}, incluso logrando en algunas simulaciones la resolución del problema y consiguiendo puntuaciones más altas, pero aun así entre unas simulaciones y otras los resultados eran muy dispares, de modo que en allgunas de ellas encontraba un camino bueno y lo reforzaba hasta el punto de lograr la resolución del problema, como que en otras de no encontrarse ese buen camino, la red no lograba aprender o suficiente, obteniendo mucha volatibilidad en los resultados y un mejor aprendizaje.

\figura{Bitmap/Capitulo5/Results-RandomBatchAgent.png}{width=0.6\textwidth}{fig:CartPole-RandomBatchAgent}{Resultados del RandomBatchAgent}

\subsection{Estabilizar la red}

Lograda la resolución del problema, pero aún con la gran diferencia de los resultados entre distintas simulaciones, y siguiendo las soluciones consideradas en el capítulo anterior vamos a optar por el uso de dos redes diferentes. Con esto esperamos conseguir estabilizar los resultados obtenidos.

Tras la implementación de este método, comprobamos que lográbamos una mayor consistencia en los datos entre unas simulaciones y otras, obteniendo resultados menos dispares que con el entrenamiento por conjunto aleatorio como podemos observar en la imagen \ref{fig:CartPole-DoubleAgent}.

\figura{Bitmap/Capitulo5/Results-DoubleAgent.png}{width=0.6\textwidth}{fig:CartPole-DoubleAgent}{Resultados del DoubleNetworkAgent}

\section{MountainCar}

\begin{quote}
    Un coche se encuentra en una pista unidimensional, posicionado entre dos "montañas". El objetivo es alcanzar la cima de la montaña derecha; sin embargo, el motor del coche no es lo bastante potente como para alcanzar la cima de la montaña con un único impulso. Por ello, la única forma de lograrlo es conducir hacia delante y hacia atrás aumentando el impulso.
\end{quote}

\figura{Bitmap/Capitulo5/MountainCar_01}{width=0.7\textwidth}{fig:mountaincar_01}%
       {Entorno de simulación MountainCar, \citet{brockman2016openai}}

Este problema fue descrito por primera vez por A. \citet{Moore90efficientmemory-based}.

\subsection{Especificación del problema}

\begin{itemize}
    \item \textbf{Estado:} el estado de este problema nos viene dado a través de dos observaciones: la posición respecto al eje de abscisas y la velocidad del coche.
    \item \textbf{Recompensa:} la recompensa recibida será de \texttt{-1} por cada paso en el que el coche no alcance la meta.
    \item \textbf{Terminación:} el episodio terminará con éxito cuando el coche alcance la meta, o fracasará si no lo logra tras 200 pasos.
    \item \textbf{Acciones:} dispondremos de tres acciones disponibles, impulsarse hacia la derecha, impulsarse hacia la izquierda y no hacer nada. El cada situación el agente deberá elegir la opción que más impulso le ayude a acumular.
\end{itemize}

Se puede apreciar que las características del problema son muy similares a las vistas en CartPole. Por ello para resolver este problema partiremos de la versión más estable de nuestro agente desarrollado en \ref{sec:cartpoleDQN}.


\subsection{Enfoques de resolución}

Ejecutando el problema con el agente descrito en [INSERTAR REF] y los parámetros usados en [INSERTAR REF], obtendremos unos resultados como los mostrados en la figura [INSERTAR REF]. La topología de la red también se mantiene igual, por supuesto adaptando el número de entradas y salidas a las del problema; ya que CartPole describía su estado a través de cuatro observaciones y tenía dos acciones disponibles, frente a las dos observaciones y tres acciones de las que MountainCar dispone.

Los resultados tan pobres obtenidos puede que sorprendan viniendo del rendimiento visto en CartPole, pero recordemos que el tamaño de las capas internas de la red se eligió para ese problema en concreto. Es de esperar que distintos escenarios necesiten redes de distinto tamaño que sean capaces de interpretar las observaciones de manera efectiva. En definitiva, nuestra red actual \textbf{no es lo bastante descriptiva} para la complejidad del problema.

\subsubsection*{Nueva topología e hiperparámetros}
Modificando el agente para usar la siguiente red e hiperparámetros:

((Imagen de la topología))

$$ alpha = $$
$$ gamma = $$
$$ epsilon = $$
$$ ... $$

Obteniendo resultados... 

((Grafica de resultados))

\subsubsection*{Función de recompensa}
Modificando la funcion de recompensa blablabla

((algoritmo de modificacion))

((Gráfica de resultados))

\subsubsection*{Otros enfoques}
Hablar sobre randomwalks y transfer learning