\chapter{Conclusiones y trabajo futuro}
\label{cap:conclusiones}

\chapterquote{Somos lo que hacemos repetidamente. La excelencia, entonces, no es un acto, es un hábito}{Aristóteles}

\section{Conclusiones}

Este proyecto nos ha servido como un extenso estudio sobre los orígenes y la evolución de la que actualmente es una de las tecnologías más exploradas dentro de la inteligencia artificial: aprendizaje automático, concretamente la subcategoría de aprendizaje por refuerzo profundo. 

Cuando empezamos el proyecto, sólo teníamos una leve idea de la complejidad que implicaba este término o del alcance de las utilidades que se pueden conseguir a través de esta tecnología. Pero por suerte, teníamos claro nuestro objetivo y nuestra investigación fue dirigida a conseguirlo: conseguir que una IA pudiera jugar por sí sola a un videojuego.

Comenzamos estudiando las bases del aprendizaje automático, tomando como punto de partida la técnica de aprendizaje por refuerzo. Ésta encajaba perfectamente en nuestro proyecto, ya que tratándose de videojuegos, era fácil encontrar recompensas en ese entorno. Investigando sobre ello, descubrimos el método de Q-Learning, que parecía ser el que mejor se adecuaba a nuestro caso. Nos permitía tomar decisiones en una representación fiable de cualquier entorno, ya que no era necesario establecer un modelo del mismo, así que la IA podría jugar con normalidad independientemente del juego.

Tras el estudio previo, era el momento de hacer nuestras primeras pruebas y aprender cómo utilizar el entorno. Empezamos con un juego sencillo, el CartPole disponible en la librería de OpenAI Gym. El entorno era sencillo y las acciones que debía tomar el agente, reducidas. Sin embargo, el modelo utilizado en Q-Learning nos podía dar problemas en entornos mayores, a pesar de que el enfoque era bueno en un principio: el tamaño que requería la representación de los estados posibles crecía de forma casi exponencial cada vez que se introducían nuevas variables en el entorno, o aumentaba el rango de valores de las existentes. Por ello, necesitaríamos ir un paso más allá. 

No fue fácil acostumbrarse al uso del método de Q-Learning. El concepto resultaba curioso a la vez que simple, debido a su parecido con una máquina de estados. No obstante, fue durante la implementación cuando más comenzamos a notar la complejidad que implicaba su uso, incluso en un problema a primera vista sencillo, como resultó ser el de CartPole. El hecho de plantear la representación del problema supuso un reto, ya que tratábamos con valores continuos, y fueron necesarias varias configuraciones para la discretización de dichos valores, hasta ver con cuál trabajaba mejor el algoritmo.

Como primera toma de contacto con los elementos que conformarían nuestras futuras investigaciones, en este caso todo lo aplicado sobre el ejemplo del CartPole en Q-Learning, podemos decir que los resultados fueron satisfactorios, y no solo porque el algoritmo consiguiese predecir correctamente los movimientos en el juego. Nos sirvió para prepararnos para los errores que podríamos encontrarnos más adelante, además de comprobar el funcionamiento de este método y crear lo que sería una base teórica para saber cómo aplicarlo a otros entornos en un futuro.

Simultáneamente, comenzamos buscando otros caminos por los que desarrollar nuestro proyecto, optando finalmente por el uso de redes neuronales. Comprobamos con varios ejemplos que éstas eran compatibles con nuestro problema, llegando a la conclusión de que era un buen punto por el que avanzar. Explorando más allá de lo que ya conocíamos de ellas, encontramos la forma perfecta de combinar la idea y resultados del método de Q-Learning con la rapidez y comodidad que suponían las redes neuronales: las DQNs (o \textit{Deep Q-Networks}). 

A pesar de que los conceptos que implicaban la creación de las DQNs ya nos eran familiares, los resultados en un primer momento nos pillaron totalmente por sorpresa. El agente no aprendía correctamente, y aunque esperábamos peores resultados por ser nuestro primer intento, no nos habíamos planteado unos tan malos. Fue en este punto cuando la tarea de investigación tomó un papel vital para nosotros, obligándonos a todos a volcarnos en ella, hasta ver nuestros fallos y posibles soluciones a ellos. 

Finalmente, gracias al uso de una memoria para que nuestro agente también aprendiese de sus propios errores y a una segunda red neuronal para estabilizar sus resultados, conseguimos un correcto aprendizaje. A partir de estos elementos, entendimos que no sólo importaba el volumen de los datos de los que disponíamos, o el número de episodios que dejásemos al agente aprendiendo. Lo importante era lo relevantes y representativos que debían ser éstos datos para que sirviera de algo aprender de ellos. De la misma forma, comprendimos que no siempre que aprendía algo nuevo, tenía que ser bueno. La segunda red nos fue muy útil para "guiar" al agente por la mejor ruta de aprendizaje, consiguiendo buenos resultados en pocos episodios, en comparación con todas las pruebas anteriores.  

El último paso al que llegamos en nuestro proyecto fue a trasladar todo lo aprendido y lo conseguido en el CartPole a otro entorno para comprobar que el agente seguía aprendiendo correctamente. Esta vez se trataba de el juego de MountainCar, también disponible en la librería de OpenAI Gym. 

El principal problema al que nos enfrentamos aquí fue a la velocidad de aprendizaje. Tras observar como se desenvolvía nuestro sistema en el nuevo entrono, nos dimos cuenta de que la recompensa no trasmitía una información útil para el aprendizaje del modelo, no hasta que se alcanzaba el objetivo final, lo cual implicaba un entrenamiento muy largo. Por ello decidimos centrarnos en cambiar la recompensa. Tras muchas pruebas con diversas ideas, llegamos a acelerar el aprendizaje, sorprendentemente las soluciones más simples fueron las que nos dieron mejores resultados.  

Una vez solucionado el aprendizaje nos enfrentamos a la divergencia, pero esta vez supuso un problema mayor de lo que esperábamos. De todas formas aplicando lo aprendido en CartPole realizando una serie de adaptaciones y algunos nuevos componentes menores, obtuvimos resultados exitosos, por lo que concluimos que habíamos conseguido desarrollar una IA capaz de jugar a videojuegos.


Tal vez en un primer momento la idea de hacer un trabajo de investigación, con una meta que cada vez pueda resultar más lejana, no suene muy atractiva. Sin embargo, para nosotros esto ha resultado mucho más provechoso que centrarnos en terminar un producto final. Este proyecto nos ha abierto una puerta hacia el aprendizaje por refuerzo profundo, las técnicas que lo componen y los avances que se siguen haciendo en este campo. 

La investigación y el tiempo invertidos nos han llevado a comprender el trabajo que supone construir una IA de este tipo, incluso teniendo una pequeña base inicial en la que sostenerse; de la misma forma, hemos podido experimentar de primera mano las facilidades que supone su uso, sobre el nuestro y sobre otros proyectos. Además de nuevos conocimientos, hemos adquirido nuevas ideas y una base sobre la que construirlas. 

El aprendizaje por refuerzo profundo es un campo que todavía está en crecimiento y del que nos queda mucho por aprender. Pero si algo podemos decir con seguridad es que su evolución todavía no ha acabado y que con el tiempo tendrá mucha más importancia de la que ya tiene a día de hoy. Y, con suerte para nosotros, estaremos preparados para seguir su avance.

\section{Cumplimiento de objetivos}

Al principio de este documento, planteábamos unos objetivos que explorar durante la realización de este proyecto, los cuales nos han servido para organizar el trabajo y su exposición en el presente documento:

\begin{enumerate}
        \item El primer objetivo era comprender qué es el aprendizaje por refuerzo y en qué se diferencia de otras ramas del aprendizaje automático. A lo largo del capítulo \ref{cap:reinforcementLearning} vimos una explicación de dicho concepto, así como las ideas en las que se basaba y sus diferencias respecto a otras ramas del aprendizaje automático y por qué la elegimos como punto de partida. Siguiendo con esta idea, explicamos todos los elementos que lo componen y la forma que tienen de relacionarse.
        \item Una vez visto cómo se relacionan todos los elementos del aprendizaje por refuerzo, decidimos poner dichos conceptos a prueba durante el capítulo \ref{cap:q-learning}, en el que explicamos la librería Gym de OpenAI. Elegimos el entorno de CartPole para llevar a cabo nuestros experimentos, logrando superar el objetivo planteado para dicho entorno, pudiendo experimentar los problemas que suponía la implementación de dicho algoritmo en referencia al aumento exponencial de la tabla-Q, así como la complejidad que implicaba mantenerla completamente actualizada, y conseguir el correcto aprendizaje de nuestro agente.
        \item Otro objetivo era el de adentrarnos en el campo del aprendizaje profundo y los fundamentos de las redes neuronales. Lo tratamos a lo largo del capítulo \ref{cap:deepLearning}, durante el cual explicamos los principios de las redes neuronales, su composición, estructura y funcionamiento. De este modo, jugamos con distintos problemas de clasificación y regresión que nos sirvieron para tener una primera toma de contacto que más adelante nos ayudaría para manejar problemas dentro del ámbito del aprendizaje por refuerzo profundo.
        \item Nuestro último objetivo era el de combinar las redes neuronales y el aprendizaje por refuerzo, de forma que consiguiéramos obtener los beneficios de ambos, así como encontrar soluciones a los inconvenientes que tenían por separado, como vimos durante el capítulo \ref{cap:dqnEnAccion}. Estudiamos paso a paso cómo combinar los dos ámbitos, primero reemplazando la tabla-Q del aprendizaje por refuerzo por una red neuronal y viendo que no nos otorgaba un aprendizaje suficiente en nuestro agente. Tras ello introdujimos el concepto de reproductor de experiencias y más tarde el uso de dos redes, gracias a los cuales conseguimos estabilizar el aprendizaje y una convergencia de los resultados.
        \item Cumplidos todos los objetivos y en vista a los resultados obtenidos, hemos conseguido ver las limitaciones que tiene este campo de estudio actualmente, el cual sigue en evolución y sobre el que queda mucho trabajo por hacer.
\end{enumerate}

\section{Trabajo futuro}

El proyecto iba dirigido a enseñar a una IA a jugar y, aunque ésta ha aprendido de forma exitosa, apenas ha sido probada en un par de entornos con pocas variables. El camino de nuestra investigación podía ser el correcto, pero sin más avances, o pruebas en otros entornos, no podemos estar seguros de ello. 

%TODO JCA referencias
%https://en.wikipedia.org/wiki/Transfer_learning
Dicho esto, en caso de poder continuar nuestro proyecto, nos centraríamos en continuar con más pruebas en distintos juegos. Tras comprobar que nuestra IA es adaptable, empezaríamos actualizándola para que resultase escalable a problemas con más acciones a elegir o un mayor volumen de variables, en caso de que no lo fuera ya. Un punto a considerar sería si el agente podría aplicarse directamente a problemas parecidos a los que ya conoce (\textit{Transfer Learning}), ya entrenado en ellos, y explorar cómo adaptarlo para mejorar sus resultados.

%TODO JCA referencias
%https://en.wikipedia.org/wiki/Multi-agent_system
A partir de estas modificaciones, tendríamos que considerar los posibles elementos nuevos que pudieran surgir, en función del problema o juego que tratásemos. El reto consistiría en adaptar el agente a elementos que se salen de lo visto anteriormente y que pueden provocar que la resolución o victoria no dependa sólo del agente. El caso más común en el que podemos pensar es en entornos multijugador, donde el agente tenga que enfrentarse a otros jugadores, o agentes (entornos multiagente - INSERTAR REF AQUÍ). 

Entre estos casos, también podríamos considerar los entornos con información incompleta: situaciones en las que no se pueda ver todo el mapa y en las que el agente tendría que actuar sin conocer el problema en su completitud. Los entornos probabilísticos, fáciles de ver en juegos de cartas, dados o cualquier \textit{RPG} con encuentros y daños no deterministas, también serían algo a tener en cuenta. Además de tener una estrategia ya aprendida, el agente debería aprender a adaptarla o a optar por otra totalmente distinta en función de resultados que no siempre son los que puede esperar. 

El trabajo más a largo plazo podría seguir muchos caminos. Por una parte se podría experimentar con otro tipo de algoritmos, buscando una mayor eficiencia. Prueba de ello es ``Baselines'', el repositorio de OpenAI que ya mencionamos \citep{baselines}. Otra opción sería dar un paso más allá y empezar a interpretar directamente imágenes como entrada, para así no depender de observaciones en forma de variable como hemos hecho hasta ahora.

DeepMind ya demostró esto hace algunos años. En \citet{mnih2013playing} demostraron que su IA era capaz de aprender a jugar a una serie de juegos de Atari a nivel profesional, todo esto únicamente a través de los píxeles de cada imagen. Para lograr esto habría que especializarse en redes neuronales convolucionales, lo cual sería todo un reto. Ya no sólo por ser más complejas sino porque, al necesitar mayores recursos a la hora de entrenarse, el sistema debe ser mucho más preciso para ser viable.

Pero todo esfuerzo trae su recompensa: una vez se dispone de una IA capaz de reconocer imágenes, las posibilidades se disparan. Al no estar atado a unas observaciones provistas a través de variables, deja de ser necesario depender de frameworks o tediosas implementaciones manuales para realizar pruebas. Se podría trabajar a un nivel mucho más ``real'', en el sentido de que tal vez ya no sería necesario describir un problema a la perfección, con todas sus leyes físicas, para trabajar en él. Tal vez simplemente se necesitaría proveer al agente de una serie de imágenes para que fuese capaz de aprender sobre ellas, entendiendo comportamientos y patrones y aprendiendo de forma mucho más humana. No obstante, esta idea todavía queda algo lejana para nosotros.
