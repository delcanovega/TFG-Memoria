\chapter{Conclusiones y trabajo futuro}
\label{cap:conclusiones}

\chapterquote{Somos lo que hacemos repetidamente. La excelencia, entonces, no es un acto, es un hábito}{Aristóteles}

\section{Conclusiones}

Este proyecto nos ha servido como un extenso estudio sobre los orígenes y la evolución de la que actualmente es una de las tecnologías más exploradas dentro de la Inteligencia Artificial: Aprendizaje Automático, concretamente la subcategoría de Aprendizaje Profundo. 

Cuando empezamos el proyecto, sólo teníamos una leve idea de la complejidad que implicaba este término o del alcance de las utilidades que se pueden conseguir a través de esta tecnología. Pero por suerte, teníamos claro nuestro objetivo y nuestra investigación fue dirigida a conseguirlo: conseguir que una IA pudiera jugar por sí sola a un videojuego.

Comenzamos estudiando las bases del Aprendizaje Automático, tomando como punto de partida la técnica de Aprendizaje por Refuerzo. Ésta encajaba perfectamente en nuestro proyecto, ya que tratándose de videojuegos, es fácil encontrar recompensas en ese entorno. Investigando sobre ello, descubrimos el método de Q-Learning, que parecía ser el que mejor se adecuaba a nuestro caso. Nos permitía tomar decisiones a cada momento, manteniendo una representación fiable de cualquier posible entorno, por lo que la IA podría jugar con normalidad independientemente del juego.

Tras el estudio previo, era el momento de hacer nuestras primeras pruebas y aprender cómo utilizar el entorno. Empezamos con un juego sencillo, el CartPole disponible en la librería de OpenAI Gym. El entorno era sencillo y las acciones que debía tomar el agente, reducidas. Los resultados eran buenos, pero necesitábamos ir un paso más allá. El modelo utilizado en Q-Learning nos podía dar muchos problemas en entornos mayores, a pesar de que el enfoque era bueno en un principio.

Casi simultáneamente, comenzamos buscando otros caminos por los que desarrollar nuestro proyecto, optando finalmente por el uso de redes neuronales. Comprobamos con varios ejemplos que éstas eran compatibles con nuestro problema, llegando a la conclusión de que era un buen punto por el que avanzar. Explorando más allá de lo que ya conocíamos de ellas, encontramos la forma perfecta de combinar la idea y resultados del Q-Learning con la rapidez y comodidad que suponían las redes neuronales: DQNs. 

Las pruebas utilizando DQNs resultaron algo más tediosas de realizar, ya que se trataba de algo totalmente nuevo para nosotros y requería un tiempo de investigación extra. Los resultados no eran buenos, pero tras conseguir estabilizarlos con una segunda red y añadirle una memoria para que aprendiese de sus propios errores, conseguimos que nuestro agente aprendiese correctamente, dando mejores resultados.

El último paso al que llegamos en nuestro proyecto fue a trasladar todo lo aprendido y lo conseguido en el CartPole a otro entorno para comprobar que el agente seguía aprendiendo correctamente. Esta vez se trataba de el juego de MountainCar, también disponible en la librería de OpenAI Gym. Tras una serie de adaptaciones y algunos nuevos componentes menores, obtuvimos resultados exitosos, por lo que concluimos que habíamos conseguido desarrollar una IA capaz de jugar. 

Lo conseguido en este proyecto nos ha demostrado la cantidad de posibilidades que existen a la hora de resolver un problema de este tipo. La investigación y tiempo invertidos, nos ha llevado a comprender el trabajo que supone construir una IA, incluso teniendo una pequeña base inicial en la que sostenerse, y la cantidad de información que nos queda por aprender respecto a este tema.  

\section{Cumplimiento de objetivos}

Al principio de este documento, planteábamos unos objetivos que explorar durante la realización de este proyecto, los cuales nos han servido para organizar el trabajo y su exposición en el presente documento:

\begin{enumerate}
        \item El primer objetivo era comprender qué es el Aprendizaje por Refuerzo y en qué se diferencia de otras ramas del Aprendizaje Automático. A lo largo del capítulo \ref{cap:reinforcementLearning} vimos una explicación de dicho concepto, así como las ideas en las que se basaba y sus diferencias respecto a otras ramas del Aprendizaje Automático y por qué la elegimos como punto de partida. Siguiendo con esta idea, explicamos todos los elementos que lo componen y la forma que tienen de relacionarse.
        \item Una vez visto cómo se relacionan todos los elementos del Aprendizaje por Refuerzo, decidimos poner dichos conceptos a prueba durante el capítulo \ref{cap:q-learning}, en el que explicamos la librería Gym de OpenAI. Elegimos el entorno de CartPole para llevar a cabo nuestros experimentos, logrando superar el objetivo planteado para dicho entorno, pudiendo experimentar los problemas que suponía la implementación de dicho algoritmo en referencia al aumento exponencial de la tabla-Q, así como la complejidad que implicaba mantenerla completamente actualizada, y conseguir el correcto aprendizaje de nuestro agente.
        \item Otro objetivo era el de adentrarnos en el campo del Aprendizaje Profundo y los fundamentos de las Redes Neuronales. Lo tratamos a lo largo del capítulo \ref{cap:deepLearning}, durante el cual explicamos los principios de las Redes Neuronales, su composición, estructura y funcionamiento. De este modo, jugamos con distintos problemas de clasificación y regresión que nos sirvieron para tener una primera toma de contacto que más adelante nos ayudaría para manejar problemas dentro del ámbito del Aprendizaje por Refuerzo Profundo.
        \item Nuestro último objetivo era el de combinar las Redes Neuronales y el Aprendizaje por Refuerzo, de forma que consiguiéramos obtener los beneficios de ambos, así como encontrar soluciones a los inconvenientes que tenían por separado, como vimos durante el capítulo \ref{cap:dqnEnAccion}. Estudiamos paso a paso cómo combinar los dos ámbitos, primero reemplazando la tabla-Q del Aprendizaje por Refuerzo por una red neuronal y viendo que no nos otorgaba un aprendizaje suficiente en nuestro agente. Tras ello introdujimos el concepto de reproductor de experiencias y más tarde el uso de dos redes, gracias a los cuales conseguimos estabilizar el aprendizaje y una convergencia de los resultados.
        \item Cumplidos todos los objetivos y en vista a los resultados obtenidos, hemos conseguido ver las limitaciones que tiene este campo de estudio actualmente, el cual sigue en evolución y sobre el que queda mucho trabajo por hacer.
\end{enumerate}

\section{Trabajo futuro}

El proyecto iba dirigido a enseñar a una IA a jugar y, aunque ésta ha aprendido de forma exitosa, apenas ha sido probada en un par de entornos con pocas variables. El camino de nuestra investigación podía ser el correcto, pero sin más avances, o pruebas en otros entornos, no podemos estar seguros de ello. 

Dicho esto, en caso de poder continuar nuestro proyecto, nos centraríamos en continuar con más pruebas en distintos juegos. Tras comprobar que nuestra IA es adaptable, empezaríamos añadiendo más opciones a las posibles acciones que puede realizar el jugador, actualizándola para que resulte escalable, en caso de que no lo fuera ya. 

A partir de ahí, el trabajo podría seguir muchos caminos. Por una parte se podría experimentar con otro tipo de algoritmos, buscando una mayor eficiencia. Prueba de ello es ``Baselines'', el repositorio de OpenAI que ya mencionamos \citep{baselines}. Otra opción sería dar un paso más allá y empezar a interpretar directamente imágenes como entrada, para así no depender de observaciones en forma de variable como hemos hecho hasta ahora.

DeepMind ya demostró esto hace algunos años. En \citet{mnih2013playing} demostraron que su IA era capaz de aprender a jugar a una serie de juegos de Atari a nivel profesional, todo esto únicamente a través de los píxeles de cada imagen. Para lograr esto habría que especializarse en Redes Neuronales Convolucionales, lo cual sería todo un reto. Ya no sólo por ser más complejas sino porque, al necesitar mayores recursos a la hora de entrenarse, el sistema debe ser mucho más preciso para ser viable.

Pero todo esfuerzo trae su recompensa: una vez se dispone de una IA capaz de reconocer imágenes, las posibilidades se disparan. Al no estar atado a unas observaciones provistas a través de variables, deja de ser necesario depender de frameworks o tediosas implementaciones manuales para realizar pruebas. Se podría trabajar a un nivel mucho más ``real'', en el sentido de que tal vez ya no sería necesario describir un problema a la perfección, con todas sus leyes físicas, para trabajar en él. Tal vez simplemente se necesitaría proveer al agente de un vídeo para que fuese capaz de aprender sobre él, entendiendo comportamientos y patrones y aprendiendo de forma mucho más humana. No obstante, esta idea todavía queda algo lejana para nosotros.
