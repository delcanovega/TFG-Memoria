\chapter{Redes Neuronales y Q-Learning}
\label{cap:deepLearning}

\chapterquote{En otras actividades más allá del puro pensamiento lógico, nuestras mentes funcionan mucho más rápido que cualquier ordenador jamás concebido}{Daniel Crevier}

\section{Redes Neuronales: definición y elementos}
En vistas a solucionar problemas, más complejos que los vistos anteriormente, surgió la necesidad de buscar diferentes modelos computacionales. Una rama a destacar sería la de \textbf{Aprendizaje Profundo} (o \textit{Deep Learning}), englobada dentro de Aprendizaje Automático, que decidió diseñar estos nuevos modelos basándose en la estructura del cerebro humano, desarrollando de esta forma las primeras neuronas artificiales.

\subsection{Neuronas}
La neurona es el elemento básico de una red neuronal. Al igual que las existentes en el cerebro humano, estas neuronas son células de procesamiento; reciben varias entradas de datos y devuelven una única salida. Cada neurona contiene uno o varios pesos propios, en función del número de variables que reciba, que se relacionan con ellas de la siguiente forma:

$$Y = f(x\ w\ + b)$$

Echando un vistazo a la representación de la neurona en la imagen \ref{fig:esquema_neurona}, distinguiremos las $x$ como las variables de entrada y $w$ los pesos que les asigna la propia neurona. La variable $b$ es el \textit{bias}, o desplazamiento, que se suma a la multiplicación entre las variables y sus respectivos pesos y $f$ es la \textbf{función de activación}, que nos devolverá la salida $Y$ de la neurona.

\figura{Bitmap/AprendizajeProfundo/Neuron}{width=0.8\textwidth}{fig:esquema_neurona}%
       {Esquema de una neurona artificial}

Sin embargo, dentro de estas neuronas existen varios tipos. Las que nos interesan y trataremos en profundidad son las especializadas en no linealidad, manteniendo ésta característica en su función de activación. A continuación, listaremos las más utilizadas y las que podremos observar en la gráfica \ref{fig:func_neurona}.

\begin{itemize}
    \item Sigmoide $f(z) = \frac{1}{(1+e^{-z})}$: esta función logit marca una diferenciación clara entre valores pequeños y grandes, agrupándolos en 0 o 1 respectivamente. Entre estos dos extremos, las salidas de las neuronas adoptarán una forma de S.
    \item Tanh $f(z)= tanh(z)$: este tipo de neurona tiene una salida muy parecida a las tipo sigmoide, manteniendo la forma de S, pero con la diferencia de que el marco está entre -1 y 1. Suele ser usada con más asiduidad debido a que está centrada en 0.
    \item ReLu $f(z) = max(0,z)$: la neurona de linealidad restringida o ReLu (Restricted Linear Unit) utiliza un distinto tipo de no linealidad. A pesar de su simplicidad, ha sido la función de activación utilizada para la resolución de diferentes problemas debido a su mayor facilidad de convergencia y facilidad computacional \citep{Enyinna2018}.
\end{itemize}

\figura{Bitmap/AprendizajeProfundo/Func_Neuron}{width=0.8\textwidth}{fig:func_neurona}%
       {Comparación entre funciones Sigmoide, Tanh y ReLu}

\subsection{Estructura y funcionamiento}
A pesar de que una neurona podría resolver cualquier problema linealmente separable, nos resultaría muy difícil resolver problemas de Aprendizaje Automático más complejos a través de una sola de ellas. Para ello, de nuevo basándose en el modelo de un cerebro humano, las neuronas se encuentran agrupadas por capas, formando así una red neuronal. Cada capa consta de un número finito de neuronas del mismo tipo, recibiendo todas los mismos tipos y cantidades de datos devueltos por las neuronas de la capa anterior pero, como hemos mencionado antes, devolviendo resultados diferentes cada una.

Las redes neuronales están compuestas generalmente por varias capas, de forma que las salidas de la anterior forman las entradas de la siguiente, tal y como podemos observar en la imagen \ref{fig:red_neuronal}. Entre ellas, podemos distinguir la capa de entrada, que es la que recibe los datos a analizar, la capa de salida, que es la que devuelve los resultados en el formato que nos interesa en función del problema, y las capas intermedias u ocultas. Éstas son las encargadas de abstraer todos los datos recibidos por la capa anterior para que sea más fácil procesarlos y analizarlos. 

\figura{Bitmap/AprendizajeProfundo/Red_Neuronal}{width=1\textwidth}{fig:red_neuronal}%
       {Esquema de estructura de una red neuronal profunda}

Las redes con múltiples capas intermedias son conocidas como \textbf{Redes Neuronales Profundas}, o \textit{Deep Neural Networks} (DNNs). El ejemplo más conocido para el que podemos utilizar estas redes es el de reconocimiento de objetos en imágenes. Existen múltiples formas de utilizar las DNNs en este ámbito, así que utilizaremos el ejemplo que podemos ver en la imagen \ref{fig:dnn}. 

La capa de entrada comienza recibiendo un conjunto de píxeles de distintos colores. Las capas intermedias se encargan de distinguir el objeto, ya sea diferenciando los píxeles según sus colores y su proximidad, distinguiendo siluetas o bien reconociendo patrones que puedan distinguir al coche del resto de la fotografía. Sin conocer como tal la forma exacta en que se ha resuelto el problema, podríamos intuir que con una sola capa intermedia nos sería imposible, o realmente difícil, resolver este problema, ya que el proceso requiere varios pasos entre neuronas (por ejemplo, reconocer los faros, las ventanas, la matrícula...). Observando el esquema, la capa de salida sería la encargada de marcar las regiones en las que el coche es visible, o más bien, en las que existe el objeto coche. En otros casos, las capas intermedias también podrían ser utilizadas para disminuir las dimensiones del problema, reduciendo el número de píxeles de una imagen desde una magnitud de miles a una de decenas.

\figura{Bitmap/AprendizajeProfundo/DNN_ejemplo}{width=1\textwidth}{fig:dnn}%
       {Ejemplo de uso de una DNN \citep{NIPS2013_5207}}    

\subsubsection{Descenso de gradiente y retropropagación}

Como ya hemos visto en este mismo capítulo, la neurona dispone de unos pesos para asignar a sus entradas y así producir una salida que nos resulte útil. Sin embargo, el proceso de asignar un valor óptimo a esos pesos no resulta para nada trivial, así que necesitamos un método que nos ayude a encontrar una configuración óptima de todas las neuronas de nuestra red.
 
El método del \textbf{descenso de gradiente} (o \textit{gradient descent}) \citep[cap. 2]{Buduma:general} es un algoritmo de optimización que permite converger hacia el valor mínimo de una función de manera iterativa. En este caso, la función a minimizar es la de error (\textit{loss}), que calcula la diferencia entre la salida estimada (\textit{y}) y la salida real (\textit{t}), consiguiendo mejores resultados. Para identificar el mínimo de la función, el método del descenso del gradiente calcula la derivada parcial respecto a cada parámetro (\textit{x}) en el punto de evaluación. La derivada indica el valor y la dirección en la que la función crece más rápido (por eso en el entrenamiento mueves los pesos en dirección contraria). No obstante, uno de los problemas que surgen en este punto es que éste puede ser tanto un mínimo local como global, siendo posible que nunca lleguemos a unos valores óptimos.

El resultado de la derivada se resta a cada uno de los parámetros, multiplicando finalmente por la \textbf{tasa de aprendizaje} ($\alpha$). Aunque hemos hablado en capítulos anteriores sobre ella, aplicada a este caso indica lo rápido que converge el algoritmo hasta un mínimo local, manteniendo un valor entre 0 y 1. Es importante ver que la tasa influye mucho a la hora de utilizar el descenso de gradiente y es necesario escoger un valor adecuado para evitar problemas. 

El descenso de gradiente resulta útil a la hora de asignar pesos a una neurona, pero para aplicarlo a una red neuronal completa hay que dar un paso más allá. Es por ello que existe un algoritmo conocido como \textbf{retropropagación} (o \textit{backpropagation}) \citep[cap. 2]{Buduma:general}. Al igual que en el descenso de gradiente, compara la salida real de la red neuronal con la salida deseada y minimiza su error. El resultado se propaga hacia todas las neuronas de la capa anterior que han contribuido directamente a crear la salida de la red. No obstante, cada una de ellas recibe una fracción de la señal total del error, según la contribución realizada. Este proceso se repite capa por capa, hasta la de entrada, ya que cada una de las capas tiene su propia función de activación y no serviría de nada aplicarles a todas las mismas modificaciones.

\subsubsection{Optimizadores}
Antes hemos hablado de la tasa de aprendizaje, pero no hemos llegado a ver qué supone un menor o mayor valor. Concretamente, la tasa influye sobre todo dependiendo del tamaño del problema: mientras que un ratio pequeño puede aproximarse mejor al error mínimo, en problemas de gran magnitud puede resultar demasiado lento. Con un ratio de aprendizaje demasiado grande, ocurre exactamente lo contrario: aprenderá más rápidamente pero puede resultar muy difícil converger en un mínimo error local. 

Para evitar que ocurrieran este tipo de problemas, se estudiaron múltiples algoritmos de optimización, de forma que dicho ratio se pudiera modificar dinámicamente durante el entrenamiento. A continuación explicaremos los optimizadores más utilizados \citep{NIPS2017_7003}:

\begin{itemize}
    \item \textbf{AdaGrad}: intenta adaptar la tasa de aprendizaje global a partir de la acumulación del historial de gradientes. Para ser específicos, mantiene un seguimiento de la tasa de cada parámetro, de forma que éste sea inversamente proporcional a la raíz de la suma de los cuadrados del historial de gradiente de todos los estados.
    \item \textbf{RMSProp}: este algoritmo utiliza el de descenso de gradiente con momento que, en resumen, se trata de un descenso de gradiente donde el tamaño de los pasos para reducir el error se modifica dinámicamente, de forma que se pueda converger en un error mínimo sin problemas. Combinando este algoritmo con el historial de gradientes de AdaGrad, obtenemos el optimizador RMSProp. 
    \item \textbf{Adam}: mientras que RMSProp utiliza la media de los gradientes que guarda Adagrad (primer momento), Adam utiliza la media de la varianza (segundo momento). Hay que tener en cuenta que, ya que los momentos están inicializados a cero, es necesario un factor de corrección para obtener el valor real de ellos. Adam será el optimizador que utilizaremos, por ser más eficiente y poder adaptarse a la mayoría de problemas \citep[cap. 4]{Buduma:general}.  
\end{itemize}

\subsection{Problemas de clasificación}
\label{sec:classif_NN}
Los problemas de clasificación consisten, a grandes rasgos, en asignar un dato recibido a una categoría de un conjunto ya establecido. El ejemplo más claro de esta categoría sería proveer de imágenes de perros y gatos a una red neuronal, y que sea ésta la que decida de qué animal se trata en cada caso. Modificando el comportamiento del ojo humano, la red recibiría dichas imágenes como un conjunto de píxeles y a partir ellos, crearía contornos y buscaría patrones para distinguir entre un animal u otro.

A pesar de que no utilizaremos directamente este tipo de problemas en futuras secciones, este ejercicio nos ha servido para entrar en contacto con Keras y comprobar su funcionamiento a la hora de crear redes neuronales. Para este ejemplo hemos elegido un conjunto de datos conocido: reconocimiento de cifras escritas a mano \citep{lecun-mnisthandwrittendigit-2010}. Éste consiste en un conjunto de 4500 imágenes de diferentes cifras escritas a mano de distintas formas, como se ve en la imagen \ref{fig:cifras}, incluyendo la respuesta correcta para comprobar los resultados de nuestro clasificador.

\figura{Bitmap/AprendizajeProfundo/Ejemplo_Cifras}{width=1\textwidth}{fig:cifras}%
       {Imagen del conjunto de datos de cifras escritas a mano de MNIST \citep{Lecun98gradient-basedlearning}} 

Para este caso, hemos establecido una red neuronal de tres capas, la de entrada, la de salida y una capa intermedia u oculta para que la red realice los cálculos necesarios. Hemos utilizado un optimizador Adam, de forma que el descenso de gradiente se realice de forma más efectiva y los valores de los pesos en la red se reajusten más rápidamente. Además, puede regular automáticamente el valor de la tasa de aprendizaje, comenzando en un valor de 0.001.

Tanto la capa de entrada como la oculta, son de tipo \textit{ReLu}. La capa de entrada se encarga de procesar los 400 píxeles que representan la imagen de una cifra escrita a mano. A modo de salida, hemos utilizado un tipo especial de capa llamado \textit{softmax}. En ella, se da la condición de que la suma de todas las salidas sea igual a 1. De esta forma, las probabilidades que reflejan las distintas salidas se encuentran normalizadas y son excluyentes entre sí, dejando que haya una probabilidad que destaque por encima de las demás en su conjunto; ésa será nuestra predicción. 

Además, para comprobar que nuestra red neuronal entrena y aprende correctamente, hemos utilizado validación cruzada en K-iteraciones (\textbf{K-Fold cross-validation}). La técnica de validación cruzada consiste en dividir en varios grupos el conjunto de datos proporcionado y entrenar varias veces el modelo sobre ellos, alternando la función de dichos grupos entre entrenamiento y prueba. El algoritmo de \textit{K-Fold} nos permite barajar el conjunto de datos y dividirlo en k grupos, todos destinados a entrenamiento, a excepción de uno que se reserva para la prueba. De esta forma, podemos comprobar las distintas formas en las que ha aprendido, en función de los grupos formados durante la aplicación del K-Fold.

Como podemos ver en la imagen \ref{fig:k-fold}, las distintas agrupaciones presentan una precisión distinta entre sí, por lo que nos quedaremos con la más óptima como resultado final. Siendo un resultado tan preciso podemos concluir que la técnica es claramente efectiva. Todo esto nos ha servido para aprender el uso adecuado de las redes y la importancia de los datos a tratar, tanto los de entrenamiento como los de prueba, lecciones que nos serán de gran utilidad para el tipo de problema que realmente nos interesa: regresión.

\figura{Bitmap/AprendizajeProfundo/ClassificationNN/Class_accuracy1}{width=1\textwidth}{fig:k-fold}%
       {Comparación de la precisión de cada una de las iteracciones del k-fold}

\subsection{Problemas de regresión}
\label{sec:regres_NN}
Los problemas de regresión se distinguen de los de clasificación en que la respuesta de la red neuronal no pertenece a un grupo, sino que se intenta predecir un valor real. La forma de entender este tipo de problemas sería tomando como ejemplo una red neuronal que sea capaz de predecir el precio de una casa en función de su tamaño, número de habitaciones, localización, etc.

Para este ejemplo, hemos utilizado el conjunto de datos \textbf{Boston Housing}, incluido en uno de los paquetes de Keras \citep{BostonKeras}. Éste consta de 13 características distintas de un número determinado de viviendas (506 casos), a partir de las cuales se puede averiguar su precio. 

Esta vez los datos son muy diversos y los atributos pueden representar tanto unidades en escalas de miles, como de unos o ceros, para representar si una característica se da o no. Por ello, antes de crear la red neuronal, es necesario estandarizar los valores mediante la función \textit{StandardScaler} de \textit{sklearn}. De esta forma, todos los atributos estarán normalizados y la red podrá trabajar con ellos de forma más eficiente.

La red neuronal se compone de tres capas, igual que en el ejemplo visto de clasificación, utilizando el optimizador Adam. De nuevo, las capas de entrada y la oculta, serán de tipo \textbf{ReLU}, con un tamaño de 13 y 26 neuronas cada una. La capa de salida consta de una única neurona, de tipo \textbf{ReLU}, de forma que nos devolverá directamente el precio predicho por la red neuronal. La fórmula de diferencia de gradiente para este problema quedaría expresada como la siguiente:

$$\Delta w_{k} = \sum_{i} \alpha\ x_{k}^{(i)} |t^{(i)} - y^{(i)} |$$

Así que con un error final tan bajo podemos confirmar que la red predice correctamente los precios. Además, para asegurarnos de la fiabilidad de la red neuronal y comprobar que no se ha dado una situación de \textbf{sobreentrenamiento} (\textit{overfitting}), en el conjunto de datos disponemos dos partes separadas, una para entrenamiento y otra para la prueba de validación, en una proporción de 80\% frente a un 20\%, respectivamente. De esta forma, mientras entrenamos la red con la primera, se puede comprobar la diferencia de errores con la segunda. Como podemos apreciar en la imagen \ref{fig:regresion}, los resultados en la validación son algo peores que en el entrenamiento, lo cual indica que no estamos cometiendo sobreentrenamiento y que la red aprende correctamente.

\figura{Bitmap/AprendizajeProfundo/regresion}{width=1\textwidth}{fig:regresion}%
       {Disminución del error sobre el entrenamiento respecto a la validación} 

En los casos de regresión haremos uso del \textbf{error cuadrático medio} (o \textit{ECM}), el más utilizado de los múltiples métodos de cálculo de error existentes. En nuestro caso, $Ypred$ se refiere al valor calculado por la red neuronal mientras que $Yreal$ es el valor real del precio de las casas. El cálculo del \textit{ECM} nos indicará la diferencia entre ambos, pudiendo así comprobar lo cerca que han estado las predicciones de la red neuronal de los valores reales.

$$ECM = 1/n \sum^n_{i=1}(Ypred_i - Yreal_i)^2$$


\section{Redes Neuronales y Q-learning}

Al hablar de Aprendizaje por Refuerzo anteriormente, nos centramos en el método de Q-Learning. Observamos que las soluciones planteadas eran válidas, pero el mayor problema que presentaba se debía al tamaño de las tabla-Q: ésta terminaba con tamaños intratables incluso con problemas de complejidad baja. 

Buscando la forma de arreglar este problema, descubrimos la posibilidad de realizar aproximaciones a la función-Q a través del uso de una red neuronal, de forma que no fuera necesario recorrer todos los pares estado-valor constantemente. Es así como surgieron las Redes-Q Profundas (\textit{Deep Q-Networks} o \textbf{DQNs}, como nos referiremos a ellas).\ref{fig:ejemplo_dqn} 

\figura{Bitmap/AprendizajeProfundo/ejemplo_dqn}{width=1\textwidth}{fig:ejemplo_dqn}%
       {Ejemplo simplificado de la red de una DQN}

Las transiciones creadas a partir de las DQNs se pueden representar como un vector de valores de la siguiente forma: 

$$Q(s, . ; T )$$ 

Donde $T$ son los parámetros de la red y $s$ es el estado actual. Esto significa que la red es capaz de calcular múltiples valores-Q de forma dinámica para todas las acciones posibles en un estado, sin abandonar la representación que suponía la tabla-Q. Además, permite generalizar estados no visitados pero sí muy parecidos a otros que ya ha visitado, ya que la red ya tendrá calculadas las decisiones para éstos también.

Podemos ver el funcionamiento de las DQNs en el pseudo-código \ref{code:dqn}, sin embargo esta es una versión bastante simplificada y para conseguir un verdadero éxito habrá que probar otras técnicas a mayores que veremos en el siguiente capítulo.

\begin{minipage}{0.9\linewidth}%
    \begin{lstlisting}[frame=tb, language=python, caption=Pseudocódigo DQN,   inputencoding=latin1, label={code:dqn}]

    Initialize environment and agent;
    
    for i in range(EPISODES):
            Initialize current-state   
            
            while not done:
         
                //Choose between a random choice (exploration) or based in the enviroment memory.
                action = agent.predict_action(state)
                //Take de chosen action and save the new data.
                next_state, reward, done, _ = env.step(action)
                agent.update_model(state, action, reward, next_state, done)
                
                //execute the bellman algorithm
                agent.replay()
                //update state
                state = next_state
                
                //update the exploration ratio (random choice)
                agent.update_hyperparameters()
    \end{lstlisting}%
\end{minipage}
