\chapter{Aprendizaje Profundo}
\label{cap:deepLearning}

En el que estudiaremos la estructura y las utilidades de las redes neuronales. 

\chapterquote{En otras actividades más allá del puro pensamiento lógico, nuestras mentes funcionan mucho más rápido que cualquier ordenador jamás concebido.}{Daniel Crevier}

\section{Redes Neuronales: definición y elementos}
En vistas a imitar el funcionamiento de un cerebro humano, en cuanto lo que a procesamiento se refiere, se crearon las primeras redes neuronales artificiales. Éstas se basaron en cálculos matemáticos y algoritmos para conseguir dicho objetivo, a partir del diseño de neuronas artificiales y procesos de auto-aprendizaje.

\subsection{Neuronas}
La neurona es el elemento básico de una red neuronal \ref{fig:esquema_neurona}. Al igual que las existentes en el cerebro humano, estas neuronas son células de procesamiento; reciben varias entradas de datos y devuelven una única salida. En la mayoría de los casos, esta salida consiste en la suma de la multiplicación de cada entrada por un peso asignado por la propia neurona, de forma que cada neurona devuelve una salida distinta a pesar de que las entradas sean las mismas.

\figura{Bitmap/Capitulo3/Neuron}{width=1\textwidth}{fig:esquema_neurona}%
       {Esquema de una neurona artificial, donde X son las entradas, Y la salida y W los pesos.}

Sin embargo, dentro de estas neuronas existen varios tipos, siendo las especializadas en no linealidad las que nos interesan, ya qué este tipo de neuronas va a producir respuestas acotadas, desapareciendo los problemas de fluctuación y la falta de adecuación a señales pequeñas y grandes. 

Estas neuronas se caracterizan por tener función de activación, o bien la función de transferencia (o ambas) no lineales. Éstas también se dividen en función de la función de activación antes mencionada. A continuación, listaremos las más utilizadas.

\begin{itemize}
    \item Sigmoide: $f(z) = \frac{1}{(1-e^{-z})}$: Esta función logit marca una diferenciación clara entre valores pequeños y grandes agrupándolos en 0 o 1 respectivamente, entre estos dos extremos las neuronas adoptarán una forma de S.\ref{fig:funcion_sigmoide}.
    \item Tanh: $f(z)= tahn(z)$: Este tipo de neurona tiene una salida muy parecida a las tipo sigmoide, manteniendo la forma de S, pero con la diferencia de que el marco esta entre -1 y 1. Suele ser usada con más asiduidad debido a que esta centrada en 0.\ref{fig:funcion_tanh}.
    \item ReLu: $f(z) = max(0,z)$: La neurona de linealidad restringida o ReLu (Restricted Linear Unit) utiliza un distinto tipo de no linealidad. A pesar de su variadas desventajas, en los últimos años se ha posicionado como la más usada en los problemas de visión computacional. Y será la que más usaremos nosotros en nuestros ejemplos.\ref{fig:funcion_ReLu}.
\end{itemize}

\figura{Bitmap/Capitulo3/sigmoide}{width=1\textwidth}{fig:funcion_sigmoide}%
       {...}
\figura{Bitmap/Capitulo3/tanh}{width=1\textwidth}{fig:funcion_tanh}%
       {...}
\figura{Bitmap/Capitulo3/Relu}{width=1\textwidth}{fig:funcion_ReLu}%
       {...}

\subsection{Estructura y funcionamiento}
Ya que las funcionalidades de una neurona son limitadas, nos resultaría imposible resolver un problema de Aprendizaje Automático con ella. Para ello, de nuevo basándose en el modelo de nuestro cerebro, las neuronas se encuentran agrupadas por capas, formando así una red neuronal. Cada capa consta de un número finito de neuronas del mismo tipo, recibiendo todas el mismo volumen de datos pero, como hemos mencionado antes, devolviendo resultados diferentes cada una.

Las redes neuronales están compuestas generalmente por varias capas, de forma que las salidas de la anterior forman las entradas de la siguiente. Entre ellas, podemos distinguir la capa de entrada, que es la que recibe los datos a analizar, la capa de salida, que es la que devuelve los resultados en el formato que nos interesa en función del problema, y las capas intermedias u ocultas. Éstas son las encargadas de abstraer todos los datos recibidos por la capa anterior para que sea más fácil procesarlos y analizarlos. Las redes con múltiples capas intermedias, son conocidas como \textbf{redes profundas}. Un ejemplo sería el ser capaz de reconocer formas y contornos: Nuestros ojos reciben sólo un grupo de colores que el cerebro ayuda a procesarlo añadiendo el concepto de bordes y separaciones para poder interpretar las imágenes.

La relación entre cada capa esta condicionada por unos pesos, estos son los que dan distintas prioridades a las entradas y consiguen la abstracción deseada. El problema es que estos pesos no tienen porque ser los ideales y para afinarlos se lleva a cabo un algoritmo conocido como \textbf{retropropagación} (backpropagation). El algoritmo consiste en comparar la salida de de la red neuronal con la salida deseada y comprobar su error para minimizarlo, cambiando los pesos de la capa anterior. Este proceso se lleva a cabo hasta alcanzar la capa de entrada. Existen distintos métodos de cálculo de error pero el más usado, y el que nosotros utilizaremos, es el de error cuadrático.

\subsubsection{Optimizadores}
Uno de los grandes problemas a la hora de entrenar redes neuronales, es el \textbf{ratio de aprendizaje}. Mientras que un ratio pequeño puede aproximarse mejor al error mínimo, en problemas de gran volumen puede resultar demasiado lento. Con un ratio de aprendizaje demasiado grande, ocurre exactamente lo contrario: Aprenderá más rápidamente pero puede resultar muy difícil converger en un mínimo error local.

Para evitar que ocurriera este tipo de problemas, se estudiaron múltiples algoritmos de optimización, de forma que dicho ratio se pudiera modificar dinámicamente durante el entrenamiento. A continuación explicaremos los optimizadores más utilizados.

\begin{itemize}
    \item \textbf{AdaGrad}: Intenta adaptar el ratio de aprendizaje global a partir de la acumulación del historial de gradientes. Para ser específicos, mantiene un seguimiento del ratio de cada parámetro, de forma que éste sea inversamente proporcional a la raíz de la suma de los cuadrados del historial de gradiente de todos los estados. 
    \item \textbf{RMSProp}: Este algoritmo utiliza el de descenso de gradiente con momento que, en resumen, se trata de un descenso de gradiente donde los pasos para reducir el error son cada vez más cortos, de forma que se pueda converger en un error mínimo sin problemas. Combinando este algoritmo con el historial de gradientes de AdaGrad, obtenemos el optimizador RMSProp. 
    \item \textbf{Adam}: Mientras que RMSProp utiliza la media de los gradientes que guarda Adagrad (primer momento), Adam utiliza la media de la varianza (segundo momento). Hay que tener en cuenta que ya que los momentos están inicializados a cero, es necesario un factor de corrección para obtener el valor real de ellos. Adam será el optimizador que utilizaremos, por ser más eficiente y poder adaptarse a la mayoría de problemas. 
\end{itemize}

Por otro lado podemos dividir los problemas de Aprendizaje Automático en dos grupos principales, Supervisado y No Supervisado. La diferencia consiste en que los No Supervisado analizan los datos y detectan patrones mientras que los Supervisado (que son en los que nos vamos a centrar) se centran en conseguir predecir una salida a partir de ciertas entradas. Para ello cuenta con una función de error en la que se apoya para minimizar las diferencias entre la salida predicha y la deseada.

\subsection{Problemas de clasificación}
Los problemas de clasificación consisten, a grandes rasgos, en asignar un dato recibido a una categoría de un conjunto ya establecido. El ejemplo más claro de esta categoría sería proveer de imágenes de perros y gatos a una red neuronal, y que sea ésta la que decida de qué animal se trata en cada caso. Modificando el comportamiento del ojo humano, la red recibiría dichas imágenes como un conjunto de píxeles y a partir ellos, crearía contornos y buscaría patrones para distinguir entre un animal u otro.

En nuestro ejemplo de aprendizaje supervisado sobre clasificación, hemos elegido otro conjunto de datos conocido: Reconocimiento de cifras escritas a mano. Este consiste en un conjunto de imágenes de cada número escrito a mano de distintas formas, incluyendo la respuesta correcta para comprobar los resultados de nuestro clasificador.

Para este caso, hemos establecido una red neuronal de tres capas, la de entrada, la de salida y una capa intermedia u oculta para que la red realice los cálculos necesarios. Hemos utilizado un optimizador Adam, de forma que el descenso de gradiente se realice de forma más efectiva y los valores de los pesos en la red se reajusten más rápidamente.

Tanto la capa de entrada como la oculta, son del tipo \textit{ReLu}, visto anteriormente. La capa de entrada se encarga de procesar los 400 píxeles que representan la imagen de una cifra escrita a mano. A modo de salida, hemos utilizado un tipo especial de capa llamado \textit{softmax}. En ella, se da la condición de que la suma de todas las salidas sea igual a 1. De esta forma, las probabilidades que reflejan las distintas salidas se encuentran normalizadas y son excluyentes entre sí, dejando que haya una probabilidad que destaque por encima de las demás en su conjunto; ésa será nuestra predicción. 

Además, para comprobar que nuestra red neuronal entrena y aprende correctamente, hemos utilizado \textbf{cross-validation} mediante \textbf{K-Fold}. La técnica de cross-validation (o validación cruzada) consiste en dividir en varios grupos el conjunto de datos proporcionado y entrenar varias veces el modelo sobre ellos, alternando la función de dichos grupos entre entrenamiento y testeo. El algoritmo K-Fold nos permite barajar el conjunto de datos y dividirlo en k grupos, todos para entrenamiento a excepción de uno que se reserva para el testeo. De esta forma, podemos comprobar las distintas en las que ha aprendido, en función de los grupos formados durante la aplicación del K-Fold \ref{fig:Class_accuracy}.

\figura{Bitmap/Capitulo3/ClassificationNN/Class_accuracy3}{width=1\textwidth}{fig:Class_accuracy}%
        {Resultados de la clasificación con K-Fold de k = 10.}

\subsection{Problemas de regresión}
Los problemas de regresión se distinguen de los de clasificación en que la respuesta de la red neuronal no pertenece a un grupo, sino que forma parte de un conjunto continuo de posibles resultados. La forma de entender este tipo de problemas sería tomando como ejemplo una red neuronal que sea capaz de predecir el precio de una casa en función de su tamaño, número de habitaciones, localización, etc.

Para este ejemplo, hemos utilizado el conjunto de datos \textbf{Boston Housing}, incluido en uno de los paquetes de Keras. Éste consta de catorce atributos y características de un número determinado de viviendas, a partir de los cuales se puede averiguar su precio. 

Esta vez los datos son muy diversos y los atributos pueden representar tanto unidades en escala de miles, como unos o ceros para representar si una característica se da o no. Por ello, antes de crear la red neuronal, es necesario estandarizar los valores mediante la función \textit{StandardScaler} de \textit{sklearn}. De esta forma, todos los atributos estarán normalizados y la red podrá trabajar con ellos de forma más eficiente.

La red neuronal se compone de tres capas, igual que en el ejemplo visto de clasificación, utilizando el optimizador Adam. De nuevo, las capas de entrada y la oculta, serán de tipo \textbf{ReLU}, con un tamaño de 14 y 26 neuronas cada una. La capa de salida consta de una única neurona, de tipo \textbf{ReLU}, de forma que nos devolverá directamente el precio predicho por la red neuronal. 

Además, para asegurarnos de la fiabilidad de la red neuronal y comprobar que no se ha dado una situación de \textbf{sobreentrenamiento} (\textit{overfitting}), en el conjunto de datos disponemos dos partes separadas de estos últimos, una para entrenamiento y otra para testeo. De esta forma, mientras entrenamos la red con la primera, se puede comprobar la diferencia de errores con la segunda. 


\section{Aprendizaje por Refuerzo Profundo}

Cuando hablamos de Aprendizaje por Refuerzo anteriormente, nos centramos en el método de Q-Learning. Observamos que las soluciones planteadas eran válidas, pero el mayor problema que presentaba se debía al tamaño de las tabla-Q: Ésta terminaba con tamaños intratables incluso con problemas de complejidad baja. En un intento de arreglar este problema, se generó la posibilidad de realizar aproximaciones a la función-Q, de forma que no fuera necesario recorrer todos los pares estado-valor. Mediante un nuevo modelo, el agente podría aproximar o generalizar múltiples valores de la tabla. A pesar de que el sistema no sería tan exacto como el anterior, el problema sería tratable y su eficiencia se seguiría manteniendo.

\subsection{DQNs}

Tras conocer la posibilidad de aproximar las funciones-Q, se planteó la forma más eficiente de realizarlas sin tener que generar un modelo específico para cada problema. Es así como surgieron las \textit{Deep Q-Networks} o \textbf{DQNs}, como nos referiremos a ellas. 

Como su nombre bien puede dar a entender, las DQNs resultan de una fusión entre el método de Q-Learning explicado anteriormente y una red neuronal profunda. Una DQN es una red neuronal multicapa que dado un estado "s" devuelve un vector de valores Q(s, · ; θ), donde θ son los parámetros de la red. Esto significa que nuestra red es capaz de calcular múltiples valores-Q de forma dinámica para todas las acciones posibles en un estado, terminando así con la intratabilidad que suponía el tamaño de la tabla-Q en problemas de Q-Learning. 

Sin embargo, ya que cualquier pequeño cambio en los parámetros de entrada modificará los resultados de la red, los valores-Q se recalcularán constantemente y podrían darse situaciones en las que el agente sea inducido a error durante su aprendizaje. Al tener multitud de estados similares con diferentes valores-Q, es muy posible que el agente ignore soluciones similares, aprendiendo mucho más despacio o repitiendo los mismos caminos constantemente. Con el fin de arreglar esta inestabilidad, se estudian varios métodos, de los cuales explicaremos los dos más relevantes.

\subsubsection{Múltiples redes neuronales}
Con el fin de en estabilizar y reducir los cambios que sufriría la red neuronal ante estados similares, surge el concepto de la \textbf{Target Q-Network}. Éste implica la utilización de una segunda red objetivo (\textit{target}), como copia de la red principal de predicción. La red objetivo sólo se actualiza cada cierto número de pasos, ahorrándose los cambios en los pesos que sufre constantemente la red de predicción y consiguiendo así más estabilidad en los resultados.

(poner fórmula Target Q-Netwrok)

Como derivación de este método existe otro conocido como \textbf{Double Q-Network}, donde las redes se dividen entre ``agente" y ``aprendizaje". La red agente se utiliza para guardar los estados con los que tomar las decisiones en cuanto a los movimientos a ejecutar, mientras que la red de aprendizaje se encarga de guardar los valores de la ejecución actualizados: Cada cierto número de pasos ambas redes son comparadas, guardando la que haya dado mejores resultados como agente, para la toma de decisiones, y utilizando la otra para el aprendizaje. Éste será el tipo de red que utilizaremos más adelante en nuestro proyecto.

(poner fórmula Double Q-Network)

\subsubsection{Experiencias pasadas}
Otra manera de conseguir estabilidad es reduciendo las similitudes entre los estados de los que aprende nuestro agente, de forma que no sean estados relacionados y puedan ser representativos de distintos momentos dentro de nuestra ejecución. Para hacerlo recurriremos al \textit{Experience Replay} (reproductor de experiencias, también conocidas como valores-Q), al que nos referiremos como \textbf{replay}. Éste método consiste en almacenar todas las experiencias pasadas del agente al finalizar un episodio y tomar una muestra aleatoria de ellas. De esta forma conseguiremos casos no relacionados para optimizar el aprendizaje de nuestro agente.